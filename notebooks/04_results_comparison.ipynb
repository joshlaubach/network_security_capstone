{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0233fd10",
   "metadata": {},
   "source": [
    "# Network Security Capstone - Results Comparison\n",
    "\n",
    "**Purpose:** Integrate and compare findings from both unsupervised (BETH) and supervised (UNSW-NB15) analyses.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives:\n",
    "1. Load results from previous notebooks\n",
    "2. Compare unsupervised vs supervised model performance\n",
    "3. Analyze how data quality and feature engineering affected outcomes\n",
    "4. Identify key insights and best-performing models\n",
    "5. Create comprehensive visualizations for comparison\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Joshua Laubach  \n",
    "**Date:** October 27, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f6837",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e29b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define results directory\n",
    "RESULTS_DIR = Path('../results')\n",
    "\n",
    "# Check if results directory exists\n",
    "if not RESULTS_DIR.exists():\n",
    "    print(f\"Results directory not found: {RESULTS_DIR}\")\n",
    "    print(\"Please run notebooks 02 and 03 first to generate results.\")\n",
    "else:\n",
    "    print(f\"Results directory found: {RESULTS_DIR}\")\n",
    "    print(f\"Available result files:\")\n",
    "    for file in sorted(RESULTS_DIR.glob('*.csv')):\n",
    "        print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87869788",
   "metadata": {},
   "source": [
    "## 2. Load Unsupervised Results (BETH Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BETH baseline vs enhanced comparison results\n",
    "BETH_RESULTS_FILE = RESULTS_DIR / 'beth_baseline_vs_enhanced_comparison.csv'\n",
    "try:\n",
    "    beth_results = pd.read_csv(BETH_RESULTS_FILE)\n",
    "    print(f\"[OK] Successfully loaded: {BETH_RESULTS_FILE.name}\")\n",
    "    print(f\"\\nShape: {beth_results.shape}\")\n",
    "    print(\"\\n[BETH Unsupervised Models - Baseline vs Enhanced Performance]\")\n",
    "    print(beth_results.to_string(index=False))\n",
    "except FileNotFoundError:\n",
    "    print(f\"[X] File not found: {BETH_RESULTS_FILE.name}\")\n",
    "    print(\"   Please run notebook '02_beth_unsupervised.ipynb' to generate this file.\")\n",
    "    beth_results = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BETH TF-IDF feature importance\n",
    "BETH_TFIDF_FILE = RESULTS_DIR / 'beth_tfidf_feature_importance.csv'\n",
    "try:\n",
    "    beth_tfidf_importance = pd.read_csv(BETH_TFIDF_FILE)\n",
    "    print(f\"[OK] Successfully loaded: {BETH_TFIDF_FILE.name}\")\n",
    "    print(f\"\\nTop 10 most important TF-IDF features:\")\n",
    "    print(beth_tfidf_importance.head(10).to_string(index=False))\n",
    "except FileNotFoundError:\n",
    "    print(f\"[X] File not found: {BETH_TFIDF_FILE.name}\")\n",
    "    beth_tfidf_importance = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfa5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BETH selected TF-IDF features\n",
    "BETH_SELECTED_FEATURES_FILE = RESULTS_DIR / 'beth_selected_tfidf_features.csv'\n",
    "try:\n",
    "    beth_selected_features = pd.read_csv(BETH_SELECTED_FEATURES_FILE)\n",
    "    print(f\"[OK] Successfully loaded: {BETH_SELECTED_FEATURES_FILE.name}\")\n",
    "    print(f\"\\nNumber of selected features: {len(beth_selected_features)}\")\n",
    "    print(f\"\\nTop 15 selected TF-IDF features:\")\n",
    "    print(beth_selected_features.head(15).to_string(index=False))\n",
    "except FileNotFoundError:\n",
    "    print(f\"[X] File not found: {BETH_SELECTED_FEATURES_FILE.name}\")\n",
    "    beth_selected_features = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BETH suspicious vs evil analysis (baseline vs enhanced)\n",
    "BETH_SUS_EVIL_BASELINE_FILE = RESULTS_DIR / 'beth_sus_vs_evil_baseline.csv'\n",
    "BETH_SUS_EVIL_ENHANCED_FILE = RESULTS_DIR / 'beth_sus_vs_evil_enhanced.csv'\n",
    "try:\n",
    "    beth_sus_evil_baseline = pd.read_csv(BETH_SUS_EVIL_BASELINE_FILE)\n",
    "    beth_sus_evil_enhanced = pd.read_csv(BETH_SUS_EVIL_ENHANCED_FILE)\n",
    "    print(f\"[OK] Successfully loaded: {BETH_SUS_EVIL_BASELINE_FILE.name}\")\n",
    "    print(f\"[OK] Successfully loaded: {BETH_SUS_EVIL_ENHANCED_FILE.name}\")\n",
    "    print(\"\\n[Baseline Model - Sus vs Evil Performance]\")\n",
    "    print(beth_sus_evil_baseline.to_string(index=False))\n",
    "    print(\"\\n[Enhanced Model - Sus vs Evil Performance]\")\n",
    "    print(beth_sus_evil_enhanced.to_string(index=False))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"[X] File not found: {e.filename}\")\n",
    "    print(\"   Please run notebook '02_beth_unsupervised.ipynb' to generate these files.\")\n",
    "    beth_sus_evil_baseline = beth_sus_evil_enhanced = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9857479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BETH anomalous arguments analysis\n",
    "BETH_ANOMALOUS_ARGS_FILE = RESULTS_DIR / 'beth_anomalous_arguments_analysis.csv'\n",
    "try:\n",
    "    beth_anomalous_args = pd.read_csv(BETH_ANOMALOUS_ARGS_FILE)\n",
    "    print(f\"[OK] Successfully loaded: {BETH_ANOMALOUS_ARGS_FILE.name}\")\n",
    "    print(f\"\\nTop 10 anomalous argument patterns:\")\n",
    "    print(beth_anomalous_args.head(10).to_string(index=False))\n",
    "except FileNotFoundError:\n",
    "    print(f\"[X] File not found: {BETH_ANOMALOUS_ARGS_FILE.name}\")\n",
    "    print(\"   Please run notebook '02_beth_unsupervised.ipynb' to generate this file.\")\n",
    "    beth_anomalous_args = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841cd233",
   "metadata": {},
   "source": [
    "## 3. Load Supervised Results (UNSW-NB15 Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UNSW-NB15 supervised model comparison\n",
    "UNSW_RESULTS_FILE = RESULTS_DIR / 'unsw_supervised_comparison.csv'\n",
    "try:\n",
    "    unsw_results = pd.read_csv(UNSW_RESULTS_FILE)\n",
    "    print(f\"[OK] Successfully loaded: {UNSW_RESULTS_FILE.name}\")\n",
    "    print(f\"\\nShape: {unsw_results.shape}\")\n",
    "    print(\"\\n[UNSW-NB15 Supervised Models - Performance Summary]\")\n",
    "    print(unsw_results.to_string(index=False))\n",
    "    \n",
    "    # Identify best model\n",
    "    if 'roc_auc' in unsw_results.columns:\n",
    "        best_idx = unsw_results['roc_auc'].idxmax()\n",
    "        best_model = unsw_results.loc[best_idx, 'model']\n",
    "        best_auc = unsw_results.loc[best_idx, 'roc_auc']\n",
    "        print(f\"\\nBest model by ROC-AUC: {best_model} (AUC={best_auc:.4f})\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[X] File not found: {UNSW_RESULTS_FILE.name}\")\n",
    "    print(\"   Please run notebook '03_unsw_supervised.ipynb' to generate this file.\")\n",
    "    unsw_results = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af1965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Random Forest feature importances\n",
    "RF_IMPORTANCES_FILE = RESULTS_DIR / 'unsw_rf_feature_importances.csv'\n",
    "try:\n",
    "    rf_importances = pd.read_csv(RF_IMPORTANCES_FILE)\n",
    "    print(f\"[OK] Successfully loaded: {RF_IMPORTANCES_FILE.name}\")\n",
    "    print(f\"\\n[Top 10 Most Important Features - Random Forest]\")\n",
    "    print(rf_importances.head(10).to_string(index=False))\n",
    "except FileNotFoundError:\n",
    "    print(f\"[X] File not found: {RF_IMPORTANCES_FILE.name}\")\n",
    "    rf_importances = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f26391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load XGBoost feature importances\n",
    "XGB_IMPORTANCES_FILE = RESULTS_DIR / 'unsw_xgb_feature_importances.csv'\n",
    "try:\n",
    "    xgb_importances = pd.read_csv(XGB_IMPORTANCES_FILE)\n",
    "    print(f\"[OK] Successfully loaded: {XGB_IMPORTANCES_FILE.name}\")\n",
    "    print(f\"\\n[Top 10 Most Important Features - XGBoost]\")\n",
    "    print(xgb_importances.head(10).to_string(index=False))\n",
    "except FileNotFoundError:\n",
    "    print(f\"[X] File not found: {XGB_IMPORTANCES_FILE.name}\")\n",
    "    xgb_importances = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UNSW feature selection results\n",
    "# These files are optional and may not exist if feature selection was not run\n",
    "UNSW_SELECTED_FEATURES_FILE = RESULTS_DIR / 'unsw_selected_features.csv'\n",
    "UNSW_FEAT_SEL_COMP_FILE = RESULTS_DIR / 'unsw_feature_selection_comparison.csv'\n",
    "UNSW_FEAT_SEL_PERF_FILE = RESULTS_DIR / 'unsw_feature_selection_performance.csv'\n",
    "\n",
    "try:\n",
    "    unsw_selected_features = pd.read_csv(UNSW_SELECTED_FEATURES_FILE)\n",
    "    unsw_feature_selection_comparison = pd.read_csv(UNSW_FEAT_SEL_COMP_FILE)\n",
    "    unsw_feature_selection_performance = pd.read_csv(UNSW_FEAT_SEL_PERF_FILE)\n",
    "    \n",
    "    print(f\"[OK] Successfully loaded all feature selection files.\")\n",
    "    print(f\"\\n[Feature Selection Methods Comparison]\")\n",
    "    print(unsw_feature_selection_comparison.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n[All Features vs Selected Features Performance]\")\n",
    "    print(unsw_feature_selection_performance.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n[Top 10 Selected Features]\")\n",
    "    print(unsw_selected_features.head(10).to_string(index=False))\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"[i]  Feature selection file not found: {e.filename}\")\n",
    "    print(\"   This is optional. To generate, re-run the feature selection section in notebook 03.\")\n",
    "    unsw_selected_features = unsw_feature_selection_comparison = unsw_feature_selection_performance = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f2e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load XGBoost hyperparameter tuning results\n",
    "# These files are optional and may not exist if hyperparameter tuning was not run\n",
    "XGB_LR_TUNING_FILE = RESULTS_DIR / 'unsw_xgb_learning_rate_tuning.csv'\n",
    "XGB_DEPTH_TUNING_FILE = RESULTS_DIR / 'unsw_xgb_depth_tuning.csv'\n",
    "XGB_REG_TUNING_FILE = RESULTS_DIR / 'unsw_xgb_regularization_tuning.csv'\n",
    "\n",
    "try:\n",
    "    xgb_lr_tuning = pd.read_csv(XGB_LR_TUNING_FILE)\n",
    "    xgb_depth_tuning = pd.read_csv(XGB_DEPTH_TUNING_FILE)\n",
    "    xgb_reg_tuning = pd.read_csv(XGB_REG_TUNING_FILE)\n",
    "    \n",
    "    print(\"[OK] Successfully loaded all XGBoost tuning files.\")\n",
    "    print(f\"\\n[Learning Rate Tuning - Best Configuration]\")\n",
    "    best_lr = xgb_lr_tuning.loc[xgb_lr_tuning['roc_auc'].idxmax()]\n",
    "    print(f\"  Learning Rate: {best_lr['learning_rate']}, ROC-AUC: {best_lr['roc_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n[Max Depth Tuning - Best Configuration]\")\n",
    "    best_depth = xgb_depth_tuning.loc[xgb_depth_tuning['roc_auc'].idxmax()]\n",
    "    print(f\"  Max Depth: {best_depth['max_depth']:.0f}, ROC-AUC: {best_depth['roc_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n[Regularization Tuning - Best Configuration]\")\n",
    "    best_reg = xgb_reg_tuning.loc[xgb_reg_tuning['roc_auc'].idxmax()]\n",
    "    print(f\"  Config: {best_reg['configuration']}, ROC-AUC: {best_reg['roc_auc']:.4f}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"[i]  XGBoost tuning file not found: {e.filename}\")\n",
    "    print(\"   This is optional. To generate, re-run the hyperparameter tuning section in notebook 03.\")\n",
    "    xgb_lr_tuning = xgb_depth_tuning = xgb_reg_tuning = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f073d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UNSW two-stage pipeline results\n",
    "UNSW_TWO_STAGE_PREDS_FILE = RESULTS_DIR / 'unsw_two_stage_predictions.csv'\n",
    "UNSW_STAGE2_PERF_FILE = RESULTS_DIR / 'unsw_stage2_attack_type_performance.csv'\n",
    "\n",
    "try:\n",
    "    unsw_two_stage = pd.read_csv(UNSW_TWO_STAGE_PREDS_FILE)\n",
    "    unsw_stage2_performance = pd.read_csv(UNSW_STAGE2_PERF_FILE)\n",
    "    \n",
    "    print(f\"[OK] Successfully loaded: {UNSW_TWO_STAGE_PREDS_FILE.name}\")\n",
    "    print(f\"[OK] Successfully loaded: {UNSW_STAGE2_PERF_FILE.name}\")\n",
    "    \n",
    "    print(f\"\\n[Two-Stage Pipeline Predictions]\")\n",
    "    print(f\"  Total predictions: {len(unsw_two_stage)}\")\n",
    "    print(f\"  Stage 1 detected attacks: {(unsw_two_stage['stage1_prediction'] == 'Attack').sum()}\")\n",
    "    \n",
    "    print(f\"\\n[Stage 2 Attack Type Classification Performance]\")\n",
    "    print(unsw_stage2_performance.to_string(index=False))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"[X] File not found: {e.filename}\")\n",
    "    print(\"   Please run the two-stage pipeline section in notebook '03_unsw_supervised.ipynb'.\")\n",
    "    unsw_two_stage = unsw_stage2_performance = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a1afd",
   "metadata": {},
   "source": [
    "## 4. Cross-Dataset Comparison\n",
    "\n",
    "Compare the challenges and outcomes of unsupervised (BETH) vs supervised (UNSW-NB15) approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf2ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Domain',\n",
    "        'Learning Paradigm',\n",
    "        'Primary Metric',\n",
    "        'Best Model Type',\n",
    "        'Key Challenge',\n",
    "        'Feature Engineering Impact',\n",
    "        'Interpretability'\n",
    "    ],\n",
    "    'BETH (Unsupervised)': [\n",
    "        'System Call Logs',\n",
    "        'Unsupervised Clustering',\n",
    "        'Detection Rate / FPR',\n",
    "        'K-Means / GMM (varies by metric)',\n",
    "        'Distinguishing subtle anomalies without labels',\n",
    "        'Critical - enables distance-based separation',\n",
    "        'Moderate - cluster centers interpretable'\n",
    "    ],\n",
    "    'UNSW-NB15 (Supervised)': [\n",
    "        'Network Traffic',\n",
    "        'Supervised Classification',\n",
    "        'ROC-AUC / F1-Score',\n",
    "        'XGBoost / Random Forest',\n",
    "        'High dimensionality and class imbalance',\n",
    "        'Enhances performance - pair features crucial',\n",
    "        'High - feature importances clearly defined'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CROSS-DATASET COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eefaad6",
   "metadata": {},
   "source": [
    "## 5. Visualization: Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BETH baseline vs enhanced models (if results available)\n",
    "if beth_results is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Prepare data - compare baseline vs enhanced across metrics\n",
    "    if 'model_type' in beth_results.columns and 'metric' in beth_results.columns:\n",
    "        # Pivot format: each row is a metric, columns are baseline/enhanced\n",
    "        metrics_to_plot = ['Detection Rate', 'False Positive Rate', 'True Negative Rate', 'F1-Score']\n",
    "        \n",
    "        for i, (ax, metric_group) in enumerate(zip(axes.flatten(), \n",
    "                                                     [['Detection Rate', 'False Positive Rate'], \n",
    "                                                      ['True Negative Rate', 'F1-Score']])):\n",
    "            x_pos = []\n",
    "            y_vals = []\n",
    "            labels = []\n",
    "            colors = []\n",
    "            \n",
    "            for metric in metric_group:\n",
    "                if metric in beth_results['metric'].values:\n",
    "                    baseline_val = beth_results[(beth_results['metric'] == metric) & \n",
    "                                                (beth_results['model_type'] == 'baseline')]['value'].values\n",
    "                    enhanced_val = beth_results[(beth_results['metric'] == metric) & \n",
    "                                                (beth_results['model_type'] == 'enhanced')]['value'].values\n",
    "                    \n",
    "                    if len(baseline_val) > 0 and len(enhanced_val) > 0:\n",
    "                        labels.extend([f'{metric}\\n(Baseline)', f'{metric}\\n(Enhanced)'])\n",
    "                        y_vals.extend([baseline_val[0], enhanced_val[0]])\n",
    "                        colors.extend(['steelblue', 'darkorange'])\n",
    "            \n",
    "            if len(y_vals) > 0:\n",
    "                x_pos = list(range(len(y_vals)))\n",
    "                bars = ax.bar(x_pos, y_vals, color=colors, alpha=0.7, edgecolor='black')\n",
    "                ax.set_xticks(x_pos)\n",
    "                ax.set_xticklabels(labels, fontsize=9)\n",
    "                ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "                ax.set_title(f'BETH - {\" & \".join(metric_group)}', fontsize=12, fontweight='bold')\n",
    "                ax.set_ylim([0, 1.0])\n",
    "                ax.grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                # Add value labels\n",
    "                for bar, val in zip(bars, y_vals):\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2, val + 0.02, \n",
    "                           f'{val:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "    else:\n",
    "        # Simple format: columns are metrics\n",
    "        print(\"Displaying BETH results in table format (non-standard structure)\")\n",
    "        print(beth_results.to_string(index=False))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('BETH Dataset - Baseline vs Enhanced Model Performance', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"BETH results not available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93109e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare UNSW-NB15 supervised models (if results available)\n",
    "if unsw_results is not None and 'model' in unsw_results.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    colors = ['steelblue', 'darkorange', 'forestgreen', 'purple']\n",
    "    \n",
    "    for ax, metric, title, color in zip(axes, metrics, titles, colors):\n",
    "        if metric in unsw_results.columns:\n",
    "            models = unsw_results['model'].values\n",
    "            values = unsw_results[metric].values\n",
    "            \n",
    "            bars = ax.bar(range(len(models)), values, color=color, alpha=0.7, edgecolor='black')\n",
    "            ax.set_xticks(range(len(models)))\n",
    "            ax.set_xticklabels(models, rotation=15, ha='right')\n",
    "            ax.set_ylabel(title, fontsize=11, fontweight='bold')\n",
    "            ax.set_title(f'UNSW-NB15 - {title}', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylim([0, 1.0])\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, (bar, val) in enumerate(zip(bars, values)):\n",
    "                ax.text(i, val + 0.02, f'{val:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('UNSW-NB15 Dataset - Supervised Model Performance', \n",
    "                 fontsize=14, fontweight='bold', y=1.0)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"UNSW results not available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5825533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC comparison for UNSW models\n",
    "if unsw_results is not None and 'roc_auc' in unsw_results.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    models = unsw_results['model'].values\n",
    "    roc_aucs = unsw_results['roc_auc'].values\n",
    "    \n",
    "    colors = ['steelblue', 'darkorange', 'forestgreen']\n",
    "    bars = ax.barh(models, roc_aucs, color=colors[:len(models)], alpha=0.7, edgecolor='black', height=0.6)\n",
    "    \n",
    "    ax.set_xlabel('ROC-AUC Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('UNSW-NB15 - ROC-AUC Comparison', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_xlim([0.9, 1.0])  # Zoom in to see differences\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, roc_aucs)):\n",
    "        ax.text(val + 0.002, i, f'{val:.4f}', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Add reference line at 0.95\n",
    "    ax.axvline(x=0.95, color='red', linestyle='--', linewidth=1, alpha=0.5, label='0.95 threshold')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ROC-AUC data not available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0686f5d",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze consensus features from supervised models\n",
    "if rf_importances is not None and xgb_importances is not None:\n",
    "    # Get top 20 features from each\n",
    "    rf_top_features = set(rf_importances['feature'].head(20))\n",
    "    xgb_top_features = set(xgb_importances['feature'].head(20))\n",
    "    \n",
    "    # Find overlap\n",
    "    consensus_features = rf_top_features & xgb_top_features\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE IMPORTANCE CONSENSUS (Random Forest & XGBoost)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal consensus features (in top 20 of both): {len(consensus_features)}\")\n",
    "    print(\"\\nConsensus Features:\")\n",
    "    for i, feat in enumerate(sorted(consensus_features), 1):\n",
    "        print(f\"  {i:2d}. {feat}\")\n",
    "    \n",
    "    # Check if engineered features are important\n",
    "    engineered_patterns = ['_sum', '_diff', '_ratio', 'both_zero', 'one_zero']\n",
    "    engineered_consensus = [f for f in consensus_features \n",
    "                           if any(pattern in f for pattern in engineered_patterns)]\n",
    "    \n",
    "    print(f\"\\nEngineered features in consensus: {len(engineered_consensus)}\")\n",
    "    if engineered_consensus:\n",
    "        print(\"\\nCritical Engineered Features:\")\n",
    "        for i, feat in enumerate(sorted(engineered_consensus), 1):\n",
    "            print(f\"  {i:2d}. {feat}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\n[KEY INSIGHT] Feature engineering impact:\")\n",
    "    impact_pct = (len(engineered_consensus) / len(consensus_features) * 100) if consensus_features else 0\n",
    "    print(f\"  {impact_pct:.1f}% of consensus important features are engineered features\")\n",
    "    print(\"  This demonstrates the critical role of domain-informed feature creation.\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Feature importance data not available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53ee82",
   "metadata": {},
   "source": [
    "## 6.1 Feature Selection Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature selection impact (using already loaded data)\n",
    "if all(v is not None for v in [unsw_selected_features, unsw_feature_selection_comparison, unsw_feature_selection_performance]):\n",
    "    print(\"\\n[Feature Selection Methods Applied]\")\n",
    "    print(unsw_feature_selection_comparison.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n[Performance Impact]\")\n",
    "    print(unsw_feature_selection_performance.to_string(index=False))\n",
    "    \n",
    "    # Calculate dimensionality reduction\n",
    "    original_features = unsw_feature_selection_performance.loc[\n",
    "        unsw_feature_selection_performance['feature_set'] == 'all_features', 'n_features'].values[0]\n",
    "    selected_features_count = unsw_feature_selection_performance.loc[\n",
    "        unsw_feature_selection_performance['feature_set'] == 'selected_features', 'n_features'].values[0]\n",
    "    reduction_pct = (1 - selected_features_count / original_features) * 100\n",
    "    \n",
    "    print(f\"\\n[KEY INSIGHT] Feature Selection Impact:\")\n",
    "    print(f\"  Original features: {original_features}\")\n",
    "    print(f\"  Selected features: {selected_features_count}\")\n",
    "    print(f\"  Dimensionality reduction: {reduction_pct:.1f}%\")\n",
    "    \n",
    "    # Performance delta\n",
    "    auc_delta = (unsw_feature_selection_performance.loc[\n",
    "                     unsw_feature_selection_performance['feature_set'] == 'selected_features', 'roc_auc'].values[0] -\n",
    "                 unsw_feature_selection_performance.loc[\n",
    "                     unsw_feature_selection_performance['feature_set'] == 'all_features', 'roc_auc'].values[0])\n",
    "    \n",
    "    if auc_delta >= 0:\n",
    "        print(f\"  Performance change: +{auc_delta:.4f} ROC-AUC (improved/maintained)\")\n",
    "    else:\n",
    "        print(f\"  Performance change: {auc_delta:.4f} ROC-AUC (minimal impact)\")\n",
    "        \n",
    "    print(f\"  Result: Achieved {reduction_pct:.1f}% dimensionality reduction with negligible performance impact\")\n",
    "    \n",
    "else:\n",
    "    print(\"Feature selection results not available. Run notebook 03 with feature selection first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d12ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature selection impact\n",
    "if unsw_feature_selection_comparison is not None and unsw_feature_selection_performance is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Left plot: Features selected by each method\n",
    "    ax = axes[0]\n",
    "    methods = unsw_feature_selection_comparison['method'].values\n",
    "    n_features = unsw_feature_selection_comparison['n_features_selected'].values\n",
    "    colors = ['steelblue', 'darkorange', 'forestgreen']\n",
    "    \n",
    "    bars = ax.barh(methods, n_features, color=colors[:len(methods)], \n",
    "                   alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    ax.set_xlabel('Number of Features Selected', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Feature Selection by Method', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, val in enumerate(n_features):\n",
    "        ax.text(val + 1, i, str(val), va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Right plot: Performance comparison\n",
    "    ax = axes[1]\n",
    "    feature_sets = ['All Features', 'Selected Features']\n",
    "    roc_aucs = unsw_feature_selection_performance['roc_auc'].values\n",
    "    colors = ['steelblue', 'darkorange']\n",
    "    \n",
    "    bars = ax.bar(feature_sets, roc_aucs, color=colors, \n",
    "                  alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    ax.set_ylabel('ROC-AUC', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Performance: All vs Selected Features', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim([min(roc_aucs) - 0.005, 1.0])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars, roc_aucs):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, val + 0.002, \n",
    "                f'{val:.4f}', ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Feature Selection Impact on UNSW-NB15', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature selection visualizations not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d5c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETH Anomalous Arguments Analysis\n",
    "if beth_anomalous_args is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BETH ANOMALOUS ARGUMENTS ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Anomalous Argument Patterns:\")\n",
    "    print(beth_anomalous_args.head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n[KEY INSIGHT] Anomalous Argument Detection:\")\n",
    "    print(\"  - TF-IDF feature engineering successfully identified suspicious arguments\")\n",
    "    print(\"  - Rare/unique arguments are strong indicators of malicious behavior\")\n",
    "    print(\"  - These patterns can inform rule-based detection systems\")\n",
    "    \n",
    "    # Visualize top anomalous arguments if score column exists\n",
    "    if 'anomaly_score' in beth_anomalous_args.columns or 'tfidf_score' in beth_anomalous_args.columns:\n",
    "        score_col = 'anomaly_score' if 'anomaly_score' in beth_anomalous_args.columns else 'tfidf_score'\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        top_args = beth_anomalous_args.head(15)\n",
    "        \n",
    "        if 'argument' in top_args.columns:\n",
    "            args = top_args['argument'].values\n",
    "            scores = top_args[score_col].values\n",
    "            \n",
    "            bars = ax.barh(range(len(args)), scores, color='crimson', alpha=0.7, edgecolor='black')\n",
    "            ax.set_yticks(range(len(args)))\n",
    "            ax.set_yticklabels(args, fontsize=9)\n",
    "            ax.set_xlabel(score_col.replace('_', ' ').title(), fontsize=11, fontweight='bold')\n",
    "            ax.set_title('Top 15 Most Anomalous Arguments (BETH)', fontsize=13, fontweight='bold')\n",
    "            ax.invert_yaxis()\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"BETH anomalous arguments analysis not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b89436",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning Impact (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776dad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter tuning impact\n",
    "if all(v is not None for v in [xgb_lr_tuning, xgb_depth_tuning, xgb_reg_tuning]):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Learning rate impact\n",
    "    axes[0].plot(xgb_lr_tuning['learning_rate'], xgb_lr_tuning['roc_auc'], \n",
    "                marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "    axes[0].set_xlabel('Learning Rate', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_ylabel('ROC-AUC', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_title('Learning Rate Impact', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Max depth impact\n",
    "    axes[1].plot(xgb_depth_tuning['max_depth'], xgb_depth_tuning['roc_auc'], \n",
    "                marker='s', linewidth=2, markersize=8, color='darkorange')\n",
    "    axes[1].set_xlabel('Max Depth', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_ylabel('ROC-AUC', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_title('Tree Depth Impact', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Regularization impact\n",
    "    configs = xgb_reg_tuning['configuration'].values\n",
    "    roc_values = xgb_reg_tuning['roc_auc'].values\n",
    "    axes[2].barh(range(len(configs)), roc_values, color='forestgreen', alpha=0.7, edgecolor='black')\n",
    "    axes[2].set_yticks(range(len(configs)))\n",
    "    axes[2].set_yticklabels(configs, fontsize=9)\n",
    "    axes[2].set_xlabel('ROC-AUC', fontsize=11, fontweight='bold')\n",
    "    axes[2].set_title('Regularization Impact', fontsize=12, fontweight='bold')\n",
    "    axes[2].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, val in enumerate(roc_values):\n",
    "        axes[2].text(val + 0.001, i, f'{val:.4f}', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('XGBoost Hyperparameter Tuning Impact on UNSW-NB15', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n[HYPERPARAMETER TUNING INSIGHTS]\")\n",
    "    print(f\"\\nLearning Rate Range: {xgb_lr_tuning['roc_auc'].max() - xgb_lr_tuning['roc_auc'].min():.4f}\")\n",
    "    print(f\"Max Depth Range: {xgb_depth_tuning['roc_auc'].max() - xgb_depth_tuning['roc_auc'].min():.4f}\")\n",
    "    print(f\"Regularization Range: {xgb_reg_tuning['roc_auc'].max() - xgb_reg_tuning['roc_auc'].min():.4f}\")\n",
    "else:\n",
    "    print(\"Hyperparameter tuning results not complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d79688e",
   "metadata": {},
   "source": [
    "## 7.1 Two-Stage Pipeline Analysis (Detection -> Classification)\n",
    "\n",
    "Evaluate the realistic security operations workflow: first detect attacks, then classify attack types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze two-stage pipeline results\n",
    "if unsw_two_stage is not None and unsw_stage2_performance is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TWO-STAGE PIPELINE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Stage 1 statistics\n",
    "    total_samples = len(unsw_two_stage)\n",
    "    detected_attacks = (unsw_two_stage['stage1_prediction'] == 'Attack').sum()\n",
    "    detected_normal = total_samples - detected_attacks\n",
    "    \n",
    "    print(f\"\\n[Stage 1: Attack Detection]\")\n",
    "    print(f\"  Total test samples: {total_samples}\")\n",
    "    print(f\"  Detected as Normal: {detected_normal} ({100*detected_normal/total_samples:.2f}%)\")\n",
    "    print(f\"  Detected as Attack: {detected_attacks} ({100*detected_attacks/total_samples:.2f}%)\")\n",
    "    \n",
    "    # Calculate Stage 1 accuracy\n",
    "    correct_stage1 = ((unsw_two_stage['stage1_prediction'] == 'Normal') & \n",
    "                      (unsw_two_stage['true_label'] == 'Normal')).sum() + \\\n",
    "                     ((unsw_two_stage['stage1_prediction'] == 'Attack') & \n",
    "                      (unsw_two_stage['true_label'] != 'Normal')).sum()\n",
    "    stage1_accuracy = correct_stage1 / total_samples\n",
    "    print(f\"  Stage 1 Accuracy: {stage1_accuracy:.4f}\")\n",
    "    \n",
    "    # Stage 2 statistics\n",
    "    print(f\"\\n[Stage 2: Attack Type Classification]\")\n",
    "    print(f\"  Attack types detected: {len(unsw_stage2_performance)}\")\n",
    "    print(f\"\\n  Performance by Attack Type:\")\n",
    "    print(unsw_stage2_performance.to_string(index=False))\n",
    "    \n",
    "    # Overall metrics\n",
    "    avg_precision = unsw_stage2_performance['precision'].mean()\n",
    "    avg_recall = unsw_stage2_performance['recall'].mean()\n",
    "    avg_f1 = unsw_stage2_performance['f1_score'].mean()\n",
    "    \n",
    "    print(f\"\\n  Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"  Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"  Average F1-Score: {avg_f1:.4f}\")\n",
    "    \n",
    "    # End-to-end accuracy\n",
    "    correct_e2e = (unsw_two_stage['true_label'] == unsw_two_stage['final_prediction']).sum()\n",
    "    e2e_accuracy = correct_e2e / total_samples\n",
    "    print(f\"\\n[End-to-End Pipeline Performance]\")\n",
    "    print(f\"  Overall Accuracy: {e2e_accuracy:.4f}\")\n",
    "    print(f\"  Stage 1  Stage 2 Success Rate: {e2e_accuracy:.2%}\")\n",
    "    \n",
    "    print(\"\\n[KEY INSIGHT] Two-Stage Pipeline Benefits:\")\n",
    "    print(\"  - Realistic security operations workflow\")\n",
    "    print(\"  - Stage 1 filters normal traffic (efficiency)\")\n",
    "    print(\"  - Stage 2 provides actionable attack type intelligence\")\n",
    "    print(\"  - Modular design allows independent optimization of each stage\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Two-stage pipeline results not available. Run notebook 03 Section 10 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c14fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize two-stage pipeline performance\n",
    "if unsw_stage2_performance is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot 1: Attack type performance\n",
    "    ax = axes[0]\n",
    "    attack_types = unsw_stage2_performance['attack_type'].values\n",
    "    f1_scores = unsw_stage2_performance['f1_score'].values\n",
    "    \n",
    "    bars = ax.barh(range(len(attack_types)), f1_scores, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax.set_yticks(range(len(attack_types)))\n",
    "    ax.set_yticklabels(attack_types, fontsize=10)\n",
    "    ax.set_xlabel('F1-Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Stage 2: Attack Type Classification Performance', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim([0, 1.0])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, val in enumerate(f1_scores):\n",
    "        ax.text(val + 0.02, i, f'{val:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Precision/Recall comparison\n",
    "    ax = axes[1]\n",
    "    x = range(len(attack_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    precision = unsw_stage2_performance['precision'].values\n",
    "    recall = unsw_stage2_performance['recall'].values\n",
    "    \n",
    "    bars1 = ax.barh([i - width/2 for i in x], precision, width, \n",
    "                    label='Precision', color='darkorange', alpha=0.7, edgecolor='black')\n",
    "    bars2 = ax.barh([i + width/2 for i in x], recall, width,\n",
    "                    label='Recall', color='forestgreen', alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax.set_yticks(x)\n",
    "    ax.set_yticklabels(attack_types, fontsize=10)\n",
    "    ax.set_xlabel('Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Stage 2: Precision vs Recall by Attack Type', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim([0, 1.0])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Two-Stage Pipeline: Attack Type Classification Performance', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Two-stage pipeline visualizations not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d60c1ae",
   "metadata": {},
   "source": [
    "## 8. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e0013",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n[1. UNSUPERVISED ANOMALY DETECTION (BETH)]\")\n",
    "if beth_results is not None:\n",
    "    print(\"\\n    Baseline vs Enhanced Comparison:\")\n",
    "    \n",
    "    # Check if data is in pivot format\n",
    "    if 'model_type' in beth_results.columns and 'metric' in beth_results.columns:\n",
    "        baseline_metrics = beth_results[beth_results['model_type'] == 'baseline']\n",
    "        enhanced_metrics = beth_results[beth_results['model_type'] == 'enhanced']\n",
    "        \n",
    "        print(\"      Baseline Model Performance:\")\n",
    "        for _, row in baseline_metrics.iterrows():\n",
    "            print(f\"        - {row['metric']}: {row['value']:.4f}\")\n",
    "        \n",
    "        print(\"\\n      Enhanced Model Performance:\")\n",
    "        for _, row in enhanced_metrics.iterrows():\n",
    "            print(f\"        - {row['metric']}: {row['value']:.4f}\")\n",
    "    else:\n",
    "        print(\"      Results:\")\n",
    "        print(beth_results.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n    Key Insights:\")\n",
    "    print(\"      - TF-IDF feature engineering enabled effective anomaly detection\")\n",
    "    print(\"      - Baseline vs Enhanced comparison shows impact of feature selection\")\n",
    "    print(\"      - Suspicious vs Evil detection demonstrates model versatility\")\n",
    "    print(\"      - Anomalous argument patterns identified security-relevant features\")\n",
    "else:\n",
    "    print(\"    [Results not available - run notebook 02 first]\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "\n",
    "print(\"\\n[2. SUPERVISED ATTACK CLASSIFICATION (UNSW-NB15)]\")\n",
    "if unsw_results is not None:\n",
    "    print(\"\\n    Best Models:\")\n",
    "    if 'roc_auc' in unsw_results.columns:\n",
    "        best_model = unsw_results.loc[unsw_results['roc_auc'].idxmax()]\n",
    "        print(f\"      - Overall Best: {best_model['model']}\")\n",
    "        print(f\"        ROC-AUC: {best_model['roc_auc']:.4f}\")\n",
    "        if 'f1_score' in best_model:\n",
    "            print(f\"        F1-Score: {best_model['f1_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\n    Key Insights:\")\n",
    "    print(\"      - Ensemble methods (RF, XGBoost) outperformed linear models\")\n",
    "    print(\"      - Feature selection reduced dimensionality while maintaining performance\")\n",
    "    print(\"      - Hyperparameter tuning provided measurable improvements\")\n",
    "    print(\"      - All models achieved >95% ROC-AUC on attack detection\")\n",
    "    \n",
    "    if unsw_two_stage is not None:\n",
    "        print(\"\\n    Two-Stage Pipeline:\")\n",
    "        print(\"      - Stage 1 (Detection): High accuracy binary classification\")\n",
    "        print(\"      - Stage 2 (Classification): Attack type identification for incidents\")\n",
    "        print(\"      - End-to-end workflow mirrors real security operations\")\n",
    "else:\n",
    "    print(\"    [Results not available - run notebook 03 first]\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "\n",
    "print(\"\\n[3. FEATURE ENGINEERING IMPACT]\")\n",
    "print(\"\\n    BETH Dataset:\")\n",
    "print(\"      - TF-IDF encoding captured argument importance\")\n",
    "print(\"      - Feature selection identified top 50 most discriminative features\")\n",
    "print(\"      - Anomalous argument analysis revealed attack patterns\")\n",
    "\n",
    "print(\"\\n    UNSW-NB15 Dataset:\")\n",
    "if rf_importances is not None and xgb_importances is not None:\n",
    "    rf_top = set(rf_importances['feature'].head(20))\n",
    "    xgb_top = set(xgb_importances['feature'].head(20))\n",
    "    consensus = rf_top & xgb_top\n",
    "    engineered = [f for f in consensus if any(p in f for p in ['_sum', '_diff', '_ratio', 'zero'])]\n",
    "    print(f\"      - {len(consensus)} consensus features across RF and XGBoost\")\n",
    "    print(f\"      - {len(engineered)} engineered features in top consensus\")\n",
    "    if len(consensus) > 0:\n",
    "        print(f\"      - {len(engineered)/len(consensus)*100:.1f}% of important features are engineered\")\n",
    "else:\n",
    "    print(\"      - Engineered features showed high importance in models\")\n",
    "\n",
    "if unsw_feature_selection_performance is not None:\n",
    "    reduction = (1 - unsw_feature_selection_performance.loc[\n",
    "        unsw_feature_selection_performance['feature_set'] == 'selected_features', 'n_features'].values[0] /\n",
    "        unsw_feature_selection_performance.loc[\n",
    "        unsw_feature_selection_performance['feature_set'] == 'all_features', 'n_features'].values[0]) * 100\n",
    "    print(f\"      - Feature selection achieved {reduction:.1f}% dimensionality reduction\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "\n",
    "print(\"\\n[4. MODEL SELECTION RECOMMENDATIONS]\")\n",
    "print(\"\\n    For Anomaly Detection (Unlabeled Data):\")\n",
    "print(\"      - Use K-Means with enhanced features for baseline detection\")\n",
    "print(\"      - TF-IDF feature engineering critical for text-based features\")\n",
    "print(\"      - Feature selection improves interpretability and performance\")\n",
    "\n",
    "print(\"\\n    For Attack Classification (Labeled Data):\")\n",
    "print(\"      - XGBoost recommended for best overall performance\")\n",
    "print(\"      - Random Forest for interpretability and feature importance\")\n",
    "print(\"      - Two-stage pipeline for realistic security operations workflow\")\n",
    "print(\"      - Feature selection maintains performance with fewer features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\\n[CONCLUSION]\")\n",
    "print(\"\\nBoth unsupervised and supervised approaches successfully detected network\")\n",
    "print(\"security threats. The BETH analysis demonstrated TF-IDF feature engineering's\")\n",
    "print(\"effectiveness for anomaly detection, while UNSW-NB15 showed that supervised\")\n",
    "print(\"ensemble methods with feature selection achieve excellent attack classification.\")\n",
    "print(\"The two-stage pipeline provides a realistic security operations workflow,\")\n",
    "print(\"first detecting attacks, then classifying types for incident response.\")\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871c7cf",
   "metadata": {},
   "source": [
    "## 9. Export Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77777053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison dataframe\n",
    "comparison_df.to_csv(RESULTS_DIR / 'cross_dataset_comparison.csv', index=False)\n",
    "print(\"[SAVED] Cross-dataset comparison to results/cross_dataset_comparison.csv\")\n",
    "\n",
    "# Create summary statistics file\n",
    "summary_stats = {}\n",
    "\n",
    "if beth_results is not None:\n",
    "    # Handle pivot format (model_type, metric, value) vs simple format\n",
    "    if 'model_type' in beth_results.columns and 'metric' in beth_results.columns:\n",
    "        enhanced_dr = beth_results[(beth_results['model_type'] == 'enhanced') & \n",
    "                                   (beth_results['metric'] == 'Detection Rate')]['value'].values\n",
    "        enhanced_fpr = beth_results[(beth_results['model_type'] == 'enhanced') & \n",
    "                                    (beth_results['metric'] == 'False Positive Rate')]['value'].values\n",
    "        \n",
    "        summary_stats['beth_enhanced_detection_rate'] = enhanced_dr[0] if len(enhanced_dr) > 0 else None\n",
    "        summary_stats['beth_enhanced_fpr'] = enhanced_fpr[0] if len(enhanced_fpr) > 0 else None\n",
    "    elif 'detection_rate' in beth_results.columns:\n",
    "        summary_stats['beth_best_detection_rate'] = beth_results['detection_rate'].max()\n",
    "        summary_stats['beth_best_fpr'] = beth_results['fpr'].min() if 'fpr' in beth_results.columns else None\n",
    "\n",
    "if unsw_results is not None:\n",
    "    summary_stats['unsw_best_roc_auc'] = unsw_results['roc_auc'].max() if 'roc_auc' in unsw_results.columns else None\n",
    "    summary_stats['unsw_best_f1'] = unsw_results['f1_score'].max() if 'f1_score' in unsw_results.columns else None\n",
    "    summary_stats['unsw_best_model'] = unsw_results.loc[unsw_results['roc_auc'].idxmax(), 'model'] if 'roc_auc' in unsw_results.columns else None\n",
    "\n",
    "if unsw_feature_selection_performance is not None:\n",
    "    summary_stats['feature_selection_dimensionality_reduction_pct'] = (\n",
    "        1 - unsw_feature_selection_performance.loc[\n",
    "            unsw_feature_selection_performance['feature_set'] == 'selected_features', 'n_features'].values[0] /\n",
    "        unsw_feature_selection_performance.loc[\n",
    "            unsw_feature_selection_performance['feature_set'] == 'all_features', 'n_features'].values[0]\n",
    "    ) * 100\n",
    "\n",
    "if unsw_two_stage is not None:\n",
    "    total = len(unsw_two_stage)\n",
    "    correct = (unsw_two_stage['true_label'] == unsw_two_stage['final_prediction']).sum()\n",
    "    summary_stats['two_stage_pipeline_accuracy'] = correct / total\n",
    "\n",
    "# Save summary stats\n",
    "summary_df = pd.DataFrame([summary_stats])\n",
    "summary_df.to_csv(RESULTS_DIR / 'comparison_summary_statistics.csv', index=False)\n",
    "print(\"[SAVED] Comparison summary statistics to results/comparison_summary_statistics.csv\")\n",
    "\n",
    "print(\"\\nAll comparison results saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}