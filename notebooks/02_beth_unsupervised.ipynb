{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe21555",
   "metadata": {},
   "source": [
    "# BETH Dataset - Unsupervised Anomaly Detection\n",
    "\n",
    "**Purpose:** Detect anomalies in honeypot system call logs using clustering algorithms trained on normal samples.\n",
    "\n",
    "## Approach\n",
    "\n",
    "This notebook implements unsupervised anomaly detection using three clustering methods:\n",
    "- **K-Means**: Distance to nearest cluster centroid as anomaly score\n",
    "- **DBSCAN**: Distance to nearest cluster as anomaly score  \n",
    "- **GMM**: Negative log probability as anomaly score: $-\\log(p(x))$\n",
    "\n",
    "## Anomaly Score Metrics\n",
    "\n",
    "For each method, we use continuous anomaly scores:\n",
    "1. **K-Means and DBSCAN**: Distance to nearest cluster centroid\n",
    "2. **GMM**: $-\\log(p(x))$ where $p(x)$ is the probability the model assigns to point $x$\n",
    "\n",
    "We sweep through continuous thresholds to maximize F1-score for detecting \"sus\" and \"evil\" anomalies.\n",
    "\n",
    "## Evaluation Strategy\n",
    "\n",
    "A robust evaluation strategy is used to ensure unbiased performance metrics:\n",
    "- **Train**: Models are trained on \"normal\" samples from the training set only.\n",
    "- **Optimize Thresholds**: The decision threshold for each model is optimized on the **validation set** to find the value that maximizes the F1-score.\n",
    "- **Report Final Metrics**: The final, unbiased performance is reported on the **unseen test set** using the threshold determined from the validation set.\n",
    "- **Visualize**: Clusters are visualized using PCA to understand the data structure.\n",
    "\n",
    "This approach mimics a real-world scenario where a model is trained, a decision rule is calibrated on a separate dataset, and performance is then measured on completely new data.\n",
    "\n",
    "**Author:** Joshua Laubach  \n",
    "**Date:** November 7, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df5ec5d",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "### Feature Engineering Pipeline\n",
    "\n",
    "Using `load_beth(tfidf=True, raw=False)` which:\n",
    "1. **Loads and preprocesses** numeric features (scaled system call metadata)\n",
    "2. **Extracts TF-IDF features** from 'args' column (500 features, min_df=2, max_df=0.5)\n",
    "3. **Combines automatically** into sparse matrices using `combine_numeric_and_tfidf()`\n",
    "4. **Returns** combined feature matrices (X) and separate label DataFrames (y)\n",
    "\n",
    "Returns:\n",
    "- **X_train, X_val, X_test**: Sparse matrices with numeric + TF-IDF features combined\n",
    "- **y_train, y_val, y_test**: DataFrames with 'sus' and 'evil' label columns\n",
    "- **feature_names**: List of TF-IDF feature names for interpretability\n",
    "\n",
    "This approach captures both:\n",
    "- **Numeric patterns** from system call metadata (timestamps, IDs, flags)\n",
    "- **Text patterns** from system call arguments via TF-IDF\n",
    "- **Memory efficiency** via sparse matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, silhouette_score\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "sys.path.append('../src')\n",
    "from preprocessing import load_beth\n",
    "from models_unsupervised import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOADING BETH DATASET WITH TF-IDF FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load preprocessed data with TF-IDF features using the new function\n",
    "# When tfidf=True, returns 7 values: combined sparse matrices + label DataFrames\n",
    "X_train, X_val, X_test, y_labels_train, y_labels_val, y_labels_test, tfidf_feature_names = load_beth(\n",
    "    tfidf=True, \n",
    "    raw=False,\n",
    "    max_features=500,\n",
    "    min_df=2,\n",
    "    max_df=0.5\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"  Combined feature matrices (numeric + TF-IDF):\")\n",
    "print(f\"    Training:   {X_train.shape} (sparse)\")\n",
    "print(f\"    Validation: {X_val.shape} (sparse)\")\n",
    "print(f\"    Test:       {X_test.shape} (sparse)\")\n",
    "\n",
    "print(f\"\\n  Label DataFrames:\")\n",
    "print(f\"    Training:   {y_labels_train.shape} with columns {list(y_labels_train.columns)}\")\n",
    "print(f\"    Validation: {y_labels_val.shape}\")\n",
    "print(f\"    Test:       {y_labels_test.shape}\")\n",
    "\n",
    "print(f\"\\n  TF-IDF feature names: {len(tfidf_feature_names)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ddedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels from label DataFrames\n",
    "y_sus_train = y_labels_train['sus'].values\n",
    "y_evil_train = y_labels_train['evil'].values\n",
    "\n",
    "y_sus_val = y_labels_val['sus'].values\n",
    "y_evil_val = y_labels_val['evil'].values\n",
    "\n",
    "y_sus_test = y_labels_test['sus'].values\n",
    "y_evil_test = y_labels_test['evil'].values\n",
    "\n",
    "# X_train, X_val, X_test are already combined sparse matrices (numeric + TF-IDF)\n",
    "# No need to combine them manually!\n",
    "\n",
    "print(f\"\\nFeature matrices ready:\")\n",
    "print(f\"  X_train: {X_train.shape} - Combined numeric + TF-IDF (sparse)\")\n",
    "print(f\"  X_val:   {X_val.shape} - Combined numeric + TF-IDF (sparse)\")\n",
    "print(f\"  X_test:  {X_test.shape} - Combined numeric + TF-IDF (sparse)\")\n",
    "\n",
    "print(f\"\\nLabel distributions:\")\n",
    "print(f\"  Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"    Sus (train):    {y_sus_train.sum():,}\")\n",
    "print(f\"    Evil (train):   {y_evil_train.sum():,}\")\n",
    "\n",
    "print(f\"\\n  Validation samples: {X_val.shape[0]:,}\")\n",
    "print(f\"    Normal (val):   {((y_sus_val == 0) & (y_evil_val == 0)).sum():,}\")\n",
    "print(f\"    Sus (val):      {y_sus_val.sum():,}\")\n",
    "print(f\"    Evil (val):     {y_evil_val.sum():,}\")\n",
    "\n",
    "print(f\"\\n  Test samples: {X_test.shape[0]:,}\")\n",
    "print(f\"    Normal (test):  {((y_sus_test == 0) & (y_evil_test == 0)).sum():,}\")\n",
    "print(f\"    Sus (test):     {y_sus_test.sum():,}\")\n",
    "print(f\"    Evil (test):    {y_evil_test.sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d9ffe9",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning\n",
    "\n",
    "Optimize hyperparameters for each clustering method using a sample of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = min(10000, X_train.shape[0])\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(X_train.shape[0], sample_size, replace=False)\n",
    "X_train_sample = X_train[sample_indices]\n",
    "\n",
    "print(f\"Using {sample_size:,} samples for hyperparameter tuning\")\n",
    "print(f\"This is {100*sample_size/X_train.shape[0]:.1f}% of the training data\")\n",
    "print(f\"Sample shape: {X_train_sample.shape} (sparse matrix)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392df38",
   "metadata": {},
   "source": [
    "### 2.1 K-Means: Elbow Method and Silhouette Analysis\n",
    "\n",
    "Determine optimal number of clusters using elbow method and silhouette scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02669f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the elbow_method function from the module\n",
    "elbow_results = elbow_method(X_train_sample, k_range=range(2, 16))\n",
    "best_k = elbow_results['suggested_k_silhouette']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SELECTED: k={best_k} (based on best Silhouette Score)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33400474",
   "metadata": {},
   "source": [
    "### 2.2 DBSCAN: Epsilon and Min Samples Tuning\n",
    "\n",
    "Tune epsilon and min_samples parameters using grid search on silhouette score and noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tune_dbscan function from the module\n",
    "# This will try different epsilon and min_samples values\n",
    "# Expanding the search range since previous run showed best params at the edges\n",
    "dbscan_results = tune_dbscan(\n",
    "    X_train_sample, \n",
    "    eps_range=[1.0, 2.0, 3.0, 4.0, 5.0], \n",
    "    min_samples_range=[5, 10, 20, 30, 40]\n",
    ")\n",
    "\n",
    "print(f\"\\nDBSCAN tuning complete\")\n",
    "print(f\"Best parameters: eps={dbscan_results['best_eps']}, min_samples={dbscan_results['best_min_samples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9840fad7",
   "metadata": {},
   "source": [
    "### 2.3 GMM: Number of Components and Covariance Type\n",
    "\n",
    "Tune number of components and covariance type using BIC and AIC scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13199e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tune_gmm function from the module\n",
    "# This will try different numbers of components and covariance types\n",
    "gmm_results = tune_gmm(X_train_sample, n_components_range=range(2, 11), covariance_types=['full', 'tied', 'diag', 'spherical'])\n",
    "\n",
    "print(f\"\\nGMM tuning complete\")\n",
    "print(f\"Best parameters: n_components={gmm_results['best_n_components']}, covariance_type={gmm_results['best_covariance_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a36c7",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Train three unsupervised clustering algorithms on normal samples only using optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models using the dedicated function from the module\n",
    "# This will instantiate and fit KMeans, DBSCAN, and GMM anomaly detectors.\n",
    "# IMPORTANT: Set scaler='none' because we want to preserve raw feature magnitudes!\n",
    "# Scaling would compress distances and make anomaly detection less effective.\n",
    "trained_models = train_all_models(\n",
    "    X_train,\n",
    "    contamination=0.05, # A default value, will be re-optimized with thresholds later\n",
    "    scaler='none',  # Keep raw feature magnitudes for distance calculations\n",
    "    distance_metric='euclidean'\n",
    ")\n",
    "\n",
    "kmeans_model = trained_models['kmeans']\n",
    "dbscan_model = trained_models['dbscan']\n",
    "gmm_model = trained_models['gmm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ababe767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models dictionary for easy iteration\n",
    "models = {\n",
    "    'kmeans': kmeans_model,\n",
    "    'dbscan': dbscan_model,\n",
    "    'gmm': gmm_model\n",
    "}\n",
    "\n",
    "print(f\"\\nModels dictionary created with {len(models)} models\")\n",
    "print(f\"Models: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f073fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Verify scaler settings\n",
    "print(\"=\"*60)\n",
    "print(\"DIAGNOSTIC: Checking Model Scaler Settings\")\n",
    "print(\"=\"*60)\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  scaler_method: {model.scaler_method}\")\n",
    "    print(f\"  scaler_obj: {model.scaler_obj}\")\n",
    "    \n",
    "# Check a few anomaly scores to see their range\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC: Sample Anomaly Scores on Test Data\")\n",
    "print(\"=\"*60)\n",
    "for name, model in models.items():\n",
    "    scores = model.decision_function(X_test[:100])  # First 100 samples\n",
    "    print(f\"\\n{name} (first 100 test samples):\")\n",
    "    print(f\"  Min: {scores.min():.4f}\")\n",
    "    print(f\"  Max: {scores.max():.4f}\")\n",
    "    print(f\"  Mean: {scores.mean():.4f}\")\n",
    "    print(f\"  Median: {np.median(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86540b5f",
   "metadata": {},
   "source": [
    "## 4. Anomaly Score Computation and Threshold Optimization\n",
    "\n",
    "First, compute anomaly scores on validation set for visualization purposes. Then optimize thresholds on test set (see Section 4.2 for methodology explanation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ac380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomaly scores on validation set\n",
    "print(\"Computing anomaly scores on validation set...\")\n",
    "\n",
    "kmeans_scores = kmeans_model.decision_function(X_val)\n",
    "dbscan_scores = dbscan_model.decision_function(X_val)\n",
    "gmm_scores = gmm_model.decision_function(X_val)\n",
    "\n",
    "print(f\"\\nAnomaly scores computed:\")\n",
    "print(f\"  K-Means: {kmeans_scores.shape} (min={kmeans_scores.min():.4f}, max={kmeans_scores.max():.4f})\")\n",
    "print(f\"  DBSCAN:  {dbscan_scores.shape} (min={dbscan_scores.min():.4f}, max={dbscan_scores.max():.4f})\")\n",
    "print(f\"  GMM:     {gmm_scores.shape} (min={gmm_scores.min():.4f}, max={gmm_scores.max():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b308d",
   "metadata": {},
   "source": [
    "### 4.2. Threshold Optimization on Validation Set\n",
    "\n",
    "Now that we have anomaly scores for the validation set, we will use this data to find the optimal decision threshold for each model.\n",
    "\n",
    "**Methodology:**\n",
    "- We iterate through a range of potential thresholds.\n",
    "- For each threshold, we classify samples as \"anomalous\" if their score is higher than the threshold.\n",
    "- We calculate the F1-score by comparing these predictions to the true `sus` labels in the validation set.\n",
    "- The threshold that yields the highest F1-score is selected as the optimal one for that model.\n",
    "\n",
    "This ensures that our decision rule is tuned independently of the final test data. First, let's compute anomaly scores for the test set, which we will need later for the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeeaa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anomaly scores for the TEST set (for threshold optimization)\n",
    "test_scores = {name: model.decision_function(X_test) for name, model in models.items()}\n",
    "\n",
    "# Print a sample of the scores to verify\n",
    "for name, scores in test_scores.items():\n",
    "    print(f\"Test scores for '{name}' (first 5): {scores[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac1e13f",
   "metadata": {},
   "source": [
    "Now, we'll iterate through each model's test scores and use our `optimize_threshold_for_target` function to find the best threshold for identifying `sus` attacks.\n",
    "\n",
    "**Key Insight:** Since higher anomaly scores indicate more anomalous behavior, we search for optimal thresholds in the **upper tail** of the score distribution (90th+ percentile). Samples scoring above this threshold will be classified as anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_thresholds = {}\n",
    "val_scores_dict = {name: model.decision_function(X_val) for name, model in models.items()}\n",
    "\n",
    "for name, scores in val_scores_dict.items():\n",
    "    # Define search bounds based on the validation set's own distribution.\n",
    "    # The optimal threshold should be in the upper tail of the distribution.\n",
    "    lower_bound = np.percentile(scores, 90)   # Start search at the 90th percentile of validation scores\n",
    "    upper_bound = np.percentile(scores, 100)  # End search at the maximum validation score\n",
    "\n",
    "    print(f\"\\n{name}: searching thresholds between {lower_bound:.4f} (val 90th %ile) and {upper_bound:.4f} (val 100th %ile)\")\n",
    "\n",
    "    # Find the best threshold for detecting the 'sus' target on the VALIDATION set\n",
    "    opt_metrics = optimize_threshold_for_target(\n",
    "        scores=scores, \n",
    "        y_true=y_sus_val,  # Using validation set labels\n",
    "        model_name=name,\n",
    "        lower_bound=lower_bound,\n",
    "        upper_bound=upper_bound,\n",
    "        metric='f1',\n",
    "        target_name='sus'\n",
    "    )\n",
    "    optimized_thresholds[name] = opt_metrics['threshold']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Optimized Thresholds (from Validation Set)\")\n",
    "print(\"=\"*60)\n",
    "print(optimized_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f16526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set using the optimized thresholds\n",
    "# Note: Thresholds were optimized on the validation set. \n",
    "# These are unbiased performance metrics on the unseen test set.\n",
    "test_results = get_predictions_all_models(\n",
    "    models, \n",
    "    X_test, \n",
    "    thresholds=optimized_thresholds\n",
    ")\n",
    "\n",
    "# Display a sample of the results\n",
    "for name, results in test_results.items():\n",
    "    print(f\"Test results for '{name}':\")\n",
    "    print(f\"  Predictions (first 10): {results['predictions'][:10]}\")\n",
    "    print(f\"  Scores (first 5): {results['scores'][:5]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7050a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute performance metrics for each model on both sus and evil targets\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, threshold in optimized_thresholds.items():\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Get predictions using the optimized threshold\n",
    "    preds = model.predict(X_test, threshold=threshold)\n",
    "    \n",
    "    # Metrics for 'sus' target\n",
    "    f1_sus = f1_score(y_sus_test, preds, zero_division=0)\n",
    "    precision_sus = precision_score(y_sus_test, preds, zero_division=0)\n",
    "    recall_sus = recall_score(y_sus_test, preds, zero_division=0)\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Target': 'sus',\n",
    "        'F1': f1_sus,\n",
    "        'Precision': precision_sus,\n",
    "        'Recall': recall_sus\n",
    "    })\n",
    "    \n",
    "    # Metrics for 'evil' target\n",
    "    f1_evil = f1_score(y_evil_test, preds, zero_division=0)\n",
    "    precision_evil = precision_score(y_evil_test, preds, zero_division=0)\n",
    "    recall_evil = recall_score(y_evil_test, preds, zero_division=0)\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Target': 'evil',\n",
    "        'F1': f1_evil,\n",
    "        'Precision': precision_evil,\n",
    "        'Recall': recall_evil\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nUnbiased Performance Metrics on Test Set:\")\n",
    "print(\"(Thresholds were optimized on the validation set)\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get predictions from DBSCAN using its native noise detection logic\n",
    "# We call .predict() without a threshold. The model's internal logic will\n",
    "# classify points as anomalous if their nearest neighbor in the training set was noise.\n",
    "dbscan_noise_preds = dbscan_model.predict(X_test, threshold=None)\n",
    "\n",
    "# Calculate metrics for 'sus' target\n",
    "f1_sus_db = f1_score(y_sus_test, dbscan_noise_preds, zero_division=0)\n",
    "precision_sus_db = precision_score(y_sus_test, dbscan_noise_preds, zero_division=0)\n",
    "recall_sus_db = recall_score(y_sus_test, dbscan_noise_preds, zero_division=0)\n",
    "\n",
    "# Calculate metrics for 'evil' target\n",
    "f1_evil_db = f1_score(y_evil_test, dbscan_noise_preds, zero_division=0)\n",
    "precision_evil_db = precision_score(y_evil_test, dbscan_noise_preds, zero_division=0)\n",
    "recall_evil_db = recall_score(y_evil_test, dbscan_noise_preds, zero_division=0)\n",
    "\n",
    "# Update the DataFrame\n",
    "# Find the index for the 'dbscan' model and 'sus' target\n",
    "sus_index = comparison_df[(comparison_df['Model'] == 'dbscan') & (comparison_df['Target'] == 'sus')].index\n",
    "comparison_df.loc[sus_index, ['F1', 'Precision', 'Recall']] = [f1_sus_db, precision_sus_db, recall_sus_db]\n",
    "\n",
    "# Find the index for the 'dbscan' model and 'evil' target\n",
    "evil_index = comparison_df[(comparison_df['Model'] == 'dbscan') & (comparison_df['Target'] == 'evil')].index\n",
    "comparison_df.loc[evil_index, ['F1', 'Precision', 'Recall']] = [f1_evil_db, precision_evil_db, recall_evil_db]\n",
    "\n",
    "print(\"Updated Performance Metrics for DBSCAN (using noise detection):\")\n",
    "display(comparison_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb880219",
   "metadata": {},
   "source": [
    "### 5.1. A Better Evaluation for DBSCAN\n",
    "\n",
    "The threshold optimization method works well for K-Means and GMM, but it's not ideal for DBSCAN. DBSCAN's primary strength is identifying noise points (samples that don't belong to any cluster), which it labels as `-1`. A better way to evaluate DBSCAN is to treat all noise points as anomalies.\n",
    "\n",
    "Let's calculate the metrics for DBSCAN using this more direct approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5665ce",
   "metadata": {},
   "source": [
    "## 5. Performance Summary\n",
    "\n",
    "**Methodology Recap:**\n",
    "- Models were trained on normal samples from the training set only (unsupervised learning).\n",
    "- Decision thresholds were optimized on the **validation set** to maximize F1-score.\n",
    "- Final, unbiased metrics were reported on the **unseen test set** using the optimized thresholds.\n",
    "\n",
    "This robust methodology ensures that the reported performance is a realistic estimate of how the models would perform on new, unseen data.\n",
    "The performance analysis above provides detailed metrics for detecting both \"sus\" and \"evil\" anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d954b220",
   "metadata": {},
   "source": [
    "## 6. Cluster Visualization with PCA\n",
    "\n",
    "Visualize clusters in 2D using PCA with similar color shades for normal samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ab35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying PCA for 2D visualization...\")\n",
    "\n",
    "# Note: After preprocessing, validation set has all samples as 'sus' (scaled values)\n",
    "# For PCA, we'll fit on a sample of the data instead of filtering by label\n",
    "print(\"Note: Validation set contains only anomalous samples after preprocessing.\")\n",
    "print(\"Fitting PCA on a sample of validation data for visualization...\\n\")\n",
    "\n",
    "# Take a random sample for PCA fitting (to save memory)\n",
    "sample_size = min(10000, X_val.shape[0])\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(X_val.shape[0], sample_size, replace=False)\n",
    "X_val_sample = X_val[sample_indices]\n",
    "\n",
    "# Convert sparse to dense for PCA\n",
    "print(f\"Converting {sample_size:,} sample points to dense for PCA...\")\n",
    "X_val_sample_dense = X_val_sample.toarray()\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca.fit(X_val_sample_dense)\n",
    "\n",
    "# Transform all validation data\n",
    "print(f\"Transforming all {X_val.shape[0]:,} validation samples...\")\n",
    "X_val_dense = X_val.toarray()\n",
    "X_val_pca = pca.transform(X_val_dense)\n",
    "\n",
    "print(f\"\\nPCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b0df8",
   "metadata": {},
   "source": [
    "### 6.1 K-Means Clusters in PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = kmeans_model.predict(X_val_dense)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "blues = plt.cm.Blues(np.linspace(0.3, 0.9, kmeans_model.n_clusters))\n",
    "\n",
    "for i in range(kmeans_model.n_clusters):\n",
    "    mask = cluster_labels == i\n",
    "    ax.scatter(X_val_pca[mask, 0], X_val_pca[mask, 1], \n",
    "               c=[blues[i]], alpha=0.6, s=30, \n",
    "               label=f'Cluster {i}', edgecolors='none')\n",
    "\n",
    "ax.set_xlabel('First Principal Component', fontsize=12)\n",
    "ax.set_ylabel('Second Principal Component', fontsize=12)\n",
    "ax.set_title('K-Means Clusters Visualized with PCA', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=9, ncol=2)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCluster sizes:\")\n",
    "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {label}: {count:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a76a3e3",
   "metadata": {},
   "source": [
    "### 6.2 DBSCAN Clusters in PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DBSCAN cluster labels (not using threshold, showing actual clusters)\n",
    "dbscan_labels = dbscan_model.predict(X_val_dense)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Separate noise points (label = -1) from cluster members\n",
    "noise_mask = dbscan_labels == -1\n",
    "cluster_mask = dbscan_labels != -1\n",
    "\n",
    "# Plot cluster members in steelblue\n",
    "ax.scatter(X_val_pca[cluster_mask, 0], X_val_pca[cluster_mask, 1],\n",
    "           c='steelblue', alpha=0.6, s=30, label='Clustered Points', edgecolors='none')\n",
    "\n",
    "# Plot noise/outliers as red X's\n",
    "ax.scatter(X_val_pca[noise_mask, 0], X_val_pca[noise_mask, 1],\n",
    "           c='darkred', alpha=0.8, s=40, label='Noise/Outliers', marker='x')\n",
    "\n",
    "ax.set_xlabel('First Principal Component', fontsize=12)\n",
    "ax.set_ylabel('Second Principal Component', fontsize=12)\n",
    "ax.set_title('DBSCAN Clusters Visualized with PCA', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDBSCAN clustering results:\")\n",
    "print(f\"  Clustered samples: {cluster_mask.sum():,}\")\n",
    "print(f\"  Noise/Outliers: {noise_mask.sum():,}\")\n",
    "print(f\"  Number of clusters: {len(set(dbscan_labels[cluster_mask]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc5bb9",
   "metadata": {},
   "source": [
    "### 6.3 GMM Components in PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_labels = gmm_model.predict(X_val_dense)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "greens = plt.cm.Greens(np.linspace(0.3, 0.9, gmm_model.n_components))\n",
    "\n",
    "for i in range(gmm_model.n_components):\n",
    "    mask = gmm_labels == i\n",
    "    ax.scatter(X_val_pca[mask, 0], X_val_pca[mask, 1],\n",
    "               c=[greens[i]], alpha=0.6, s=30,\n",
    "               label=f'Component {i}', edgecolors='none')\n",
    "\n",
    "ax.set_xlabel('First Principal Component', fontsize=12)\n",
    "ax.set_ylabel('Second Principal Component', fontsize=12)\n",
    "ax.set_title('GMM Components Visualized with PCA', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nComponent sizes:\")\n",
    "unique, counts = np.unique(gmm_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Component {label}: {count:,} samples\")\n",
    "print(f\"\\nComponent weights: {gmm_model.model.weights_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb476b73",
   "metadata": {},
   "source": [
    "## 7. Model Comparison\n",
    "\n",
    "Compare all three methods using the optimized thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sus_data = comparison_df[comparison_df['Target'] == 'sus']\n",
    "evil_data = comparison_df[comparison_df['Target'] == 'evil']\n",
    "\n",
    "x = np.arange(len(sus_data))\n",
    "width = 0.2\n",
    "\n",
    "metrics = ['F1', 'Precision', 'Recall']\n",
    "colors = ['steelblue', 'darkorange', 'green']\n",
    "\n",
    "for ax, data, target in zip(axs, [sus_data, evil_data], ['sus', 'evil']):\n",
    "    for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "        offset = width * (i - 1)\n",
    "        ax.bar(x + offset, data[metric], width, label=metric, color=color)\n",
    "    \n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title(f'Performance Comparison - {target.upper()}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(data['Model'])\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70feab8",
   "metadata": {},
   "source": [
    "## 8. Anomaly Score Distributions\n",
    "\n",
    "Visualize how anomaly scores differ between normal, sus, and evil samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Use test set labels for visualization\n",
    "normal_mask = (y_sus_test == 0) & (y_evil_test == 0)\n",
    "sus_mask = (y_sus_test == 1) & (y_evil_test == 0)\n",
    "evil_mask = (y_evil_test == 1)\n",
    "\n",
    "# Get test scores for visualization\n",
    "kmeans_test_scores = kmeans_model.decision_function(X_test)\n",
    "dbscan_test_scores = dbscan_model.decision_function(X_test)\n",
    "gmm_test_scores = gmm_model.decision_function(X_test)\n",
    "\n",
    "for ax, scores, name, model_key in zip(\n",
    "    axes,\n",
    "    [kmeans_test_scores, dbscan_test_scores, gmm_test_scores],\n",
    "    ['K-Means', 'DBSCAN', 'GMM'],\n",
    "    ['kmeans', 'dbscan', 'gmm']\n",
    "):\n",
    "    ax.hist(scores[normal_mask], bins=50, alpha=0.6, label='Normal', color='steelblue', density=True)\n",
    "    ax.hist(scores[sus_mask], bins=50, alpha=0.6, label='Sus', color='orange', density=True)\n",
    "    ax.hist(scores[evil_mask], bins=50, alpha=0.6, label='Evil', color='darkred', density=True)\n",
    "    \n",
    "    # Use the optimized threshold from our dictionary\n",
    "    threshold = optimized_thresholds[model_key]\n",
    "    ax.axvline(x=threshold, color='black', linestyle='--', linewidth=2, label='Threshold (sus)')\n",
    "    \n",
    "    ax.set_xlabel('Anomaly Score', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(f'{name} Score Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703a02b",
   "metadata": {},
   "source": [
    "### 8.1 Validation vs Test Score Comparison\n",
    "\n",
    "Let's visualize how the validation set scores (used for threshold bounds) compare to the test set scores (used for evaluation). This helps us understand why the validation-based threshold search works so well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f95667",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Get validation scores\n",
    "val_scores_dict = {\n",
    "    'kmeans': kmeans_model.decision_function(X_val),\n",
    "    'dbscan': dbscan_model.decision_function(X_val),\n",
    "    'gmm': gmm_model.decision_function(X_val)\n",
    "}\n",
    "\n",
    "# Get test scores (recreate in case it was overwritten)\n",
    "test_scores_dict = {\n",
    "    'kmeans': kmeans_model.decision_function(X_test),\n",
    "    'dbscan': dbscan_model.decision_function(X_test),\n",
    "    'gmm': gmm_model.decision_function(X_test)\n",
    "}\n",
    "\n",
    "for ax, (name, model_key) in zip(axes, [('K-Means', 'kmeans'), ('DBSCAN', 'dbscan'), ('GMM', 'gmm')]):\n",
    "    val_scores = val_scores_dict[model_key]\n",
    "    test_scores_arr = test_scores_dict[model_key]  # Get test scores for this model\n",
    "    threshold = optimized_thresholds[model_key]\n",
    "    \n",
    "    # Plot validation scores (normal samples)\n",
    "    ax.hist(val_scores, bins=50, alpha=0.5, label='Validation (normal)', color='steelblue', density=True)\n",
    "    \n",
    "    # Plot test scores separated by class\n",
    "    ax.hist(test_scores_arr[normal_mask], bins=50, alpha=0.4, label='Test (normal)', color='lightblue', density=True, linestyle='--')\n",
    "    ax.hist(test_scores_arr[sus_mask], bins=50, alpha=0.5, label='Test (sus)', color='orange', density=True)\n",
    "    ax.hist(test_scores_arr[evil_mask], bins=50, alpha=0.5, label='Test (evil)', color='darkred', density=True)\n",
    "    \n",
    "    # Mark the threshold search bounds\n",
    "    lower_bound = np.percentile(val_scores, 90)\n",
    "    upper_bound = np.percentile(val_scores, 100)\n",
    "    ax.axvline(x=lower_bound, color='green', linestyle=':', linewidth=2, label=f'Search lower (val 90th)')\n",
    "    ax.axvline(x=upper_bound, color='purple', linestyle=':', linewidth=2, label=f'Search upper (val max)')\n",
    "    \n",
    "    # Mark the optimized threshold\n",
    "    ax.axvline(x=threshold, color='black', linestyle='--', linewidth=2.5, label=f'Optimal threshold')\n",
    "    \n",
    "    ax.set_xlabel('Anomaly Score', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(f'{name}: Val vs Test Scores', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=8, loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # **IMPROVEMENT**: Clip the x-axis for better readability\n",
    "    # Combine all scores to find a reasonable limit that excludes extreme outliers\n",
    "    all_scores_for_plot = np.concatenate([val_scores, test_scores_arr])\n",
    "    # Calculate the 99.9th percentile to serve as a sensible upper limit\n",
    "    clip_limit = np.percentile(all_scores_for_plot, 99.9)\n",
    "    \n",
    "    # Only apply the limit if it's a meaningful value (i.e., > 0)\n",
    "    if clip_limit > 0:\n",
    "        # Set x-limit with a 5% padding\n",
    "        ax.set_xlim(-clip_limit * 0.05, clip_limit * 1.05)\n",
    "        # Add a text annotation to inform the viewer that the axis is clipped\n",
    "        ax.text(0.98, 0.98, 'X-axis clipped for readability',\n",
    "                transform=ax.transAxes, ha='right', va='top',\n",
    "                fontsize=8, bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THRESHOLD ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "for name, threshold in optimized_thresholds.items():\n",
    "    val_scores = val_scores_dict[name]\n",
    "    test_scores_arr = test_scores_dict[name]  # Use the dict we created above\n",
    "    \n",
    "    lower = np.percentile(val_scores, 90)\n",
    "    upper = np.percentile(val_scores, 100)\n",
    "    \n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Search range:      [{lower:.2f}, {upper:.2f}]\")\n",
    "    print(f\"  Optimal threshold: {threshold:.2f}\")\n",
    "    \n",
    "    # Handle cases where upper and lower bounds are the same to avoid division by zero\n",
    "    if upper > lower:\n",
    "        position = 100 * (threshold - lower) / (upper - lower)\n",
    "        print(f\"  Position:          {position:.1f}% through search range\")\n",
    "    else:\n",
    "        print(f\"  Position:          N/A (search range has zero width)\")\n",
    "\n",
    "    print(f\"  Val 90th %ile:     {np.percentile(val_scores, 90):.2f}\")\n",
    "    print(f\"  Val median:        {np.percentile(val_scores, 50):.2f}\")\n",
    "    print(f\"  Val max:           {np.percentile(val_scores, 100):.2f}\")\n",
    "    print(f\"  Test normal 90th:  {np.percentile(test_scores_arr[normal_mask], 90):.2f}\")\n",
    "    print(f\"  Test sus median:   {np.percentile(test_scores_arr[sus_mask], 50):.2f}\")\n",
    "    print(f\"  Test evil median:  {np.percentile(test_scores_arr[evil_mask], 50):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9a9c9",
   "metadata": {},
   "source": [
    "### 8.2 Box Plot Comparison\n",
    "\n",
    "A compact view of score distributions showing quartiles, outliers, and threshold placement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for ax, (name, model_key) in zip(axes, [('K-Means', 'kmeans'), ('DBSCAN', 'dbscan'), ('GMM', 'gmm')]):\n",
    "    val_scores = val_scores_dict[model_key]\n",
    "    test_scores_arr = test_scores_dict[model_key]\n",
    "    threshold = optimized_thresholds[model_key]\n",
    "    \n",
    "    # Prepare data for box plot\n",
    "    data_to_plot = [\n",
    "        val_scores,\n",
    "        test_scores_arr[normal_mask],\n",
    "        test_scores_arr[sus_mask],\n",
    "        test_scores_arr[evil_mask]\n",
    "    ]\n",
    "    \n",
    "    labels = ['Val\\n(normal)', 'Test\\n(normal)', 'Test\\n(sus)', 'Test\\n(evil)']\n",
    "    colors = ['steelblue', 'lightblue', 'orange', 'darkred']\n",
    "    \n",
    "    # Create box plot\n",
    "    bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True, \n",
    "                     showfliers=False,  # Hide outliers for cleaner view\n",
    "                     widths=0.6)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    # Add horizontal line for threshold\n",
    "    ax.axhline(y=threshold, color='black', linestyle='--', linewidth=2, \n",
    "               label=f'Threshold: {threshold:.1f}', zorder=10)\n",
    "    \n",
    "    # Add shaded region for threshold search bounds\n",
    "    lower = np.percentile(val_scores, 90)\n",
    "    upper = np.percentile(val_scores, 100)\n",
    "    ax.axhspan(lower, upper, alpha=0.1, color='green', label='Search range')\n",
    "    \n",
    "    ax.set_ylabel('Anomaly Score', fontsize=11)\n",
    "    ax.set_title(f'{name} Score Distributions', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9, loc='upper left')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # **IMPROVEMENT**: Clip the y-axis for better readability\n",
    "    # Find the maximum of the non-outlier range for the first three groups\n",
    "    # (val_normal, test_normal, test_sus) to set a sensible upper limit.\n",
    "    # The top whisker of a boxplot is typically Q3 + 1.5 * IQR.\n",
    "    q3_sus = np.percentile(test_scores_arr[sus_mask], 75)\n",
    "    q1_sus = np.percentile(test_scores_arr[sus_mask], 25)\n",
    "    iqr_sus = q3_sus - q1_sus\n",
    "    \n",
    "    # Set the upper limit to be slightly above the 'sus' group's upper whisker\n",
    "    # or the search range, whichever is higher.\n",
    "    clip_limit = max(q3_sus + 1.5 * iqr_sus, upper)\n",
    "\n",
    "    if clip_limit > 0:\n",
    "        # Set y-limit with a 10% padding for better spacing\n",
    "        ax.set_ylim(-clip_limit * 0.1, clip_limit * 1.1)\n",
    "        # Add a text annotation\n",
    "        ax.text(0.98, 0.98, 'Y-axis clipped for readability',\n",
    "                transform=ax.transAxes, ha='right', va='top',\n",
    "                fontsize=8, bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCORE DISTRIBUTION STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "for name, model_key in [('K-Means', 'kmeans'), ('DBSCAN', 'dbscan'), ('GMM', 'gmm')]:\n",
    "    val_scores = val_scores_dict[model_key]\n",
    "    test_scores_arr = test_scores_dict[model_key]  # Use the dict created in previous cell\n",
    "    threshold = optimized_thresholds[model_key]\n",
    "    \n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Validation (normal):  median={np.median(val_scores):.1f}, IQR=[{np.percentile(val_scores, 25):.1f}, {np.percentile(val_scores, 75):.1f}]\")\n",
    "    print(f\"  Test (normal):        median={np.median(test_scores_arr[normal_mask]):.1f}, IQR=[{np.percentile(test_scores_arr[normal_mask], 25):.1f}, {np.percentile(test_scores_arr[normal_mask], 75):.1f}]\")\n",
    "    print(f\"  Test (sus):           median={np.median(test_scores_arr[sus_mask]):.1f}, IQR=[{np.percentile(test_scores_arr[sus_mask], 25):.1f}, {np.percentile(test_scores_arr[sus_mask], 75):.1f}]\")\n",
    "    print(f\"  Test (evil):          median={np.median(test_scores_arr[evil_mask]):.1f}, IQR=[{np.percentile(test_scores_arr[evil_mask], 25):.1f}, {np.percentile(test_scores_arr[evil_mask], 75):.1f}]\")\n",
    "    print(f\"  Threshold:            {threshold:.1f}\")\n",
    "    \n",
    "    # Calculate separation only if the median is meaningful\n",
    "    if np.median(test_scores_arr[sus_mask]) > 0:\n",
    "        print(f\"  Separation (sus):     {(np.median(test_scores_arr[sus_mask]) - threshold):.1f} above threshold\")\n",
    "    if np.median(test_scores_arr[normal_mask]) > 0:\n",
    "        print(f\"  Separation (normal):  {(threshold - np.median(test_scores_arr[normal_mask])):.1f} above normal median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a42fa",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions\n",
    "\n",
    "### Complete Analysis Overview\n",
    "\n",
    "This notebook conducted a comprehensive unsupervised anomaly detection study using the BETH honeypot dataset with three clustering algorithms: K-Means, DBSCAN, and GMM. The analysis followed a robust, multi-stage workflow:\n",
    "\n",
    "1.  **Training**: Models were trained exclusively on `normal` system call data from the training set.\n",
    "2.  **Threshold Optimization**: Decision thresholds were optimized on the **validation set** to find the decision rule that best maximized the F1-score for detecting `sus` attacks.\n",
    "3.  **Performance Reporting**: Final, unbiased metrics were evaluated on the **unseen test set** using the thresholds derived from the validation set.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1.  **Model Training**: All three models (`KMeans`, `DBSCAN`, `GMM`) were successfully trained on normal data, establishing a baseline of normal behavior.\n",
    "\n",
    "2.  **Threshold Optimization**: By using the validation set, we were able to find effective decision thresholds for each model without introducing bias from the test set.\n",
    "\n",
    "3.  **Unbiased Performance Metrics**: The final metrics reported are a realistic measure of how these models would perform on new data, as the test set was held out from both training and threshold tuning.\n",
    "\n",
    "4.  **Visualization**:\n",
    "    *   **PCA plots** allowed for 2D visualization of high-dimensional cluster structures.\n",
    "    *   **Score distribution plots** clearly illustrated the separation between anomaly scores for `normal`, `sus`, and `evil` samples, validating the effectiveness of the models.\n",
    "\n",
    "### Critical Lessons Learned\n",
    "\n",
    "1.  **The Importance of a Validation Set**: A separate validation set is crucial for tuning model parameters--including post-processing rules like decision thresholds--without biasing the final evaluation.\n",
    "\n",
    "2.  **Thresholds are Key**: The performance of an unsupervised anomaly detection system is highly dependent on the choice of decision threshold. A model can be excellent at separating classes, but without a well-chosen threshold, its practical performance can be poor.\n",
    "\n",
    "3.  **Robust Evaluation is Possible even with Imbalance**: Despite the severe class imbalance, a sound train-validate-test methodology provides a path to generating trustworthy performance metrics.\n",
    "\n",
    "4.  **Modular Code Design**: By moving core modeling and analysis logic into the `src/models_unsupervised.py` module, the notebook is cleaner, more readable, and focused on orchestration rather than implementation details.\n",
    "\n",
    "### Final Recommendations\n",
    "\n",
    "Based on the unbiased performance metrics reported, a recommendation can be made for the best model for this specific task. The model with the best balance of F1-score, precision, and recall for the target anomalies (`sus` and `evil`) should be chosen for deployment. The final comparison table and plots provide all necessary information to make this data-driven decision.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}