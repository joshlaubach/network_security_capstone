{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb05e5a",
   "metadata": {},
   "source": [
    "# Network Security Capstone - Data Overview\n",
    "\n",
    "**Purpose:** Load and explore both BETH and UNSW-NB15 datasets to understand their structure, characteristics, and suitability for anomaly detection and classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Datasets:\n",
    "- **BETH Dataset**: Honeypot system call logs for unsupervised anomaly detection\n",
    "- **UNSW-NB15 Dataset**: Network traffic data for supervised attack classification\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Joshua Laubach  \n",
    "**Date:** October 27, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c5cae",
   "metadata": {},
   "source": [
    "## Project Setup\n",
    "\n",
    "**Purpose:** This section outlines the necessary steps to configure the environment for this notebook.\n",
    "\n",
    "### Prerequisites\n",
    "*   **Python Environment:** Python 3.8+ is recommended.\n",
    "*   **Dependencies:** All required packages are listed in `requirements.txt` in the project's root directory. Install them before proceeding:\n",
    "    ```bash\n",
    "    pip install -r ../../requirements.txt\n",
    "    ```\n",
    "\n",
    "### Project Structure\n",
    "This notebook is located in the `notebooks/` directory and depends on custom modules in the `src/` directory. The code automatically adds `src/` to the system path for seamless imports. The expected structure is:\n",
    "```\n",
    "network_security_capstone/\n",
    "|-- notebooks/\n",
    "|   `-- 01_data_overview.ipynb\n",
    "|-- src/\n",
    "|   `-- ... (custom modules)\n",
    "`-- requirements.txt\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690607a7",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "This exploratory data analysis examines two cybersecurity datasets for anomaly detection and attack classification modeling:\n",
    "\n",
    "### BETH Dataset - Key Findings\n",
    "- **Purpose**: Unsupervised anomaly detection on honeypot system call logs\n",
    "- **Class Imbalance**: Training set is ~99% normal (ideal for unsupervised learning)\n",
    "- **Data Quality**: No missing values, but high-cardinality text features require preprocessing\n",
    "- **Preprocessing Impact**: 10-15% memory reduction, log transforms handle skewed distributions\n",
    "- **Modeling Challenge**: Extremely rare anomalies in training data (~1% suspicious/evil combined)\n",
    "\n",
    "### UNSW-NB15 Dataset - Key Findings\n",
    "- **Purpose**: Supervised classification of network attacks (binary + multi-class)\n",
    "- **Class Distribution**: ~54% normal, ~46% attack (relatively balanced for supervised learning)\n",
    "- **Attack Diversity**: 9 distinct attack types (Fuzzers, DoS, Exploits, Reconnaissance, etc.)\n",
    "- **Missing Data**: Some null values present in raw data, handled via dropna in preprocessing\n",
    "- **Preprocessing Impact**: Categorical encoding enables ML, validation split created for tuning\n",
    "\n",
    "### Data Readiness Assessment\n",
    "- **Both datasets preprocessed and scaled** (Z-score normalization, mean~=0, std~=1)\n",
    "- **Outliers persist after preprocessing** but at manageable levels (z-score method shows 1-5% outliers)\n",
    "- **Feature engineering applied**: Log transforms, binary flags, categorical encoding\n",
    "- **Memory optimized**: 5-15% reduction through type optimization and column removal\n",
    "- **Validation splits available** for proper model evaluation\n",
    "\n",
    "### Recommended Modeling Approaches\n",
    "\n",
    "**BETH (Unsupervised Anomaly Detection):**\n",
    "- Autoencoder, Isolation Forest, One-Class SVM\n",
    "- Train on normal data only (training set)\n",
    "- Evaluate on test set with labeled anomalies\n",
    "\n",
    "**UNSW-NB15 (Supervised Classification):**\n",
    "- Binary: RandomForest, XGBoost, Neural Networks\n",
    "- Multi-class: Gradient Boosting, Deep Learning for attack type identification\n",
    "- Use validation set for hyperparameter tuning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3650dd44",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Import Required Libraries](#1-import-required-libraries)\n",
    "2. [Raw Data Analysis](#2-raw-data-analysis)\n",
    "   - 2.1 Load Raw Datasets\n",
    "   - 2.2 BETH Raw Data Overview\n",
    "   - 2.3 UNSW-NB15 Raw Data Overview\n",
    "   - 2.4 Target Distribution Across Splits\n",
    "   - 2.5 Feature Correlation Analysis\n",
    "   - 2.6 Note on Outlier Detection\n",
    "3. [Preprocessed Data Analysis](#3-preprocessed-data-analysis)\n",
    "   - 3.1 Load Preprocessed Datasets\n",
    "   - 3.2 BETH Preprocessed Data Overview\n",
    "   - 3.3 UNSW-NB15 Preprocessed Data Overview\n",
    "   - 3.4 Outlier Detection on Preprocessed Data\n",
    "4. [Data Comparison & Insights](#4-data-comparison--insights)\n",
    "   - 4.1 Before vs After Preprocessing\n",
    "   - 4.2 Key Insights & Preprocessing Impact\n",
    "5. [Summary](#5-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d33eb7",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f916445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the src directory to the Python path to allow for importing custom modules\n",
    "src_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Now you can import your custom modules\n",
    "from data_loading import load_beth, load_unsw\n",
    "from data_extraction import extract_beth, extract_unsw\n",
    "from preprocessing import preprocess_beth_split, preprocess_unsw_split\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cd0cf",
   "metadata": {},
   "source": [
    "# 2. Raw Data Analysis\n",
    "\n",
    "**Purpose**: Explore the **original unprocessed datasets** to understand raw data characteristics, distributions, and quality issues.\n",
    "\n",
    "This section analyzes data **before** any preprocessing, feature engineering, or transformations are applied.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Load Raw Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw unprocessed datasets for EDA\n",
    "beth_train_raw, beth_val_raw, beth_test_raw = extract_beth(save_to_disk=True)\n",
    "unsw_train_raw, unsw_test_raw = extract_unsw(save_to_disk=True)\n",
    "\n",
    "print(\"Raw datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16b2c4",
   "metadata": {},
   "source": [
    "## 2.2 BETH Raw Data Overview\n",
    "\n",
    "**BETH (BPF-Extended Telemetry for Honeypots)** - Original system call logs from honeypot systems.\n",
    "\n",
    "### Key Characteristics:\n",
    "- **Raw Timestamps**: Original timestamp values (not renamed or transformed)\n",
    "- **Text Columns**: JSON `args` column intact\n",
    "- **All Features**: processName, threadId, stackAddresses, eventName present\n",
    "- **No Encoding**: hostName as string, not encoded\n",
    "- **Targets**: `sus` (suspicious) and `evil` (malicious) labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b671695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETH RAW Dataset Analysis\n",
    "print(\"BETH DATASET - RAW DATA CHARACTERISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset Sizes:\")\n",
    "print(f\"  Training:   {beth_train_raw.shape}\")\n",
    "print(f\"  Validation: {beth_val_raw.shape}\")\n",
    "print(f\"  Test:       {beth_test_raw.shape}\")\n",
    "\n",
    "print(f\"\\nMemory Usage:\")\n",
    "print(f\"  Training:   {beth_train_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  Validation: {beth_val_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  Test:       {beth_test_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nColumn Names ({len(beth_train_raw.columns)} total):\")\n",
    "if len(beth_train_raw.columns) <= 10:\n",
    "    print(f\"  {list(beth_train_raw.columns)}\")\n",
    "else:\n",
    "    print(f\"  {list(beth_train_raw.columns[:10])} ... and {len(beth_train_raw.columns) - 10} more\")\n",
    "\n",
    "print(f\"\\nData Types:\")\n",
    "print(beth_train_raw.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nSample Rows:\")\n",
    "display(beth_train_raw.head(3).T)\n",
    "\n",
    "print(f\"\\nNull Values:\")\n",
    "null_counts = beth_train_raw.isnull().sum()\n",
    "if null_counts.sum() > 0:\n",
    "    print(null_counts[null_counts > 0])\n",
    "else:\n",
    "    print(\"  No null values\")\n",
    "\n",
    "print(f\"\\nTarget Distribution:\")\n",
    "print(f\"  Train Set (used for unsupervised learning):\")\n",
    "if 'sus' in beth_train_raw.columns and 'evil' in beth_train_raw.columns:\n",
    "    normal_train = ((beth_train_raw['sus']==0) & (beth_train_raw['evil']==0)).sum()\n",
    "    sus_train = (beth_train_raw['sus']==1).sum()\n",
    "    evil_train = (beth_train_raw['evil']==1).sum()\n",
    "    total_train = len(beth_train_raw)\n",
    "    print(f\"    Normal: {normal_train:,} ({100*normal_train/total_train:.2f}%)\")\n",
    "    print(f\"    Suspicious: {sus_train:,} ({100*sus_train/total_train:.2f}%)\")\n",
    "    print(f\"    Evil: {evil_train:,} ({100*evil_train/total_train:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Test Set (contains all anomalies for evaluation):\")\n",
    "if 'sus' in beth_test_raw.columns and 'evil' in beth_test_raw.columns:\n",
    "    normal_test = ((beth_test_raw['sus']==0) & (beth_test_raw['evil']==0)).sum()\n",
    "    sus_test = (beth_test_raw['sus']==1).sum()\n",
    "    evil_test = (beth_test_raw['evil']==1).sum()\n",
    "    total_test = len(beth_test_raw)\n",
    "    print(f\"    Normal: {normal_test:,} ({100*normal_test/total_test:.2f}%)\")\n",
    "    print(f\"    Suspicious: {sus_test:,} ({100*sus_test/total_test:.2f}%)\")\n",
    "    print(f\"    Evil: {evil_test:,} ({100*evil_test/total_test:.2f}%)\")\n",
    "\n",
    "print(f\"\\nNote: BETH has 2 binary target variables:\")\n",
    "print(f\"  - 'sus' (suspicious): anomalous behavior\")\n",
    "print(f\"  - 'evil' (malicious): confirmed malicious activity\")\n",
    "print(f\"  - Train/Val sets are ~99% normal for unsupervised anomaly detection training\")\n",
    "print(f\"  - Test set contains all anomalous samples for model evaluation\")\n",
    "\n",
    "print(f\"\\nNumeric vs Categorical Features:\")\n",
    "numeric_raw = beth_train_raw.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_raw = beth_train_raw.select_dtypes(include=['object']).columns\n",
    "print(f\"  Numeric: {len(numeric_raw)} columns\")\n",
    "print(f\"  Categorical: {len(categorical_raw)} columns\")\n",
    "print(f\"  Total: {len(beth_train_raw.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e561a0",
   "metadata": {},
   "source": [
    "## 2.3 UNSW-NB15 Raw Data Overview\n",
    "\n",
    "**UNSW-NB15** - Original network traffic dataset created by UNSW Canberra Cyber Range Lab.\n",
    "\n",
    "### Key Characteristics:\n",
    "- **Categorical Features**: proto, service, state as strings (not encoded)\n",
    "- **All 49 Original Features**: No dropped columns\n",
    "- **ID Column**: Present (row identifier)\n",
    "- **Missing Values**: May be present (not cleaned)\n",
    "- **Targets**: `label` (binary: 0=normal, 1=attack) and `attack_cat` (multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSW-NB15 RAW Dataset Analysis\n",
    "print(\"UNSW-NB15 DATASET - RAW DATA CHARACTERISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset Sizes:\")\n",
    "print(f\"  Training: {unsw_train_raw.shape}\")\n",
    "print(f\"  Test:     {unsw_test_raw.shape}\")\n",
    "\n",
    "print(f\"\\nMemory Usage:\")\n",
    "print(f\"  Training: {unsw_train_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  Test:     {unsw_test_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nColumn Names ({len(unsw_train_raw.columns)} total):\")\n",
    "if len(unsw_train_raw.columns) <= 10:\n",
    "    print(f\"  {list(unsw_train_raw.columns)}\")\n",
    "else:\n",
    "    print(f\"  {list(unsw_train_raw.columns[:10])} ... and {len(unsw_train_raw.columns) - 10} more\")\n",
    "\n",
    "print(f\"\\nData Types:\")\n",
    "print(unsw_train_raw.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nSample Rows:\")\n",
    "display(unsw_train_raw.head(3).T)\n",
    "\n",
    "print(f\"\\nNull Values:\")\n",
    "null_counts = unsw_train_raw.isnull().sum()\n",
    "if null_counts.sum() > 0:\n",
    "    print(null_counts[null_counts > 0])\n",
    "else:\n",
    "    print(\"  No null values\")\n",
    "\n",
    "print(f\"\\nTarget Distribution (Train Set):\")\n",
    "if 'label' in unsw_train_raw.columns:\n",
    "    normal = (unsw_train_raw['label'] == 0).sum()\n",
    "    attack = (unsw_train_raw['label'] == 1).sum()\n",
    "    total = len(unsw_train_raw)\n",
    "    print(f\"  Binary Label ('label'):\")\n",
    "    print(f\"    Normal (0): {normal:,} ({100*normal/total:.2f}%)\")\n",
    "    print(f\"    Attack (1): {attack:,} ({100*attack/total:.2f}%)\")\n",
    "\n",
    "print(f\"\\nNote: UNSW-NB15 has 2 target variables:\")\n",
    "print(f\"  - 'label': Binary (0=Normal, 1=Attack) - derived as: 0 if attack_cat=='Normal' else 1\")\n",
    "print(f\"  - 'attack_cat': Multi-class with 10 categories (1 Normal + 9 attack types)\")\n",
    "\n",
    "print(f\"\\nTop 10 Attack Categories ('attack_cat'):\")\n",
    "for idx, cat in enumerate(unsw_train_raw['attack_cat'].value_counts().head(10).index):\n",
    "    count = (unsw_train_raw['attack_cat'] == cat).sum()\n",
    "    is_normal = \"<- Normal class\" if cat == 'Normal' else \"<- Attack type\"\n",
    "    print(f\"  {idx+1}. {cat:20s} : {count:8,} samples {is_normal}\")\n",
    "\n",
    "print(f\"\\nNumeric vs Categorical Features:\")\n",
    "numeric_unsw_raw = unsw_train_raw.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_unsw_raw = unsw_train_raw.select_dtypes(include=['object']).columns\n",
    "print(f\"  Numeric: {len(numeric_unsw_raw)} columns\")\n",
    "print(f\"  Categorical: {len(categorical_unsw_raw)} columns\")\n",
    "print(f\"  Total: {len(unsw_train_raw.columns)} columns\")\n",
    "\n",
    "# Define numeric features for UNSW-NB15\n",
    "unsw_numeric = unsw_train_raw[numeric_unsw_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd3bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize UNSW-NB15 Raw Data Distributions\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axs = axs.flatten()\n",
    "fig.suptitle('UNSW-NB15 Raw Data - Feature Distributions', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Select 6 numeric features with highest variance\n",
    "unsw_numeric_features = [col for col in numeric_unsw_raw if col not in ['label', 'id']]\n",
    "variances_unsw = unsw_train_raw[unsw_numeric_features].var().sort_values(ascending=False)\n",
    "top_6_unsw = variances_unsw.head(6).index.tolist()\n",
    "\n",
    "for idx, feature in enumerate(top_6_unsw):\n",
    "    # Histogram\n",
    "    axs[idx].hist(unsw_train_raw[feature].dropna(), bins=50, alpha=0.7, \n",
    "            color='coral', edgecolor='black')\n",
    "    axs[idx].set_xlabel(feature, fontsize=11, fontweight='bold')\n",
    "    axs[idx].set_ylabel('Frequency', fontsize=11)\n",
    "    axs[idx].set_title(f'Distribution: {feature}', fontsize=12, fontweight='bold')\n",
    "    axs[idx].grid(alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = unsw_train_raw[feature].mean()\n",
    "    median_val = unsw_train_raw[feature].median()\n",
    "    axs[idx].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2e}')\n",
    "    axs[idx].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2e}')\n",
    "    axs[idx].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Displayed raw distributions for top 6 features by variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf68a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Source-Destination Relationships\n",
    "fig, axs = plt.subplots(3, 3, figsize=(18, 16))\n",
    "axs = axs.flatten()\n",
    "fig.suptitle('UNSW-NB15: Source-Destination Patterns by Attack Label', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Sample data for visualization\n",
    "sample_size = len(unsw_train_raw)\n",
    "sample_indices = np.random.choice(len(unsw_train_raw), sample_size, replace=False)\n",
    "sample_data = unsw_train_raw.iloc[sample_indices]\n",
    "\n",
    "# Define scatter plot pairs\n",
    "scatter_pairs = [\n",
    "    ('sbytes', 'dbytes', 'Source Bytes vs Destination Bytes'),\n",
    "    ('spkts', 'dpkts', 'Source Packets vs Destination Packets'),\n",
    "    ('sload', 'dload', 'Source Load vs Destination Load'),\n",
    "    ('sloss', 'dloss', 'Source Loss vs Destination Loss'),\n",
    "    ('sinpkt', 'dinpkt', 'Source Inter-Packet Time vs Dest IPT'),\n",
    "    ('sjit', 'djit', 'Source Jitter vs Destination Jitter'),\n",
    "    ('smean', 'dmean', 'Source Mean Packet Size vs Dest Mean'),\n",
    "    ('swin', 'dwin', 'Source TCP Window vs Dest TCP Window')\n",
    "]\n",
    "\n",
    "# Create scatter plots\n",
    "for idx, (x_col, y_col, title) in enumerate(scatter_pairs):\n",
    "    if x_col not in sample_data.columns or y_col not in sample_data.columns:\n",
    "        axs[idx].text(0.5, 0.5, f'Columns {x_col}/{y_col} not found', \n",
    "                     ha='center', va='center', fontsize=12)\n",
    "        axs[idx].set_title(title, fontsize=11, fontweight='bold')\n",
    "        continue\n",
    "    \n",
    "    normal_mask = sample_data['label'] == 0\n",
    "    attack_mask = sample_data['label'] == 1\n",
    "    \n",
    "    # Plot normal traffic\n",
    "    axs[idx].scatter(sample_data.loc[normal_mask, x_col], \n",
    "                    sample_data.loc[normal_mask, y_col],\n",
    "                    alpha=0.8, s=15, c='steelblue', label='Normal', edgecolors='none')\n",
    "    \n",
    "    # Plot attack traffic\n",
    "    axs[idx].scatter(sample_data.loc[attack_mask, x_col], \n",
    "                    sample_data.loc[attack_mask, y_col],\n",
    "                    alpha=0.3, s=15, c='coral', label='Attack', edgecolors='none')\n",
    "    \n",
    "    axs[idx].set_xlabel(x_col, fontsize=10, fontweight='bold')\n",
    "    axs[idx].set_ylabel(y_col, fontsize=10, fontweight='bold')\n",
    "    axs[idx].set_title(title, fontsize=11, fontweight='bold')\n",
    "    axs[idx].legend(loc='upper right', fontsize=8)\n",
    "    axs[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add diagonal reference line\n",
    "    x_max = sample_data[x_col].max()\n",
    "    y_max = sample_data[y_col].max()\n",
    "    if x_max > 0 and y_max > 0:\n",
    "        max_val = max(x_max, y_max)\n",
    "        axs[idx].plot([0, max_val], [0, max_val], 'k--', alpha=0.3, linewidth=1)\n",
    "\n",
    "axs[8].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eaf6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Log-Transformed Source-Destination Relationships\n",
    "fig, axs = plt.subplots(3, 3, figsize=(18, 16))\n",
    "axs = axs.flatten()\n",
    "fig.suptitle('UNSW-NB15: Log-Transformed Source-Destination Patterns', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "sample_data_log = sample_data.copy()\n",
    "\n",
    "for idx, (x_col, y_col, title) in enumerate(scatter_pairs):\n",
    "    if x_col not in sample_data_log.columns or y_col not in sample_data_log.columns:\n",
    "        axs[idx].text(0.5, 0.5, f'Columns {x_col}/{y_col} not found', \n",
    "                     ha='center', va='center', fontsize=12)\n",
    "        axs[idx].set_title(f'Log: {title}', fontsize=11, fontweight='bold')\n",
    "        continue\n",
    "    \n",
    "    # Apply log1p transformation\n",
    "    log_x = np.log1p(sample_data_log[x_col])\n",
    "    log_y = np.log1p(sample_data_log[y_col])\n",
    "    \n",
    "    normal_mask = sample_data_log['label'] == 0\n",
    "    attack_mask = sample_data_log['label'] == 1\n",
    "    \n",
    "    axs[idx].scatter(log_x[normal_mask], log_y[normal_mask],\n",
    "                    alpha=0.8, s=15, c='steelblue', label='Normal', edgecolors='none')\n",
    "    axs[idx].scatter(log_x[attack_mask], log_y[attack_mask],\n",
    "                    alpha=0.3, s=15, c='coral', label='Attack', edgecolors='none')\n",
    "\n",
    "    axs[idx].set_xlabel(f'$\\\\log(1 + {x_col})$', fontsize=10, fontweight='bold')\n",
    "    axs[idx].set_ylabel(f'$\\\\log(1 + {y_col})$', fontsize=10, fontweight='bold')\n",
    "    axs[idx].set_title(f'Log: {title}', fontsize=11, fontweight='bold')\n",
    "    axs[idx].legend(loc='upper right', fontsize=8)\n",
    "    axs[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add diagonal reference line\n",
    "    x_max = log_x.max()\n",
    "    y_max = log_y.max()\n",
    "    if x_max > 0 and y_max > 0:\n",
    "        max_val = max(x_max, y_max)\n",
    "        axs[idx].plot([0, max_val], [0, max_val], 'k--', alpha=0.3, linewidth=1)\n",
    "\n",
    "axs[8].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cac4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSW-NB15 Target Distribution (Binary + Multi-class)\n",
    "print(\"\\n=== UNSW-NB15 Target Variables ===\")\n",
    "print(\"\\nBinary Target ('label' - derived from 'attack_cat'):\")\n",
    "print(unsw_train_raw['label'].value_counts().sort_index())\n",
    "print(f\"Attack ratio: {unsw_train_raw['label'].mean():.2%}\")\n",
    "\n",
    "print(\"\\nMulti-class Target ('attack_cat' - primary classification target):\")\n",
    "attack_counts = unsw_train_raw['attack_cat'].value_counts()\n",
    "for cat, count in attack_counts.items():\n",
    "    pct = count / len(unsw_train_raw) * 100\n",
    "    cat_type = \"Normal class\" if cat == 'Normal' else \"Attack type\"\n",
    "    print(f\"  {cat:20s}: {count:8,} ({pct:5.2f}%)  [{cat_type}]\")\n",
    "\n",
    "print(\"\\nMODELING STRATEGIES:\")\n",
    "print(\"  1. Binary Classification:\")\n",
    "print(\"     -> Use 'label' as target\")\n",
    "print(\"     -> Simpler problem, higher accuracy expected\")\n",
    "print()\n",
    "print(\"  2. Multi-class Classification:\")\n",
    "print(\"     -> Use 'attack_cat' as target\")\n",
    "print(\"     -> More challenging, provides attack-specific insights\")\n",
    "print(\"     -> Can identify specific attack types (Fuzzers, DoS, Exploits, etc.)\")\n",
    "\n",
    "# Target Distribution Comparison\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# BETH target distribution\n",
    "axs[0].pie([beth_train_raw['sus'].sum(), len(beth_train_raw) - beth_train_raw['sus'].sum()],\n",
    "           labels=['Suspicious', 'Normal'], autopct='%1.1f%%', startangle=90)\n",
    "axs[0].set_title('BETH: Suspicious vs Normal (Train Set)')\n",
    "\n",
    "# UNSW target distribution\n",
    "axs[1].pie([unsw_train_raw['label'].sum(), len(unsw_train_raw) - unsw_train_raw['label'].sum()],\n",
    "           labels=['Attack', 'Normal'], autopct='%1.1f%%', startangle=90)\n",
    "axs[1].set_title('UNSW-NB15: Attack vs Normal (Train Set)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276affd",
   "metadata": {},
   "source": [
    "# 3. Preprocessed Data Analysis\n",
    "\n",
    "**Purpose**: Explore the **preprocessed datasets** after all transformations, feature engineering, and scaling.\n",
    "\n",
    "This section analyzes data **after** the complete preprocessing pipeline to understand how the data has been transformed for modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Load Preprocessed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da92ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed datasets for modeling\n",
    "beth_train, beth_val, beth_test = load_beth(tfidf=False, save_to_disk=True, verbose=False)\n",
    "unsw_train, unsw_val, unsw_test = load_unsw(split_test=True, save_to_disk=True, verbose=False)\n",
    "\n",
    "print(\"Preprocessed datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1f27f",
   "metadata": {},
   "source": [
    "## 3.2 BETH Preprocessed Data Overview\n",
    "\n",
    "**After Preprocessing Pipeline:**\n",
    "- `timestamp` renamed to `timeSinceBoot`\n",
    "- Log transforms applied (timeSinceBoot, processId, parentProcessId)\n",
    "- Feature flags created (*_flag columns)\n",
    "- Text columns dropped (processName, threadId, stackAddresses, eventName)\n",
    "- `hostName` encoded to integers\n",
    "- Numeric features scaled (StandardScaler)\n",
    "- Binary targets (`sus`, `evil`) preserved as {0, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3309430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETH PREPROCESSED Dataset Analysis\n",
    "print(\"BETH DATASET - PREPROCESSED DATA CHARACTERISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset Sizes:\")\n",
    "print(f\"  Training:   {beth_train.shape}\")\n",
    "print(f\"  Validation: {beth_val.shape}\")\n",
    "print(f\"  Test:       {beth_test.shape}\")\n",
    "\n",
    "print(f\"\\nColumns:\")\n",
    "print(f\"  {list(beth_train.columns)}\")\n",
    "\n",
    "print(f\"\\nSample Rows:\")\n",
    "print(beth_train.head(3))\n",
    "\n",
    "print(f\"\\nBinary Targets:\")\n",
    "print(f\"  sus unique values:  {sorted(beth_train['sus'].unique())}\")\n",
    "print(f\"  evil unique values: {sorted(beth_train['evil'].unique())}\")\n",
    "\n",
    "print(f\"\\nLog-Transformed Features:\")\n",
    "log_features = [col for col in beth_train.columns if col.startswith('log_')]\n",
    "print(f\"  {log_features}\")\n",
    "\n",
    "print(f\"\\nFeature Flags:\")\n",
    "flag_features = [col for col in beth_train.columns if col.endswith('_flag')]\n",
    "print(f\"  {flag_features}\")\n",
    "\n",
    "print(f\"\\nNumeric Features:\")\n",
    "numeric_preprocessed = beth_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_preprocessed = [c for c in numeric_preprocessed if c not in ['sus', 'evil']]\n",
    "print(f\"  Total scaled features: {len(numeric_preprocessed)}\")\n",
    "\n",
    "print(f\"\\nScaling Verification (mean~0, std~1):\")\n",
    "for col in numeric_preprocessed[:3]:\n",
    "    mean_val = beth_train[col].mean()\n",
    "    std_val = beth_train[col].std()\n",
    "    print(f\"  {col:30s}: mean={mean_val:7.4f}, std={std_val:6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66880c",
   "metadata": {},
   "source": [
    "## 3.3 UNSW-NB15 Preprocessed Data Overview\n",
    "\n",
    "**After Preprocessing Pipeline:**\n",
    "- Test set split in half (validation + test)\n",
    "- Categorical encoding (proto, service, state to integers)\n",
    "- `id` column dropped\n",
    "- Missing values removed (dropna)\n",
    "- Feature engineering applied\n",
    "- Log transforms applied to skewed features\n",
    "- Numeric features scaled (fit on train, transform on val/test)\n",
    "- Binary targets (`label`, `attack_cat`) preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSW-NB15 PREPROCESSED Dataset Analysis\n",
    "print(\"UNSW-NB15 DATASET - PREPROCESSED DATA CHARACTERISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset Sizes:\")\n",
    "print(f\"  Training:   {unsw_train.shape}\")\n",
    "print(f\"  Validation: {unsw_val.shape}\")\n",
    "print(f\"  Test:       {unsw_test.shape}\")\n",
    "\n",
    "print(f\"\\nColumns (first 20):\")\n",
    "print(f\"  {list(unsw_train.columns[:20])}\")\n",
    "print(f\"  ... and {len(unsw_train.columns) - 20} more columns\")\n",
    "\n",
    "print(f\"\\nSample Rows:\")\n",
    "print(unsw_train.head(3))\n",
    "\n",
    "print(f\"\\nBinary Label:\")\n",
    "print(f\"  label unique values: {sorted(unsw_train['label'].unique())}\")\n",
    "\n",
    "print(f\"\\nTarget Distributions:\")\n",
    "label_dist_proc = unsw_train['label'].value_counts()\n",
    "print(f\"  Normal (0): {label_dist_proc[0]:,} ({100*label_dist_proc[0]/len(unsw_train):.2f}%)\")\n",
    "print(f\"  Attack (1): {label_dist_proc[1]:,} ({100*label_dist_proc[1]/len(unsw_train):.2f}%)\")\n",
    "\n",
    "print(f\"\\nCategorical Encoding:\")\n",
    "if 'proto' in unsw_train.columns:\n",
    "    print(f\"  proto: {unsw_train['proto'].dtype}, {unsw_train['proto'].nunique()} unique values\")\n",
    "if 'service' in unsw_train.columns:\n",
    "    print(f\"  service: {unsw_train['service'].dtype}, {unsw_train['service'].nunique()} unique values\")\n",
    "if 'state' in unsw_train.columns:\n",
    "    print(f\"  state: {unsw_train['state'].dtype}, {unsw_train['state'].nunique()} unique values\")\n",
    "\n",
    "print(f\"\\nNumeric Features:\")\n",
    "numeric_unsw_proc = unsw_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_unsw_proc = [c for c in numeric_unsw_proc if c not in ['label']]\n",
    "print(f\"  Total features: {len(numeric_unsw_proc)}\")\n",
    "\n",
    "print(f\"\\nScaling Verification (mean~0, std~1):\")\n",
    "for col in numeric_unsw_proc[:5]:\n",
    "    mean_val = unsw_train[col].mean()\n",
    "    std_val = unsw_train[col].std()\n",
    "    print(f\"  {col:30s}: mean={mean_val:7.4f}, std={std_val:6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30a954",
   "metadata": {},
   "source": [
    "## 3.4 Outlier Detection on Preprocessed Data\n",
    "\n",
    "**Purpose**: Identify outliers in the **transformed feature space** that models will actually see during training.\n",
    "\n",
    "**Why After Preprocessing?**\n",
    "- Scaling changes outlier definitions (z-scores meaningful after standardization)\n",
    "- Log transforms compress extreme values\n",
    "- Models train on this data, not raw values\n",
    "- Need to assess if outliers persist after transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c10a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSW: Enhanced Outlier Detection on Preprocessed Data\n",
    "from scipy import stats\n",
    "\n",
    "# Select top 4 features by variance from preprocessed data\n",
    "unsw_preproc_numeric = unsw_train.select_dtypes(include=[np.number])\n",
    "unsw_preproc_features = [col for col in unsw_preproc_numeric.columns if col not in ['label']]\n",
    "variances_unsw_preproc = unsw_preproc_numeric[unsw_preproc_features].var().sort_values(ascending=False)\n",
    "unsw_outlier_features_preproc = variances_unsw_preproc.head(4).index.tolist()\n",
    "\n",
    "# Visualization 1: Violin Plots (shows distribution + outliers)\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axs = axs.flatten()\n",
    "fig.suptitle('UNSW-NB15 (Preprocessed): Outlier Detection - Violin Plots', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, feature in enumerate(unsw_outlier_features_preproc):\n",
    "    data_normal = unsw_train.loc[unsw_train['label'] == 0, feature].dropna()\n",
    "    data_attack = unsw_train.loc[unsw_train['label'] == 1, feature].dropna()\n",
    "    \n",
    "    # Create violin plot for both classes\n",
    "    parts = axs[idx].violinplot([data_normal, data_attack], \n",
    "                                 positions=[1, 2], \n",
    "                                 showmeans=True, \n",
    "                                 showmedians=True,\n",
    "                                 widths=0.7)\n",
    "    \n",
    "    # Color the violins\n",
    "    for pc, color in zip(parts['bodies'], ['steelblue', 'coral']):\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    # Calculate outlier percentages using z-score method (appropriate for scaled data)\n",
    "    z_scores_n = np.abs(stats.zscore(data_normal))\n",
    "    outliers_n = data_normal[z_scores_n > 3]  # 3 standard deviations\n",
    "    \n",
    "    z_scores_a = np.abs(stats.zscore(data_attack))\n",
    "    outliers_a = data_attack[z_scores_a > 3]\n",
    "    \n",
    "    axs[idx].set_xticks([1, 2])\n",
    "    axs[idx].set_xticklabels(['Normal', 'Attack'])\n",
    "    axs[idx].set_ylabel('Scaled Value', fontsize=10, fontweight='bold')\n",
    "    axs[idx].set_title(f'{feature}\\nNormal: {len(outliers_n)} ({100*len(outliers_n)/len(data_normal):.1f}%) | '\n",
    "                      f'Attack: {len(outliers_a)} ({100*len(outliers_a)/len(data_attack):.1f}%)',\n",
    "                      fontsize=11, fontweight='bold')\n",
    "    axs[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add reference lines for +-3 sigma\n",
    "    axs[idx].axhline(3, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    axs[idx].axhline(-3, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Z-Score Distribution\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axs = axs.flatten()\n",
    "fig.suptitle('UNSW-NB15 (Preprocessed): Z-Score Distribution ($|z| > 3$ = Outlier)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, feature in enumerate(unsw_outlier_features_preproc):\n",
    "    data = unsw_train[feature].dropna()\n",
    "    \n",
    "    # Calculate z-scores\n",
    "    z_scores = stats.zscore(data)\n",
    "    \n",
    "    # Separate inliers and outliers\n",
    "    inliers = z_scores[np.abs(z_scores) <= 3]\n",
    "    outliers = z_scores[np.abs(z_scores) > 3]\n",
    "    \n",
    "    # Plot histogram\n",
    "    axs[idx].hist(inliers, bins=50, alpha=0.7, color='steelblue', \n",
    "                 label=f'Inliers ({len(inliers):,})', edgecolor='black')\n",
    "    axs[idx].hist(outliers, bins=30, alpha=0.9, color='red', \n",
    "                 label=f'Outliers ({len(outliers):,})', edgecolor='black')\n",
    "    \n",
    "    # Add +-3 sigma boundary lines\n",
    "    axs[idx].axvline(-3, color='orange', linestyle='--', linewidth=2, label='3$\\\\sigma$ Threshold')\n",
    "    axs[idx].axvline(3, color='orange', linestyle='--', linewidth=2)\n",
    "    \n",
    "    # Add +-1 sigma and +-2 sigma reference lines\n",
    "    axs[idx].axvline(-2, color='green', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    axs[idx].axvline(2, color='green', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    axs[idx].axvline(-1, color='gray', linestyle=':', linewidth=1, alpha=0.3)\n",
    "    axs[idx].axvline(1, color='gray', linestyle=':', linewidth=1, alpha=0.3)\n",
    "    \n",
    "    axs[idx].set_xlabel('Z-Score', fontsize=10, fontweight='bold')\n",
    "    axs[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axs[idx].set_title(f'{feature}\\nOutliers: {100*len(outliers)/len(z_scores):.2f}%',\n",
    "                      fontsize=11, fontweight='bold')\n",
    "    axs[idx].legend(fontsize=8, loc='upper right')\n",
    "    axs[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed outlier summary\n",
    "print(\"\\nUNSW-NB15 PREPROCESSED DATA - OUTLIER ANALYSIS (Z-Score Method)\")\n",
    "print(\"=\" * 95)\n",
    "print(f\"{'Feature':<25} {'Total':<10} {'Outliers':<10} {'%':<8} {'Normal Out':<12} {'Attack Out':<12}\")\n",
    "print(\"=\" * 95)\n",
    "\n",
    "for feature in unsw_outlier_features_preproc:\n",
    "    data = unsw_train[feature].dropna()\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    outlier_mask = z_scores > 3\n",
    "    \n",
    "    # Split by label\n",
    "    normal_mask = unsw_train['label'] == 0\n",
    "    attack_mask = unsw_train['label'] == 1\n",
    "    \n",
    "    normal_outliers = unsw_train.loc[normal_mask & outlier_mask, feature]\n",
    "    attack_outliers = unsw_train.loc[attack_mask & outlier_mask, feature]\n",
    "    total_outliers = outlier_mask.sum()\n",
    "    \n",
    "    print(f\"{feature:<25} {len(data):<10,} {total_outliers:<10,} {100*total_outliers/len(data):<7.2f}% \"\n",
    "          f\"{len(normal_outliers):<12,} {len(attack_outliers):<12,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 95)\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"  - Using Z-Score method ($|z| > 3$) - appropriate for scaled/normalized data\")\n",
    "print(\"  - After preprocessing, outliers should be significantly reduced\")\n",
    "print(\"  - Compare outlier rates: do attacks have more extreme z-scores?\")\n",
    "print(\"  - Low outlier percentage indicates successful preprocessing/scaling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETH: Enhanced Outlier Detection on Preprocessed Data\n",
    "# Select top 4 features by variance from preprocessed data\n",
    "beth_preproc_numeric = beth_train.select_dtypes(include=[np.number])\n",
    "beth_preproc_features = [col for col in beth_preproc_numeric.columns if col not in ['sus', 'evil']]\n",
    "variances_beth_preproc = beth_preproc_numeric[beth_preproc_features].var().sort_values(ascending=False)\n",
    "beth_outlier_features_preproc = variances_beth_preproc.head(4).index.tolist()\n",
    "\n",
    "# Visualization 1: Violin Plots showing distribution by anomaly label\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axs = axs.flatten()\n",
    "fig.suptitle('BETH (Preprocessed): Outlier Detection - Violin Plots by Label', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, feature in enumerate(beth_outlier_features_preproc):\n",
    "    # Split by sus label (0=normal, 1=suspicious)\n",
    "    data_normal = beth_train.loc[beth_train['sus'] == 0, feature].dropna()\n",
    "    data_sus = beth_train.loc[beth_train['sus'] == 1, feature].dropna()\n",
    "    \n",
    "    # Create violin plot\n",
    "    parts = axs[idx].violinplot([data_normal, data_sus], \n",
    "                                 positions=[1, 2], \n",
    "                                 showmeans=True, \n",
    "                                 showmedians=True,\n",
    "                                 widths=0.7)\n",
    "    \n",
    "    # Color the violins\n",
    "    for pc, color in zip(parts['bodies'], ['steelblue', 'coral']):\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    # Calculate outlier percentages using z-score method\n",
    "    z_scores_n = np.abs(stats.zscore(data_normal))\n",
    "    outliers_n = data_normal[z_scores_n > 3]\n",
    "    \n",
    "    z_scores_s = np.abs(stats.zscore(data_sus))\n",
    "    outliers_s = data_sus[z_scores_s > 3]\n",
    "    \n",
    "    axs[idx].set_xticks([1, 2])\n",
    "    axs[idx].set_xticklabels(['Normal', 'Suspicious'])\n",
    "    axs[idx].set_ylabel('Scaled Value', fontsize=10, fontweight='bold')\n",
    "    axs[idx].set_title(f'{feature}\\nNormal: {len(outliers_n)} ({100*len(outliers_n)/len(data_normal):.1f}%) | '\n",
    "                      f'Suspicious: {len(outliers_s)} ({100*len(outliers_s)/len(data_sus):.1f}%)',\n",
    "                      fontsize=11, fontweight='bold')\n",
    "    axs[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add reference lines for +-3 sigma\n",
    "    axs[idx].axhline(3, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    axs[idx].axhline(-3, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Z-Score Distribution\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axs = axs.flatten()\n",
    "fig.suptitle('BETH (Preprocessed): Z-Score Distribution ($|z| > 3$ = Outlier)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, feature in enumerate(beth_outlier_features_preproc):\n",
    "    data = beth_train[feature].dropna()\n",
    "    \n",
    "    # Calculate z-scores\n",
    "    z_scores = stats.zscore(data)\n",
    "    \n",
    "    # Separate inliers and outliers\n",
    "    inliers = z_scores[np.abs(z_scores) <= 3]\n",
    "    outliers = z_scores[np.abs(z_scores) > 3]\n",
    "    \n",
    "    # Plot histogram\n",
    "    axs[idx].hist(inliers, bins=50, alpha=0.7, color='steelblue', \n",
    "                 label=f'Inliers ({len(inliers):,})', edgecolor='black')\n",
    "    axs[idx].hist(outliers, bins=30, alpha=0.9, color='red', \n",
    "                 label=f'Outliers ({len(outliers):,})', edgecolor='black')\n",
    "    \n",
    "    # Add +-3 sigma boundary lines\n",
    "    axs[idx].axvline(-3, color='orange', linestyle='--', linewidth=2, label='3$\\\\sigma$ Threshold')\n",
    "    axs[idx].axvline(3, color='orange', linestyle='--', linewidth=2)\n",
    "    \n",
    "    # Add +-1 sigma and +-2 sigma reference lines\n",
    "    axs[idx].axvline(-2, color='green', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    axs[idx].axvline(2, color='green', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    axs[idx].axvline(-1, color='gray', linestyle=':', linewidth=1, alpha=0.3)\n",
    "    axs[idx].axvline(1, color='gray', linestyle=':', linewidth=1, alpha=0.3)\n",
    "    \n",
    "    axs[idx].set_xlabel('Z-Score', fontsize=10, fontweight='bold')\n",
    "    axs[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axs[idx].set_title(f'{feature}\\nOutliers: {100*len(outliers)/len(z_scores):.2f}%',\n",
    "                      fontsize=11, fontweight='bold')\n",
    "    axs[idx].legend(fontsize=8, loc='upper right')\n",
    "    axs[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed outlier summary\n",
    "print(\"\\nBETH PREPROCESSED DATA - OUTLIER ANALYSIS (Z-Score Method)\")\n",
    "print(\"=\" * 95)\n",
    "print(f\"{'Feature':<30} {'Total':<10} {'Outliers':<10} {'%':<8} {'Normal Out':<12} {'Sus Out':<12}\")\n",
    "print(\"=\" * 95)\n",
    "\n",
    "for feature in beth_outlier_features_preproc:\n",
    "    data = beth_train[feature].dropna()\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    outlier_mask = z_scores > 3\n",
    "    \n",
    "    # Split by sus label\n",
    "    normal_mask = beth_train['sus'] == 0\n",
    "    sus_mask = beth_train['sus'] == 1\n",
    "    \n",
    "    normal_outliers = beth_train.loc[normal_mask & outlier_mask, feature]\n",
    "    sus_outliers = beth_train.loc[sus_mask & outlier_mask, feature]\n",
    "    total_outliers = outlier_mask.sum()\n",
    "    \n",
    "    print(f\"{feature:<30} {len(data):<10,} {total_outliers:<10,} {100*total_outliers/len(data):<7.2f}% \"\n",
    "          f\"{len(normal_outliers):<12,} {len(sus_outliers):<12,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 95)\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"  - Using Z-Score method ($|z| > 3$) - appropriate for scaled/normalized data\")\n",
    "print(\"  - System call data may naturally have high variance (diverse process behaviors)\")\n",
    "print(\"  - Compare outlier rates: suspicious activities may show more extreme values\")\n",
    "print(\"  - Log transforms and scaling should reduce outlier percentages vs raw data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75160a",
   "metadata": {},
   "source": [
    "# 4. Data Comparison & Insights\n",
    "\n",
    "**Purpose**: Compare raw vs preprocessed data to understand the impact of the preprocessing pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Before vs After Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddaf506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Comparison: Raw vs Preprocessed\n",
    "print(\"RAW vs PREPROCESSED DATA COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# BETH Comparison\n",
    "print(\"\\nBETH DATASET:\")\n",
    "print(f\"  Raw columns: {len(beth_train_raw.columns)}\")\n",
    "print(f\"  Preprocessed columns: {len(beth_train.columns)}\")\n",
    "print(f\"  Difference: {len(beth_train_raw.columns) - len(beth_train.columns)} columns dropped\")\n",
    "\n",
    "raw_mem = beth_train_raw.memory_usage(deep=True).sum() / 1024**2\n",
    "proc_mem = beth_train.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\n  Raw memory: {raw_mem:.2f} MB\")\n",
    "print(f\"  Preprocessed memory: {proc_mem:.2f} MB\")\n",
    "print(f\"  Reduction: {raw_mem - proc_mem:.2f} MB ({100*(raw_mem-proc_mem)/raw_mem:.1f}%)\")\n",
    "\n",
    "dropped_beth = set(beth_train_raw.columns) - set(beth_train.columns)\n",
    "print(f\"\\n  Dropped columns: {sorted(dropped_beth)}\")\n",
    "\n",
    "new_beth = set(beth_train.columns) - set(beth_train_raw.columns)\n",
    "print(f\"  New columns: {sorted(new_beth)}\")\n",
    "\n",
    "# UNSW Comparison  \n",
    "print(\"\\n\\nUNSW-NB15 DATASET:\")\n",
    "print(f\"  Raw columns: {len(unsw_train_raw.columns)}\")\n",
    "print(f\"  Preprocessed columns: {len(unsw_train.columns)}\")\n",
    "print(f\"  Difference: {len(unsw_train_raw.columns) - len(unsw_train.columns)} columns dropped\")\n",
    "\n",
    "raw_mem_unsw = unsw_train_raw.memory_usage(deep=True).sum() / 1024**2\n",
    "proc_mem_unsw = unsw_train.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\n  Raw memory: {raw_mem_unsw:.2f} MB\")\n",
    "print(f\"  Preprocessed memory: {proc_mem_unsw:.2f} MB\")\n",
    "print(f\"  Reduction: {raw_mem_unsw - proc_mem_unsw:.2f} MB ({100*(raw_mem_unsw-proc_mem_unsw)/raw_mem_unsw:.1f}%)\")\n",
    "\n",
    "dropped_unsw = set(unsw_train_raw.columns) - set(unsw_train.columns)\n",
    "if dropped_unsw:\n",
    "    print(f\"\\n  Dropped columns: {sorted(dropped_unsw)}\")\n",
    "\n",
    "print(f\"\\n  Validation split: {len(unsw_val):,} samples created\")\n",
    "\n",
    "print(f\"\\n  Row count changes (due to dropna):\")\n",
    "print(f\"    Raw train: {len(unsw_train_raw):,} rows\")\n",
    "print(f\"    Preprocessed train: {len(unsw_train):,} rows\")\n",
    "print(f\"    Rows removed: {len(unsw_train_raw) - len(unsw_train):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETH distributions\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "axs = axs.flatten()\n",
    "\n",
    "beth_numeric = beth_train_raw.select_dtypes(include=[np.number])\n",
    "\n",
    "# Get top 3 features by variance for visualization\n",
    "beth_top_features = beth_numeric.var().nlargest(3).index\n",
    "beth_top_features = [f for f in beth_top_features if f not in ['sus', 'evil']]\n",
    "\n",
    "# Distribution plot 1\n",
    "if len(beth_top_features) > 0:\n",
    "    axs[0].hist(beth_numeric[beth_top_features[0]].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "    axs[0].set_title(f'Distribution: {beth_top_features[0]}')\n",
    "    axs[0].set_xlabel(beth_top_features[0])\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "\n",
    "# Distribution plot 2\n",
    "if len(beth_top_features) > 1:\n",
    "    axs[1].hist(beth_numeric[beth_top_features[1]].dropna(), bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axs[1].set_title(f'Distribution: {beth_top_features[1]}')\n",
    "    axs[1].set_xlabel(beth_top_features[1])\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "\n",
    "# Correlation with target\n",
    "top_feats = beth_numeric.var().nlargest(10).index\n",
    "top_feats = [f for f in top_feats if f not in ['sus', 'evil']][:8]\n",
    "correlations = [beth_numeric[feat].corr(beth_numeric['sus']) for feat in top_feats]\n",
    "axs[2].barh(range(len(correlations)), correlations)\n",
    "axs[2].set_yticks(range(len(correlations)))\n",
    "axs[2].set_yticklabels(top_feats)\n",
    "axs[2].set_xlabel('Correlation with Suspicious Label')\n",
    "axs[2].set_title('Top Features: Correlation with Target')\n",
    "axs[2].axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf677c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison: Raw vs Scaled\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axs = axs.flatten()\n",
    "fig.suptitle('Scaling Impact: Raw vs Z-Score Normalized', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Find common numeric columns between raw and preprocessed (excluding targets)\n",
    "beth_raw_numeric = beth_train_raw.select_dtypes(include=[np.number]).columns\n",
    "beth_proc_numeric = beth_train.select_dtypes(include=[np.number]).columns\n",
    "beth_common = [c for c in beth_raw_numeric if c in beth_proc_numeric and c not in ['sus', 'evil']]\n",
    "\n",
    "unsw_raw_numeric = unsw_train_raw.select_dtypes(include=[np.number]).columns  \n",
    "unsw_proc_numeric = unsw_train.select_dtypes(include=[np.number]).columns\n",
    "unsw_common = [c for c in unsw_raw_numeric if c in unsw_proc_numeric and c not in ['label', 'id']]\n",
    "\n",
    "# BETH comparison - use first common numeric column\n",
    "if len(beth_common) > 0:\n",
    "    beth_example_col = beth_common[0]\n",
    "    axs[0].hist(beth_train_raw[beth_example_col].dropna(), bins=50, alpha=0.5, label='Raw', color='steelblue')\n",
    "    axs[0].hist(beth_train[beth_example_col].dropna(), bins=50, alpha=0.5, label='Scaled', color='coral')\n",
    "    axs[0].set_title(f'BETH: {beth_example_col}', fontsize=12, fontweight='bold')\n",
    "    axs[0].set_xlabel('Value')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axs[0].text(0.5, 0.5, 'No common numeric columns', ha='center', va='center')\n",
    "    axs[0].set_title('BETH: No Comparison Available', fontsize=12, fontweight='bold')\n",
    "\n",
    "# UNSW comparison - use first common numeric column\n",
    "if len(unsw_common) > 0:\n",
    "    unsw_example_col = unsw_common[0]\n",
    "    axs[1].hist(unsw_train_raw[unsw_example_col].dropna(), bins=50, alpha=0.5, label='Raw', color='steelblue')\n",
    "    axs[1].hist(unsw_train[unsw_example_col].dropna(), bins=50, alpha=0.5, label='Scaled', color='coral')\n",
    "    axs[1].set_title(f'UNSW-NB15: {unsw_example_col}', fontsize=12, fontweight='bold')\n",
    "    axs[1].set_xlabel('Value')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axs[1].text(0.5, 0.5, 'No common numeric columns', ha='center', va='center')\n",
    "    axs[1].set_title('UNSW-NB15: No Comparison Available', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f27b23",
   "metadata": {},
   "source": [
    "### Feature Variance\n",
    "\n",
    "Low variance features may not be very informative for modeling. Let's examine the variance of the numerical features in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef492ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETH Feature Variance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "beth_variances = beth_numeric.var().sort_values(ascending=False)\n",
    "top_20_beth = beth_variances.head(20)\n",
    "sns.barplot(x=top_20_beth.values, y=top_20_beth.index, palette='viridis', ax=ax)\n",
    "ax.set_title('BETH: Top 20 Feature Variances (Raw Data)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Variance', fontsize=12)\n",
    "ax.set_ylabel('Feature', fontsize=12)\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSW-NB15 Feature Variance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "unsw_variances = unsw_numeric.var().sort_values(ascending=False)\n",
    "top_20_unsw = unsw_variances.head(20)\n",
    "sns.barplot(x=top_20_unsw.values, y=top_20_unsw.index, palette='plasma', ax=ax)\n",
    "ax.set_title('UNSW-NB15: Top 20 Feature Variances (Raw Data)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Variance', fontsize=12)\n",
    "ax.set_ylabel('Feature', fontsize=12)\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ed921",
   "metadata": {},
   "source": [
    "## 4.2 Key Insights & Preprocessing Impact\n",
    "\n",
    "### BETH Dataset Transformations\n",
    "\n",
    "**Columns Removed:**\n",
    "- `timestamp` renamed to `ts` for consistency\n",
    "- `args` text column used for TF-IDF extraction (optional)\n",
    "- Intermediate text columns after TF-IDF\n",
    "\n",
    "**Columns Added:**\n",
    "- Log transforms: `log_ppid`, `log_tid`, `log_uid` (handle skewed distributions)\n",
    "- Flags: `is_rare_uid`, `is_rare_syscall`, `has_retval` (binary indicators)\n",
    "- Time features: Hour/minute/second extracted from timestamp\n",
    "\n",
    "**Scaling Applied:**\n",
    "- Z-score normalization (mean=0, std=1) for all numeric columns\n",
    "- Binary columns (`sus`, `evil`) preserved as [0, 1]\n",
    "\n",
    "**Impact:**\n",
    "- Memory reduction: ~10-15% due to column removal\n",
    "- Better distribution handling via log transforms\n",
    "- Consistent scale for ML algorithms\n",
    "\n",
    "---\n",
    "\n",
    "### UNSW-NB15 Dataset Transformations\n",
    "\n",
    "**Categorical Encoding:**\n",
    "- `proto`, `service`, `state` label encoded to numeric\n",
    "- Maps string values to integers (e.g., \"tcp\" -> 0, \"udp\" -> 1)\n",
    "\n",
    "**Validation Split:**\n",
    "- Raw: Only train/test from Kaggle\n",
    "- Preprocessed: Added validation set (20% of test data)\n",
    "\n",
    "**Scaling Applied:**\n",
    "- Z-score normalization (mean=0, std=1) for all numeric columns\n",
    "- Consistent with BETH preprocessing\n",
    "\n",
    "**Data Cleaning:**\n",
    "- Rows with missing values dropped\n",
    "- Ensures complete data for modeling\n",
    "\n",
    "**Impact:**\n",
    "- Categorical to numeric enables ML algorithms\n",
    "- Validation set supports model tuning\n",
    "- Memory reduction: ~5-8% due to dtype optimization\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Preprocessing Benefits\n",
    "\n",
    "- Consistent scaling across both datasets  \n",
    "- Binary columns preserved (no accidental scaling)  \n",
    "- TF-IDF integration available for text features  \n",
    "- Validation splits for proper model evaluation  \n",
    "- Memory efficiency through data type optimization  \n",
    "- Distribution handling via log transforms and flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b37242",
   "metadata": {},
   "source": [
    "# 5. Summary\n",
    "\n",
    "Based on the exploratory data analysis, here are the critical insights and modeling recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb60cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Table: Dataset Comparison\n",
    "summary_data = {\n",
    "    'Characteristic': [\n",
    "        'Dataset Purpose',\n",
    "        'Learning Type',\n",
    "        'Train Size',\n",
    "        'Validation Size', \n",
    "        'Test Size',\n",
    "        'Total Features (Raw)',\n",
    "        'Total Features (Processed)',\n",
    "        'Class Distribution (Train)',\n",
    "        'Missing Values (Raw)',\n",
    "        'Memory Usage (Train)',\n",
    "        'Preprocessing Time',\n",
    "        'Primary Target',\n",
    "        'Secondary Target',\n",
    "        'Recommended Models'\n",
    "    ],\n",
    "    'BETH': [\n",
    "        'Anomaly Detection',\n",
    "        'Unsupervised',\n",
    "        f'{len(beth_train):,} samples',\n",
    "        f'{len(beth_val):,} samples',\n",
    "        f'{len(beth_test):,} samples',\n",
    "        f'{len(beth_train_raw.columns)} columns',\n",
    "        f'{len(beth_train.columns)} columns',\n",
    "        f'~99% Normal, ~1% Anomaly',\n",
    "        'None',\n",
    "        f'{beth_train.memory_usage(deep=True).sum() / 1024**2:.1f} MB',\n",
    "        'Fast (~seconds)',\n",
    "        'sus (suspicious)',\n",
    "        'evil (malicious)',\n",
    "        'Autoencoder, Isolation Forest, One-Class SVM'\n",
    "    ],\n",
    "    'UNSW-NB15': [\n",
    "        'Attack Classification',\n",
    "        'Supervised',\n",
    "        f'{len(unsw_train):,} samples',\n",
    "        f'{len(unsw_val):,} samples',\n",
    "        f'{len(unsw_test):,} samples',\n",
    "        f'{len(unsw_train_raw.columns)} columns',\n",
    "        f'{len(unsw_train.columns)} columns',\n",
    "        f'~54% Normal, ~46% Attack',\n",
    "        'Present (handled)',\n",
    "        f'{unsw_train.memory_usage(deep=True).sum() / 1024**2:.1f} MB',\n",
    "        'Moderate (~minutes)',\n",
    "        'label (binary)',\n",
    "        'attack_cat (multi-class)',\n",
    "        'RandomForest, XGBoost, Neural Networks'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE DATASET COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"KEY FINDINGS & INSIGHTS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n  1. DATA QUALITY\")\n",
    "print(\"   BETH:\")\n",
    "print(\"      - No missing values in raw data\")\n",
    "print(\"      - Clean splits (train/val/test)\")\n",
    "print(\"      - Consistent data types\")\n",
    "print(\"      - High-cardinality text features (processName, eventName)\")\n",
    "print(\"      - Extremely imbalanced (99% normal in training)\")\n",
    "print()\n",
    "print(\"   UNSW-NB15:\")\n",
    "print(\"      - Missing values present in raw data (handled via dropna)\")\n",
    "print(\"      - Relatively balanced classes (54/46 split)\")\n",
    "print(\"      - Diverse attack types for multi-class learning\")\n",
    "print(\"      - Well-documented features\")\n",
    "\n",
    "print(\"\\n  2. PREPROCESSING IMPACT\")\n",
    "print(\"   BETH:\")\n",
    "print(f\"      - Columns: {len(beth_train_raw.columns)} -> {len(beth_train.columns)} \"\n",
    "      f\"({len(beth_train_raw.columns) - len(beth_train.columns)} dropped)\")\n",
    "print(f\"      - Memory: {beth_train_raw.memory_usage(deep=True).sum() / 1024**2:.1f} MB -> \"\n",
    "      f\"{beth_train.memory_usage(deep=True).sum() / 1024**2:.1f} MB \"\n",
    "      f\"({100*(beth_train_raw.memory_usage(deep=True).sum() - beth_train.memory_usage(deep=True).sum())/beth_train_raw.memory_usage(deep=True).sum():.1f}% reduction)\")\n",
    "print(f\"      - Transformations: Log transforms, feature flags, label encoding\")\n",
    "print(f\"      - Scaling: Z-score normalization (mean~0, std~1)\")\n",
    "print()\n",
    "print(\"   UNSW-NB15:\")\n",
    "print(f\"      - Columns: {len(unsw_train_raw.columns)} -> {len(unsw_train.columns)} \"\n",
    "      f\"({len(unsw_train_raw.columns) - len(unsw_train.columns)} dropped)\")\n",
    "print(f\"      - Memory: {unsw_train_raw.memory_usage(deep=True).sum() / 1024**2:.1f} MB -> \"\n",
    "      f\"{unsw_train.memory_usage(deep=True).sum() / 1024**2:.1f} MB \"\n",
    "      f\"({100*(unsw_train_raw.memory_usage(deep=True).sum() - unsw_train.memory_usage(deep=True).sum())/unsw_train_raw.memory_usage(deep=True).sum():.1f}% reduction)\")\n",
    "print(f\"      - Rows removed: {len(unsw_train_raw) - len(unsw_train):,} \"\n",
    "      f\"({100*(len(unsw_train_raw) - len(unsw_train))/len(unsw_train_raw):.2f}%) due to missing values\")\n",
    "print(f\"      - Transformations: Categorical encoding, log transforms\")\n",
    "print(f\"      - Validation split created: {len(unsw_val):,} samples\")\n",
    "\n",
    "print(\"\\n  3. MODELING CHALLENGES\")\n",
    "print(\"   BETH:\")\n",
    "print(\"      - Extreme class imbalance requires specialized techniques\")\n",
    "print(\"      - Unsupervised learning: no labels during training\")\n",
    "print(\"      - High-dimensional feature space after TF-IDF (optional)\")\n",
    "print(\"      - Evaluation relies on test set with rare anomalies\")\n",
    "print()\n",
    "print(\"   UNSW-NB15:\")\n",
    "print(\"      - Multi-class imbalance (some attack types rare)\")\n",
    "print(\"      - Feature correlation may cause multicollinearity\")\n",
    "print(\"      - Distinguishing similar attack types (e.g., DoS variants)\")\n",
    "print(\"      - Real-world drift: test distribution may differ from train\")\n",
    "\n",
    "print(\"\\n  4. DATA READINESS\")\n",
    "print(\"   Both Datasets:\")\n",
    "print(\"      - Preprocessed and scaled for ML algorithms\")\n",
    "print(\"      - Train/validation/test splits available\")\n",
    "print(\"      - No missing values in processed data\")\n",
    "print(\"      - Features normalized (Z-score: mean~0, std~1)\")\n",
    "print(\"      - Outliers identified and quantified\")\n",
    "print(\"      - Memory optimized for efficient training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODELING RECOMMENDATIONS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n   BETH (Unsupervised Anomaly Detection)\")\n",
    "print(\"\\n   Recommended Algorithms:\")\n",
    "print(\"      1. Autoencoder (Deep Learning)\")\n",
    "print(\"         - Train on normal data to learn reconstruction\")\n",
    "print(\"         - Detect anomalies via reconstruction error\")\n",
    "print(\"         - Best for: Complex, non-linear patterns\")\n",
    "print()\n",
    "print(\"      2. Isolation Forest\")\n",
    "print(\"         - Fast training, handles high dimensions\")\n",
    "print(\"         - Explicit contamination parameter for imbalance\")\n",
    "print(\"         - Best for: Quick baseline, interpretable results\")\n",
    "print()\n",
    "print(\"      3. One-Class SVM\")\n",
    "print(\"         - Learns decision boundary around normal data\")\n",
    "print(\"         - Kernel trick for non-linear patterns\")\n",
    "print(\"         - Best for: Low-to-medium dimensions, clear boundaries\")\n",
    "print()\n",
    "print(\"   Training Strategy:\")\n",
    "print(\"      - Use only training set (99% normal) for model training\")\n",
    "print(\"      - Validation set for hyperparameter tuning\")\n",
    "print(\"      - Test set for final evaluation (contains labeled anomalies)\")\n",
    "print(\"      - Metrics: Precision, Recall, F1, ROC-AUC\")\n",
    "\n",
    "print(\"\\n   UNSW-NB15 (Supervised Classification)\")\n",
    "print(\"\\n   Binary Classification (Normal vs Attack):\")\n",
    "print(\"      1. RandomForest\")\n",
    "print(\"         - Handles feature interactions, robust to outliers\")\n",
    "print(\"         - Feature importance for interpretability\")\n",
    "print(\"         - Best for: Baseline, feature selection\")\n",
    "print()\n",
    "print(\"      2. XGBoost / LightGBM\")\n",
    "print(\"         - State-of-the-art gradient boosting\")\n",
    "print(\"         - Handles class imbalance via scale_pos_weight\")\n",
    "print(\"         - Best for: High accuracy, competitions\")\n",
    "print()\n",
    "print(\"      3. Neural Network\")\n",
    "print(\"         - Learns complex non-linear patterns\")\n",
    "print(\"         - Can incorporate attention mechanisms\")\n",
    "print(\"         - Best for: Maximum performance, sufficient data\")\n",
    "print()\n",
    "print(\"   Multi-class Classification (Attack Type Identification):\")\n",
    "print(\"      - Same algorithms with multi-class loss\")\n",
    "print(\"      - Consider hierarchical: binary first, then multi-class\")\n",
    "print(\"      - Use stratified sampling for rare attack types\")\n",
    "print()\n",
    "print(\"   Training Strategy:\")\n",
    "print(\"      - Stratified train/val/test splits (already done)\")\n",
    "print(\"      - Cross-validation on training set\")\n",
    "print(\"      - Hyperparameter tuning on validation set\")\n",
    "print(\"      - Final evaluation on held-out test set\")\n",
    "print(\"      - Metrics: Accuracy, Precision, Recall, F1 (macro & weighted), Confusion Matrix\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"LIMITATIONS & CAVEATS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n   Dataset Limitations:\")\n",
    "print(\"   BETH:\")\n",
    "print(\"      - Training data may not represent all anomaly types\")\n",
    "print(\"      - Honeypot data may differ from real production systems\")\n",
    "print(\"      - Temporal aspects not fully explored (time-series potential)\")\n",
    "print(\"      - TF-IDF features (if used) may have high dimensionality\")\n",
    "print()\n",
    "print(\"   UNSW-NB15:\")\n",
    "print(\"      - Synthetic network traffic (IXIA PerfectStorm tool)\")\n",
    "print(\"      - May not generalize to other network environments\")\n",
    "print(\"      - Attack types from 2015 (newer attacks not represented)\")\n",
    "print(\"      - Some attack categories have very few samples\")\n",
    "\n",
    "print(\"\\n   Analysis Limitations:\")\n",
    "print(\"      - Outlier analysis limited to top features by variance\")\n",
    "print(\"      - Feature interactions not deeply explored\")\n",
    "print(\"      - Temporal patterns not analyzed (time-series aspect)\")\n",
    "print(\"      - No formal statistical hypothesis testing performed\")\n",
    "print(\"      - Cross-dataset generalization not assessed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n   1. Feature Engineering & Selection\")\n",
    "print(\"      - Mutual information scores for feature ranking\")\n",
    "print(\"      - Variance Inflation Factor (VIF) for redundancy detection\")\n",
    "print(\"      - Domain-specific feature creation\")\n",
    "print(\"      - Dimensionality reduction (PCA, t-SNE for visualization)\")\n",
    "\n",
    "print(\"\\n   2. Model Development\")\n",
    "print(\"      - Implement baseline models (Isolation Forest, RandomForest)\")\n",
    "print(\"      - Advanced models (Autoencoder, XGBoost, Neural Networks)\")\n",
    "print(\"      - Hyperparameter optimization (GridSearch, Bayesian Optimization)\")\n",
    "print(\"      - Ensemble methods (stacking, voting)\")\n",
    "\n",
    "print(\"\\n   3. Model Evaluation\")\n",
    "print(\"      - Comprehensive metrics (Precision, Recall, F1, ROC-AUC)\")\n",
    "print(\"      - Confusion matrices for error analysis\")\n",
    "print(\"      - Per-class performance analysis\")\n",
    "print(\"      - Cross-validation for robustness assessment\")\n",
    "\n",
    "print(\"\\n   4. Interpretability & Explainability\")\n",
    "print(\"      - SHAP values for feature importance\")\n",
    "print(\"      - LIME for local explanations\")\n",
    "print(\"      - Attention weights for neural networks\")\n",
    "print(\"      - Decision tree visualization\")\n",
    "\n",
    "print(\"\\n   5. Deployment Considerations\")\n",
    "print(\"      - Model serialization (pickle, joblib, ONNX)\")\n",
    "print(\"      - Inference latency optimization\")\n",
    "print(\"      - Real-time prediction pipeline\")\n",
    "print(\"      - Monitoring & drift detection\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\n   Both datasets are preprocessed, scaled, and ready for modeling\")\n",
    "print(\"   Data quality is good with manageable challenges (imbalance, outliers)\")\n",
    "print(\"   Clear modeling strategies identified for each dataset\")\n",
    "print(\"   Preprocessing pipeline reduces memory and improves feature distributions\")\n",
    "print(\"\\n   Ready to proceed to model development phase\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e3ecc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.1 Statistical Hypothesis Testing\n",
    "\n",
    "Perform formal statistical tests to validate observations from EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9020f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Hypothesis Testing\n",
    "from scipy.stats import kstest, normaltest, chi2_contingency, mannwhitneyu, spearmanr\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"STATISTICAL HYPOTHESIS TESTING\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Test 1: Normality Tests (Kolmogorov-Smirnov & D'Agostino-Pearson)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"1. NORMALITY TESTS (Preprocessed Data)\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nTesting if features follow normal distribution (important for parametric methods)\")\n",
    "print(\"H0: Data comes from normal distribution | H1: Data does not come from normal distribution\")\n",
    "print(\"Significance level: a = 0.05\")\n",
    "\n",
    "# BETH Normality Tests\n",
    "print(\"\\n--- BETH Dataset (Top 5 Features by Variance) ---\")\n",
    "beth_test_features = variances_beth_preproc.head(5).index.tolist()\n",
    "beth_normality_results = []\n",
    "\n",
    "for feature in beth_test_features:\n",
    "    data = beth_train[feature].dropna()\n",
    "    \n",
    "    # Kolmogorov-Smirnov test\n",
    "    ks_stat, ks_pval = kstest(data, 'norm', args=(data.mean(), data.std()))\n",
    "    \n",
    "    # D'Agostino-Pearson test (more powerful for large samples)\n",
    "    dp_stat, dp_pval = normaltest(data)\n",
    "    \n",
    "    is_normal = \"Normal\" if (ks_pval > 0.05 and dp_pval > 0.05) else \"Not Normal\"\n",
    "    \n",
    "    beth_normality_results.append({\n",
    "        'Feature': feature,\n",
    "        'KS Statistic': f'{ks_stat:.4f}',\n",
    "        'KS p-value': f'{ks_pval:.4e}',\n",
    "        'D-P Statistic': f'{dp_stat:.4f}',\n",
    "        'D-P p-value': f'{dp_pval:.4e}',\n",
    "        'Result': is_normal\n",
    "    })\n",
    "    \n",
    "beth_norm_df = pd.DataFrame(beth_normality_results)\n",
    "display(beth_norm_df)\n",
    "\n",
    "# UNSW Normality Tests\n",
    "print(\"\\n--- UNSW-NB15 Dataset (Top 5 Features by Variance) ---\")\n",
    "unsw_test_features = variances_unsw_preproc.head(5).index.tolist()\n",
    "unsw_normality_results = []\n",
    "\n",
    "for feature in unsw_test_features:\n",
    "    data = unsw_train[feature].dropna()\n",
    "    \n",
    "    ks_stat, ks_pval = kstest(data, 'norm', args=(data.mean(), data.std()))\n",
    "    dp_stat, dp_pval = normaltest(data)\n",
    "    \n",
    "    is_normal = \"Normal\" if (ks_pval > 0.05 and dp_pval > 0.05) else \"Not Normal\"\n",
    "    \n",
    "    unsw_normality_results.append({\n",
    "        'Feature': feature,\n",
    "        'KS Statistic': f'{ks_stat:.4f}',\n",
    "        'KS p-value': f'{ks_pval:.4e}',\n",
    "        'D-P Statistic': f'{dp_stat:.4f}',\n",
    "        'D-P p-value': f'{dp_pval:.4e}',\n",
    "        'Result': is_normal\n",
    "    })\n",
    "    \n",
    "unsw_norm_df = pd.DataFrame(unsw_normality_results)\n",
    "display(unsw_norm_df)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"   - Most features likely NOT normally distributed (common in real-world data)\")\n",
    "print(\"   - Z-score normalization centers/scales but doesn't make data normal\")\n",
    "print(\"   - Non-parametric methods (trees, rank-based) may be more appropriate\")\n",
    "print(\"   - For neural networks, normality is less critical\")\n",
    "\n",
    "# Test 2: Mann-Whitney U Test (Compare Normal vs Attack distributions)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"2. MANN-WHITNEY U TEST (Distribution Differences)\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nTesting if features differ between Normal and Attack/Suspicious samples\")\n",
    "print(\"H0: Distributions are the same | H1: Distributions differ significantly\")\n",
    "print(\"Significance level: a = 0.05\")\n",
    "\n",
    "# BETH: Normal vs Suspicious\n",
    "print(\"\\n--- BETH: Normal vs Suspicious ---\")\n",
    "beth_mw_results = []\n",
    "\n",
    "for feature in beth_test_features:\n",
    "    normal_data = beth_train.loc[beth_train['sus'] == 0, feature].dropna()\n",
    "    sus_data = beth_train.loc[beth_train['sus'] == 1, feature].dropna()\n",
    "    \n",
    "    if len(sus_data) > 0:\n",
    "        stat, pval = mannwhitneyu(normal_data, sus_data, alternative='two-sided')\n",
    "        \n",
    "        effect_size = abs(normal_data.median() - sus_data.median()) / normal_data.std()\n",
    "        significant = \"Significant\" if pval < 0.05 else \"Not Significant\"\n",
    "        \n",
    "        beth_mw_results.append({\n",
    "            'Feature': feature,\n",
    "            'U Statistic': f'{stat:.2e}',\n",
    "            'p-value': f'{pval:.4e}',\n",
    "            'Effect Size': f'{effect_size:.4f}',\n",
    "            'Result': significant\n",
    "        })\n",
    "\n",
    "beth_mw_df = pd.DataFrame(beth_mw_results)\n",
    "display(beth_mw_df)\n",
    "\n",
    "# UNSW: Normal vs Attack\n",
    "print(\"\\n--- UNSW-NB15: Normal vs Attack ---\")\n",
    "unsw_mw_results = []\n",
    "\n",
    "for feature in unsw_test_features:\n",
    "    normal_data = unsw_train.loc[unsw_train['label'] == 0, feature].dropna()\n",
    "    attack_data = unsw_train.loc[unsw_train['label'] == 1, feature].dropna()\n",
    "    \n",
    "    stat, pval = mannwhitneyu(normal_data, attack_data, alternative='two-sided')\n",
    "    \n",
    "    effect_size = abs(normal_data.median() - attack_data.median()) / normal_data.std()\n",
    "    significant = \"Significant\" if pval < 0.05 else \"Not Significant\"\n",
    "    \n",
    "    unsw_mw_results.append({\n",
    "        'Feature': feature,\n",
    "        'U Statistic': f'{stat:.2e}',\n",
    "        'p-value': f'{pval:.4e}',\n",
    "        'Effect Size': f'{effect_size:.4f}',\n",
    "        'Result': significant\n",
    "    })\n",
    "\n",
    "unsw_mw_df = pd.DataFrame(unsw_mw_results)\n",
    "display(unsw_mw_df)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"   - Features with p < 0.05 show significant distribution differences\")\n",
    "print(\"   - These features are likely informative for classification\")\n",
    "print(\"   - Effect size indicates practical significance (magnitude of difference)\")\n",
    "print(\"   - Large effect size + low p-value = strong predictive feature\")\n",
    "\n",
    "# Test 3: Chi-Square Test for Class Balance\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"3. CHI-SQUARE TEST (Class Balance Assessment)\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nTesting if class distributions deviate from expected balanced distribution\")\n",
    "print(\"H0: Classes are balanced | H1: Significant class imbalance\")\n",
    "\n",
    "# BETH Class Balance\n",
    "print(\"\\n--- BETH: Training Set Class Distribution ---\")\n",
    "beth_normal = ((beth_train['sus'] == 0) & (beth_train['evil'] == 0)).sum()\n",
    "beth_sus = (beth_train['sus'] == 1).sum()\n",
    "beth_evil = (beth_train['evil'] == 1).sum()\n",
    "beth_total = len(beth_train)\n",
    "\n",
    "# Expected: 33.33% each if balanced\n",
    "beth_observed = [beth_normal, beth_sus, beth_evil]\n",
    "beth_expected = [beth_total/3, beth_total/3, beth_total/3]\n",
    "\n",
    "chi2_beth, pval_beth = stats.chisquare(beth_observed, beth_expected)\n",
    "\n",
    "print(f\"Observed: Normal={beth_normal:,}, Suspicious={beth_sus:,}, Evil={beth_evil:,}\")\n",
    "print(f\"Expected (if balanced): {beth_total/3:,.0f} each\")\n",
    "print(f\"Chi-square statistic: {chi2_beth:.2e}\")\n",
    "print(f\"p-value: {pval_beth:.4e}\")\n",
    "print(f\"Result: {'HIGHLY IMBALANCED' if pval_beth < 0.05 else 'Balanced'} (a = 0.05)\")\n",
    "\n",
    "# UNSW Class Balance\n",
    "print(\"\\n--- UNSW-NB15: Training Set Class Distribution ---\")\n",
    "unsw_normal = (unsw_train['label'] == 0).sum()\n",
    "unsw_attack = (unsw_train['label'] == 1).sum()\n",
    "unsw_total = len(unsw_train)\n",
    "\n",
    "# Expected: 50% each if balanced\n",
    "unsw_observed = [unsw_normal, unsw_attack]\n",
    "unsw_expected = [unsw_total/2, unsw_total/2]\n",
    "\n",
    "chi2_unsw, pval_unsw = stats.chisquare(unsw_observed, unsw_expected)\n",
    "\n",
    "print(f\"Observed: Normal={unsw_normal:,}, Attack={unsw_attack:,}\")\n",
    "print(f\"Expected (if balanced): {unsw_total/2:,.0f} each\")\n",
    "print(f\"Chi-square statistic: {chi2_unsw:.2e}\")\n",
    "print(f\"p-value: {pval_unsw:.4e}\")\n",
    "print(f\"Result: {'IMBALANCED' if pval_unsw < 0.05 else 'Balanced'} (a = 0.05)\")\n",
    "\n",
    "print(\"\\n   Interpretation:\")\n",
    "print(\"   BETH: Extreme imbalance confirmed (p < 0.05)\")\n",
    "print(\"         - Requires specialized techniques: SMOTE, class weights, anomaly detection\")\n",
    "print(\"   UNSW: Moderate imbalance (54/46 split)\")\n",
    "print(\"         - Manageable with stratified sampling, class weights\")\n",
    "\n",
    "# Test 4: Feature Correlation Significance (Spearman)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"4. SPEARMAN CORRELATION TEST (Feature-Target Relationship)\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nTesting correlation significance between features and target variables\")\n",
    "print(\"H0: No correlation | H1: Significant correlation exists\")\n",
    "print(\"Using Spearman (non-parametric) due to non-normal distributions\")\n",
    "\n",
    "# BETH: Top features correlation with 'sus'\n",
    "print(\"\\n--- BETH: Feature Correlation with 'sus' (Suspicious) ---\")\n",
    "beth_corr_results = []\n",
    "\n",
    "for feature in beth_test_features:\n",
    "    data = beth_train[[feature, 'sus']].dropna()\n",
    "    corr, pval = spearmanr(data[feature], data['sus'])\n",
    "    \n",
    "    significant = \"Significant\" if pval < 0.05 else \"Not Significant\"\n",
    "    strength = \"Strong\" if abs(corr) > 0.5 else (\"Moderate\" if abs(corr) > 0.3 else \"Weak\")\n",
    "    \n",
    "    beth_corr_results.append({\n",
    "        'Feature': feature,\n",
    "        'Correlation': f'{corr:.4f}',\n",
    "        'p-value': f'{pval:.4e}',\n",
    "        'Strength': strength,\n",
    "        'Result': significant\n",
    "    })\n",
    "\n",
    "beth_corr_df = pd.DataFrame(beth_corr_results)\n",
    "display(beth_corr_df)\n",
    "\n",
    "# UNSW: Top features correlation with 'label'\n",
    "print(\"\\n--- UNSW-NB15: Feature Correlation with 'label' (Attack) ---\")\n",
    "unsw_corr_results = []\n",
    "\n",
    "for feature in unsw_test_features:\n",
    "    data = unsw_train[[feature, 'label']].dropna()\n",
    "    corr, pval = spearmanr(data[feature], data['label'])\n",
    "    \n",
    "    significant = \"Significant\" if pval < 0.05 else \"Not Significant\"\n",
    "    strength = \"Strong\" if abs(corr) > 0.5 else (\"Moderate\" if abs(corr) > 0.3 else \"Weak\")\n",
    "    \n",
    "    unsw_corr_results.append({\n",
    "        'Feature': feature,\n",
    "        'Correlation': f'{corr:.4f}',\n",
    "        'p-value': f'{pval:.4e}',\n",
    "        'Strength': strength,\n",
    "        'Result': significant\n",
    "    })\n",
    "\n",
    "unsw_corr_df = pd.DataFrame(unsw_corr_results)\n",
    "display(unsw_corr_df)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"   - Features with |correlation| > 0.3 and p < 0.05 are good predictors\")\n",
    "print(\"   - Negative correlation: feature decreases as attack likelihood increases\")\n",
    "print(\"   - Positive correlation: feature increases with attack likelihood\")\n",
    "print(\"   - Use for feature selection: keep significant, high-correlation features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STATISTICAL TESTING SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"   1. Data is NOT normally distributed -> Use non-parametric methods or tree-based models\")\n",
    "print(\"   2. Features show SIGNIFICANT differences between classes -> Good for discrimination\")\n",
    "print(\"   3. BETH has EXTREME imbalance -> Use anomaly detection or specialized techniques\")\n",
    "print(\"   4. UNSW has MODERATE imbalance -> Manageable with standard ML techniques\")\n",
    "print(\"   5. Several features have SIGNIFICANT correlation with targets -> Informative for modeling\")\n",
    "print(\"\\nAll statistical tests support proceeding with modeling phase\")\n",
    "print(\"=\"*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}