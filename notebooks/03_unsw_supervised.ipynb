{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3427cf85",
   "metadata": {},
   "source": [
    "# UNSW-NB15 Dataset - Supervised Attack Classification\n",
    "\n",
    "**Purpose:** Train supervised machine learning models to classify network traffic as normal or attack, then classify attack types.\n",
    "\n",
    "---\n",
    "\n",
    "## Approach\n",
    "\n",
    "This notebook implements a comprehensive supervised learning pipeline:\n",
    "\n",
    "### Binary Classification (Stage 1)\n",
    "- **Logistic Regression**: Linear baseline with L1/L2 regularization\n",
    "- **Random Forest**: Ensemble decision trees with feature importance\n",
    "- **XGBoost**: Gradient boosting with hyperparameter tuning\n",
    "\n",
    "### Multi-class Classification (Stage 2)\n",
    "- **Attack Type Classification**: Trained only on attack samples\n",
    "- **Class Imbalance Handling**: SMOTE oversampling for minority attack types\n",
    "- **Two-Stage Pipeline**: Detection -> Classification (mimics real SOC workflow)\n",
    "\n",
    "## Evaluation Metrics\n",
    "- **Binary Classification**: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
    "- **Multi-class Classification**: Per-class metrics, confusion matrix\n",
    "- **Feature Importance**: Model-based rankings\n",
    "- **Visualizations**: ROC curves, confusion matrices, pipeline flow\n",
    "\n",
    "## Training Strategy\n",
    "\n",
    "1. **Train baseline models** on training set with default hyperparameters\n",
    "2. **Evaluate baseline models** on test set to establish performance floor\n",
    "3. **Hyperparameter tuning** using validation set (Grid/Random Search)\n",
    "4. **Evaluate tuned models** on test set and compare improvements\n",
    "5. **Two-stage pipeline** for realistic deployment scenario\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Joshua Laubach  \n",
    "**Date:** November 9, 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577d239",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Data Loading](#1-setup-and-data-loading)\n",
    "2. [Prepare Features and Labels](#2-prepare-features-and-labels)\n",
    "3. [Train Baseline Models](#3-train-baseline-models)\n",
    "4. [Evaluate Baseline Models on Test Set](#4-evaluate-baseline-models-on-test-set)\n",
    "   - 4.1 Logistic Regression\n",
    "   - 4.2 Random Forest\n",
    "   - 4.3 XGBoost\n",
    "5. [Compare Baseline Models](#5-compare-baseline-models)\n",
    "6. [Hyperparameter Tuning](#6-hyperparameter-tuning)\n",
    "   - 6.1 Tune Logistic Regression\n",
    "   - 6.2 Tune Random Forest\n",
    "   - 6.3 Tune XGBoost\n",
    "   - 6.4 Compare Tuned vs Baseline\n",
    "7. [Evaluate Tuned Models on Test Set](#7-evaluate-tuned-models-on-test-set)\n",
    "8. [Two-Stage Prediction Pipeline](#8-two-stage-prediction-pipeline)\n",
    "   - 8.1 Stage 1: Binary Attack Detection\n",
    "   - 8.2 Stage 2: Attack Type Classification (with SMOTE)\n",
    "   - 8.3 End-to-End Pipeline Evaluation\n",
    "9. [Save Results](#9-save-results)\n",
    "10. [Summary and Conclusions](#10-summary-and-conclusions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3157af60",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "### Import Libraries and Load Dataset\n",
    "\n",
    "Load the UNSW-NB15 network intrusion dataset with preprocessing applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1e2c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Configure settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "import importlib\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "import preprocessing\n",
    "from preprocessing import load_unsw\n",
    "from models_supervised import (\n",
    "    LogisticRegressionClassifier,\n",
    "    RandomForestClassifierModel,\n",
    "    XGBoostClassifier,\n",
    "    train_all_models,\n",
    "    compare_models,\n",
    "    plot_feature_importances_comparison\n",
    ")\n",
    "from evaluation import (\n",
    "    evaluate_classification,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_precision_recall_curve,\n",
    "    compare_roc_curves,\n",
    "    create_evaluation_report,\n",
    "    print_summary_stats\n",
    ")\n",
    "from utils import set_seed, Timer\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LIBRARIES IMPORTED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc02286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOADING UNSW-NB15 DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load UNSW-NB15 dataset using the preprocessing function\n",
    "# This returns the feature matrices (X) and a dictionary of label DataFrames (y)\n",
    "X_train, X_val, X_test, y_labels = load_unsw(\n",
    "    raw=False\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"  Feature matrices (numeric only):\")\n",
    "print(f\"    Training:   {X_train.shape}\")\n",
    "print(f\"    Validation: {X_val.shape}\")\n",
    "print(f\"    Test:       {X_test.shape}\")\n",
    "\n",
    "print(f\"\\n  Label dictionaries contain 'attack_cat' and 'label' arrays:\")\n",
    "print(f\"    y_train keys: {list(y_labels['train'].keys())}\")\n",
    "print(f\"    y_val keys:   {list(y_labels['val'].keys())}\")\n",
    "print(f\"    y_test keys:  {list(y_labels['test'].keys())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046157c5",
   "metadata": {},
   "source": [
    "## 2. Prepare Features and Labels\n",
    "\n",
    "Extract features and target variable for binary classification (normal vs attack)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0199fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the binary 'label' for Stage 1 classification from the y_labels dictionary\n",
    "y_train = y_labels['train']['label']\n",
    "y_val = y_labels['val']['label']\n",
    "y_test = y_labels['test']['label']\n",
    "\n",
    "# The feature matrices X_train, X_val, X_test are already prepared from the load_unsw function\n",
    "\n",
    "print(f\"Feature dimension: {X_train.shape[1]} features\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Validation samples: {len(X_val):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "\n",
    "# Display class distribution for the binary classification target\n",
    "print_summary_stats(y_train, dataset_name='Training Set')\n",
    "print_summary_stats(y_val, dataset_name='Validation Set')\n",
    "print_summary_stats(y_test, dataset_name='Test Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77772",
   "metadata": {},
   "source": [
    "## 3. Train Baseline Models\n",
    "\n",
    "Train Logistic Regression, Random Forest, and XGBoost models with default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING BASELINE SUPERVISED MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train all supervised models with default hyperparameters\n",
    "with Timer(\"Training all supervised models\"):\n",
    "    models = train_all_models(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val=X_val,  # Use validation set for XGBoost early stopping\n",
    "        y_val=y_val,\n",
    "        scaler='standard'\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE MODELS TRAINED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Models available: {list(models.keys())}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cae40a",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning\n",
    "\n",
    "**Execution Note**: This section should be run AFTER Section 3 (baseline training) but placement in the notebook is flexible. The tuned models will be compared against baseline models from Section 3.\n",
    "\n",
    "### 6.1 Tune Logistic Regression\n",
    "\n",
    "Perform grid search with cross-validation to find optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CREATING STRATIFIED SAMPLE FOR HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create stratified 10k sample for fast hyperparameter search\n",
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "if len(X_train) > SAMPLE_SIZE:\n",
    "    X_train_sample, _, y_train_sample, _ = train_test_split(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        train_size=SAMPLE_SIZE,\n",
    "        stratify=y_train,  # Maintain class balance\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[Stratified Sample Created]\")\n",
    "    print(f\"   Original training size: {len(X_train):,}\")\n",
    "    print(f\"   Sample size: {len(X_train_sample):,}\")\n",
    "    print(f\"\\n[Class Distribution Comparison]\")\n",
    "    print(f\"   Original - Normal: {(y_train == 0).sum():,} ({100*(y_train == 0).mean():.2f}%)\")\n",
    "    print(f\"   Original - Attack: {(y_train == 1).sum():,} ({100*(y_train == 1).mean():.2f}%)\")\n",
    "    print(f\"   Sample   - Normal: {(y_train_sample == 0).sum():,} ({100*(y_train_sample == 0).mean():.2f}%)\")\n",
    "    print(f\"   Sample   - Attack: {(y_train_sample == 1).sum():,} ({100*(y_train_sample == 1).mean():.2f}%)\")\n",
    "    print(f\"\\n   -> Will use sample for hyperparameter tuning (speed)\")\n",
    "    print(f\"   -> Will refit final models on full training set (performance)\")\n",
    "else:\n",
    "    # If dataset is small, use full dataset\n",
    "    X_train_sample = X_train\n",
    "    y_train_sample = y_train\n",
    "    print(f\"\\n[Dataset already small ({len(X_train):,} samples), using full training set]\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a4425",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note on Training Strategy:**\n",
    "\n",
    "For efficient hyperparameter tuning, we'll use a **stratified 10k sample** to quickly find optimal hyperparameters, then **refit on the full training set** with those parameters. This provides:\n",
    "- Fast tuning (~5-10 min instead of 45-60 min)\n",
    "- Maintained class balance via stratified sampling\n",
    "- Final models trained on full data for best performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TUNING LOGISTIC REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import hyperparameter tuning utilities with progress bars\n",
    "from model_tuning import (\n",
    "    tune_logistic_regression,\n",
    "    tune_random_forest,\n",
    "    tune_xgboost\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Tune on stratified sample for speed\n",
    "print(\"\\n[Phase 1: Hyperparameter Search on 10k Sample]\")\n",
    "lr_best_model_sample, lr_best_params, lr_best_score, lr_results_df = tune_logistic_regression(\n",
    "    X_train_sample, y_train_sample, X_val, y_val,\n",
    "    cv=3,  # Reduce folds for speed\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n[Best Parameters Found]: {lr_best_params}\")\n",
    "print(f\"[Best CV Score on Sample]: {lr_best_score:.4f}\")\n",
    "\n",
    "# Refit on full training set with best parameters\n",
    "print(\"\\n[Phase 2: Refitting on Full Training Set]\")\n",
    "print(f\"   Training on {len(X_train):,} samples with best hyperparameters...\")\n",
    "\n",
    "lr_best_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(**lr_best_params, random_state=42))\n",
    "])\n",
    "\n",
    "with Timer(\"Refitting Logistic Regression\"):\n",
    "    lr_best_model.fit(X_train, y_train)\n",
    "\n",
    "# Store validation score for comparison\n",
    "from sklearn.metrics import roc_auc_score\n",
    "lr_tuned_val_score = roc_auc_score(y_val, lr_best_model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f\"\\n[Final Model - Validation ROC-AUC]: {lr_tuned_val_score:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0737721",
   "metadata": {},
   "source": [
    "### 6.2 Tune Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560551cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TUNING RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Tune on stratified sample for speed\n",
    "print(\"\\n[Phase 1: Hyperparameter Search on 10k Sample]\")\n",
    "rf_best_model_sample, rf_best_params, rf_best_score, rf_results_df = tune_random_forest(\n",
    "    X_train_sample, y_train_sample, X_val, y_val,\n",
    "    cv=3,  # Use 3-fold CV due to large parameter space\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n[Best Parameters Found]: {rf_best_params}\")\n",
    "print(f\"[Best CV Score on Sample]: {rf_best_score:.4f}\")\n",
    "\n",
    "# Refit on full training set with best parameters\n",
    "print(\"\\n[Phase 2: Refitting on Full Training Set]\")\n",
    "print(f\"   Training on {len(X_train):,} samples with best hyperparameters...\")\n",
    "\n",
    "rf_best_model = RandomForestClassifier(**rf_best_params, random_state=42, n_jobs=-1)\n",
    "\n",
    "with Timer(\"Refitting Random Forest\"):\n",
    "    rf_best_model.fit(X_train, y_train)\n",
    "\n",
    "# Store validation score for comparison\n",
    "rf_tuned_val_score = roc_auc_score(y_val, rf_best_model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f\"\\n[Final Model - Validation ROC-AUC]: {rf_tuned_val_score:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bac73e",
   "metadata": {},
   "source": [
    "### 6.3 Tune XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TUNING XGBOOST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Tune on stratified sample for speed\n",
    "print(\"\\n[Phase 1: Hyperparameter Search on 10k Sample]\")\n",
    "xgb_best_model_sample, xgb_best_params, xgb_best_score, xgb_results_df = tune_xgboost(\n",
    "    X_train_sample, y_train_sample, X_val, y_val,\n",
    "    n_iter=50,  # Test 50 random combinations\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n[Best Parameters Found]: {xgb_best_params}\")\n",
    "print(f\"[Best CV Score on Sample]: {xgb_best_score:.4f}\")\n",
    "\n",
    "# Refit on full training set with best parameters\n",
    "print(\"\\n[Phase 2: Refitting on Full Training Set]\")\n",
    "print(f\"   Training on {len(X_train):,} samples with best hyperparameters...\")\n",
    "\n",
    "xgb_best_model = xgb.XGBClassifier(\n",
    "    **xgb_best_params,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "with Timer(\"Refitting XGBoost\"):\n",
    "    xgb_best_model.fit(X_train, y_train)\n",
    "\n",
    "# Store validation score for comparison\n",
    "xgb_tuned_val_score = roc_auc_score(y_val, xgb_best_model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f\"\\n[Final Model - Validation ROC-AUC]: {xgb_tuned_val_score:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54730459",
   "metadata": {},
   "source": [
    "### 6.4 Compare Tuned vs Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b20f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TUNING SUMMARY - BASELINE VS TUNED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare validation scores\n",
    "tuning_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n",
    "    'Baseline Val ROC-AUC': [\n",
    "        roc_auc_score(y_val, models['logistic_regression'].predict_proba(X_val)[:, 1]),\n",
    "        roc_auc_score(y_val, models['random_forest'].predict_proba(X_val)[:, 1]),\n",
    "        roc_auc_score(y_val, models['xgboost'].predict_proba(X_val)[:, 1])\n",
    "    ], \n",
    "    'Tuned Val ROC-AUC': [\n",
    "        lr_tuned_val_score,\n",
    "        rf_tuned_val_score,\n",
    "        xgb_tuned_val_score\n",
    "    ]\n",
    "})\n",
    "\n",
    "tuning_comparison['Improvement'] = tuning_comparison['Tuned Val ROC-AUC'] - tuning_comparison['Baseline Val ROC-AUC']\n",
    "tuning_comparison['Improvement %'] = 100 * tuning_comparison['Improvement'] / tuning_comparison['Baseline Val ROC-AUC']\n",
    "\n",
    "print(\"\\n\", tuning_comparison.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Store tuned models for evaluation\n",
    "tuned_models = {\n",
    "    'logistic_regression': lr_best_model,\n",
    "    'random_forest': rf_best_model,\n",
    "    'xgboost': xgb_best_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd4232e",
   "metadata": {},
   "source": [
    "### 6.5 Threshold Optimization for High Recall (Security Focus)\n",
    "\n",
    "**Security-Focused Classification:**\n",
    "\n",
    "In network security applications, missing an actual attack is much more costly than investigating a false alarm. To optimize for high recall (catching more attacks), we can lower the classification threshold from the default 0.5 to 0.05.\n",
    "\n",
    "**How Thresholding Works:**\n",
    "- Default (0.5): Classify as attack if predicted probability >= 50%\n",
    "- Low (0.05): Classify as attack if predicted probability >= 5%\n",
    "\n",
    "**Expected Impact:**\n",
    "- Higher Recall: Catch more actual attacks (reduce false negatives)\n",
    "- Lower Precision: More false alarms (increase false positives)\n",
    "- Security Trade-off: Better to investigate 100 false alarms than miss 1 real breach\n",
    "\n",
    "**Target:** Achieve 95%+ recall while maintaining reasonable precision for operational feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61857027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models with lower threshold for higher recall\n",
    "threshold = 0.05\n",
    "print(f\"[Testing Threshold = {threshold}]\\n\")\n",
    "\n",
    "# Store models with high-recall configuration\n",
    "tuned_models_high_recall = {}\n",
    "\n",
    "# Map display names to dictionary keys\n",
    "model_mapping = {\n",
    "    'Logistic Regression': 'logistic_regression',\n",
    "    'Random Forest': 'random_forest',\n",
    "    'XGBoost': 'xgboost'\n",
    "}\n",
    "\n",
    "for display_name, key in model_mapping.items():\n",
    "    print(f\"--- {display_name} ---\")\n",
    "    model = tuned_models[key]\n",
    "    \n",
    "    # Get predicted probabilities\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Apply custom threshold\n",
    "    y_pred_low = (y_proba >= threshold).astype(int)\n",
    "    y_pred_default = (y_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics for both thresholds\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    low_metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_low),\n",
    "        'Precision': precision_score(y_test, y_pred_low),\n",
    "        'Recall': recall_score(y_test, y_pred_low),\n",
    "        'F1-Score': f1_score(y_test, y_pred_low)\n",
    "    }\n",
    "    \n",
    "    default_metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_default),\n",
    "        'Precision': precision_score(y_test, y_pred_default),\n",
    "        'Recall': recall_score(y_test, y_pred_default),\n",
    "        'F1-Score': f1_score(y_test, y_pred_default)\n",
    "    }\n",
    "    \n",
    "    print(f\"Default (0.5): Acc={default_metrics['Accuracy']:.4f}, Prec={default_metrics['Precision']:.4f}, \"\n",
    "          f\"Rec={default_metrics['Recall']:.4f}, F1={default_metrics['F1-Score']:.4f}\")\n",
    "    print(f\"Low ({threshold}): Acc={low_metrics['Accuracy']:.4f}, Prec={low_metrics['Precision']:.4f}, \"\n",
    "          f\"Rec={low_metrics['Recall']:.4f}, F1={low_metrics['F1-Score']:.4f}\")\n",
    "    \n",
    "    # Store model with high-recall configuration using display name\n",
    "    tuned_models_high_recall[display_name] = {\n",
    "        'model': model,\n",
    "        'threshold': threshold,\n",
    "        'metrics': low_metrics,\n",
    "        'predictions': y_pred_low\n",
    "    }\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"\\n[COMPARISON: Default (0.5) vs Low ({threshold}) Threshold]\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n",
    "    'Default Recall': [tuned_models[model_mapping[m]].predict(X_test).sum() / y_test.sum() for m in ['Logistic Regression', 'Random Forest', 'XGBoost']],\n",
    "    'Low Threshold Recall': [tuned_models_high_recall[m]['metrics']['Recall'] for m in ['Logistic Regression', 'Random Forest', 'XGBoost']],\n",
    "    'Recall Improvement': [\n",
    "        tuned_models_high_recall[m]['metrics']['Recall'] - tuned_models[model_mapping[m]].predict(X_test).sum() / y_test.sum() \n",
    "        for m in ['Logistic Regression', 'Random Forest', 'XGBoost']\n",
    "    ]\n",
    "})\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a2534",
   "metadata": {},
   "source": [
    "## 4. Evaluate Baseline Models on Test Set\n",
    "\n",
    "**Important**: Run this section **BEFORE** Section 6 (Hyperparameter Tuning) to evaluate baseline performance. If you've already run Section 6, the cells below will still work but will use the tuned models instead of pure baselines.\n",
    "\n",
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression predictions\n",
    "lr_pred = models['logistic_regression'].predict(X_test)\n",
    "lr_pred_proba = models['logistic_regression'].predict_proba(X_test)\n",
    "\n",
    "# Evaluate\n",
    "lr_results = evaluate_classification(\n",
    "    y_test,\n",
    "    lr_pred,\n",
    "    lr_pred_proba,\n",
    "    labels=['Normal', 'Attack'],\n",
    "    model_name='Logistic Regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c336b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    y_test,\n",
    "    lr_pred,\n",
    "    labels=['Normal', 'Attack'],\n",
    "    normalize=False,\n",
    "    title='Logistic Regression - Confusion Matrix'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad9c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "lr_roc = plot_roc_curve(\n",
    "    y_test,\n",
    "    lr_pred_proba,\n",
    "    model_name='Logistic Regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature coefficients\n",
    "lr_coefficients = models['logistic_regression'].get_coefficients(top_n=20, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581a7fc",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d361f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest predictions\n",
    "rf_pred = models['random_forest'].predict(X_test)\n",
    "rf_pred_proba = models['random_forest'].predict_proba(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rf_results = evaluate_classification(\n",
    "    y_test,\n",
    "    rf_pred,\n",
    "    rf_pred_proba,\n",
    "    labels=['Normal', 'Attack'],\n",
    "    model_name='Random Forest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    y_test,\n",
    "    rf_pred,\n",
    "    labels=['Normal', 'Attack'],\n",
    "    normalize=True,\n",
    "    title='Random Forest - Confusion Matrix (Normalized)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a090b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "rf_roc = plot_roc_curve(\n",
    "    y_test,\n",
    "    rf_pred_proba,\n",
    "    model_name='Random Forest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "rf_importances = models['random_forest'].get_feature_importances(top_n=20, plot=True)\n",
    "\n",
    "# Tree depth statistics\n",
    "depth_stats = models['random_forest'].get_tree_depths()\n",
    "print(\"\\n[Random Forest Tree Statistics]\")\n",
    "print(f\"  Mean Depth: {depth_stats['mean_depth']:.2f}\")\n",
    "print(f\"  Max Depth:  {depth_stats['max_depth']}\")\n",
    "print(f\"  Min Depth:  {depth_stats['min_depth']}\")\n",
    "print(f\"  Std Depth:  {depth_stats['std_depth']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de04f42d",
   "metadata": {},
   "source": [
    "### 4.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost predictions\n",
    "xgb_pred = models['xgboost'].predict(X_test)\n",
    "xgb_pred_proba = models['xgboost'].predict_proba(X_test)\n",
    "\n",
    "# Evaluate\n",
    "xgb_results = evaluate_classification(\n",
    "    y_test,\n",
    "    xgb_pred,\n",
    "    xgb_pred_proba,\n",
    "    labels=['Normal', 'Attack'],\n",
    "    model_name='XGBoost'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae243ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    y_test,\n",
    "    xgb_pred,\n",
    "    labels=['Normal', 'Attack'],\n",
    "    normalize=True,\n",
    "    title='XGBoost - Confusion Matrix (Normalized)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "xgb_roc = plot_roc_curve(\n",
    "    y_test,\n",
    "    xgb_pred_proba,\n",
    "    model_name='XGBoost'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances (Gain)\n",
    "# Note: If Section 6 was run before this cell, models['xgboost'] is a raw XGBClassifier\n",
    "# In that case, extract importances directly from the booster\n",
    "try:\n",
    "    # Try wrapper class method first (if baseline model still exists)\n",
    "    xgb_importances_gain = models['xgboost'].get_feature_importances(\n",
    "        importance_type='gain',\n",
    "        top_n=20,\n",
    "        plot=True\n",
    "    )\n",
    "except AttributeError:\n",
    "    # Fallback: Extract from raw XGBoost model\n",
    "    print(\"[INFO] Using raw XGBoost model for feature importances\")\n",
    "    \n",
    "    # Get feature importances from booster\n",
    "    booster = models['xgboost'].get_booster()\n",
    "    importance_dict = booster.get_score(importance_type='gain')\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    xgb_importances_gain = pd.DataFrame({\n",
    "        'feature': list(importance_dict.keys()),\n",
    "        'importance': list(importance_dict.values())\n",
    "    }).sort_values('importance', ascending=False).head(20)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(xgb_importances_gain)), xgb_importances_gain['importance'])\n",
    "    plt.yticks(range(len(xgb_importances_gain)), xgb_importances_gain['feature'])\n",
    "    plt.xlabel('Gain', fontweight='bold')\n",
    "    plt.ylabel('Feature', fontweight='bold')\n",
    "    plt.title('XGBoost Feature Importances (Gain)', fontweight='bold', fontsize=14)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost learning curve\n",
    "models['xgboost'].plot_learning_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f610f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost hyperparameters summary\n",
    "xgb_params = models['xgboost'].get_params_summary()\n",
    "print(\"\\n[XGBoost Hyperparameters]\")\n",
    "for param, value in xgb_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f30d0da",
   "metadata": {},
   "source": [
    "## 7. Evaluate Tuned Models on Test Set\n",
    "\n",
    "Evaluate the hyperparameter-tuned models on the held-out test set and compare with baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb761ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned Logistic Regression\n",
    "lr_tuned_pred = tuned_models['logistic_regression'].predict(X_test)\n",
    "lr_tuned_proba = tuned_models['logistic_regression'].predict_proba(X_test)\n",
    "\n",
    "lr_tuned_results = evaluate_classification(\n",
    "    y_test,\n",
    "    lr_tuned_pred,\n",
    "    lr_tuned_proba,\n",
    "    labels=['Normal', 'Attack'],\n",
    "    model_name='Logistic Regression (Tuned)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned Random Forest\n",
    "rf_tuned_pred = tuned_models['random_forest'].predict(X_test)\n",
    "rf_tuned_proba = tuned_models['random_forest'].predict_proba(X_test)\n",
    "\n",
    "rf_tuned_results = evaluate_classification(\n",
    "    y_test,\n",
    "    rf_tuned_pred,\n",
    "    rf_tuned_proba,\n",
    "    labels=['Normal', 'Attack'],\n",
    "    model_name='Random Forest (Tuned)'\n",
    ")\n",
    "\n",
    "# Evaluate tuned XGBoost\n",
    "xgb_tuned_pred = tuned_models['xgboost'].predict(X_test)\n",
    "xgb_tuned_proba = tuned_models['xgboost'].predict_proba(X_test)\n",
    "\n",
    "xgb_tuned_results = evaluate_classification(\n",
    "    y_test,\n",
    "    xgb_tuned_pred,\n",
    "    xgb_tuned_proba,\n",
    "    labels=['Normal', 'Attack'],\n",
    "    model_name='XGBoost (Tuned)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb16e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPARISON - BASELINE VS TUNED (TEST SET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive comparison table\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Logistic Regression (Baseline)',\n",
    "        'Logistic Regression (Tuned)',\n",
    "        'Random Forest (Baseline)',\n",
    "        'Random Forest (Tuned)',\n",
    "        'XGBoost (Baseline)',\n",
    "        'XGBoost (Tuned)'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        lr_results['accuracy'],\n",
    "        lr_tuned_results['accuracy'],\n",
    "        rf_results['accuracy'],\n",
    "        rf_tuned_results['accuracy'],\n",
    "        xgb_results['accuracy'],\n",
    "        xgb_tuned_results['accuracy']\n",
    "    ],\n",
    "    'Precision': [\n",
    "        lr_results['precision'],\n",
    "        lr_tuned_results['precision'],\n",
    "        rf_results['precision'],\n",
    "        rf_tuned_results['precision'],\n",
    "        xgb_results['precision'],\n",
    "        xgb_tuned_results['precision']\n",
    "    ],\n",
    "    'Recall': [\n",
    "        lr_results['recall'],\n",
    "        lr_tuned_results['recall'],\n",
    "        rf_results['recall'],\n",
    "        rf_tuned_results['recall'],\n",
    "        xgb_results['recall'],\n",
    "        xgb_tuned_results['recall']\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        lr_results['f1_score'],\n",
    "        lr_tuned_results['f1_score'],\n",
    "        rf_results['f1_score'],\n",
    "        rf_tuned_results['f1_score'],\n",
    "        xgb_results['f1_score'],\n",
    "        xgb_tuned_results['f1_score']\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        lr_results['roc_auc'],\n",
    "        lr_tuned_results['roc_auc'],\n",
    "        rf_results['roc_auc'],\n",
    "        rf_tuned_results['roc_auc'],\n",
    "        xgb_results['roc_auc'],\n",
    "        xgb_tuned_results['roc_auc']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\", final_comparison.to_string(index=False))\n",
    "\n",
    "# Highlight improvements\n",
    "print(\"\\n[Performance Improvements]\")\n",
    "for model_type in ['Logistic Regression', 'Random Forest', 'XGBoost']:\n",
    "    baseline_idx = final_comparison[final_comparison['Model'] == f'{model_type} (Baseline)'].index[0]\n",
    "    tuned_idx = final_comparison[final_comparison['Model'] == f'{model_type} (Tuned)'].index[0]\n",
    "    \n",
    "    acc_improvement = final_comparison.loc[tuned_idx, 'Accuracy'] - final_comparison.loc[baseline_idx, 'Accuracy']\n",
    "    roc_improvement = final_comparison.loc[tuned_idx, 'ROC-AUC'] - final_comparison.loc[baseline_idx, 'ROC-AUC']\n",
    "    \n",
    "    print(f\"\\n{model_type}:\")\n",
    "    print(f\"  Accuracy:  {final_comparison.loc[baseline_idx, 'Accuracy']:.4f} -> {final_comparison.loc[tuned_idx, 'Accuracy']:.4f} ({acc_improvement:+.4f})\")\n",
    "    print(f\"  ROC-AUC:   {final_comparison.loc[baseline_idx, 'ROC-AUC']:.4f} -> {final_comparison.loc[tuned_idx, 'ROC-AUC']:.4f} ({roc_improvement:+.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline vs tuned ROC curves\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Logistic Regression\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_pred_proba[:, 1])\n",
    "fpr_lr_tuned, tpr_lr_tuned, _ = roc_curve(y_test, lr_tuned_proba[:, 1])\n",
    "\n",
    "axes[0].plot(fpr_lr, tpr_lr, label=f'Baseline (AUC={lr_results[\"roc_auc\"]:.4f})', linewidth=2)\n",
    "axes[0].plot(fpr_lr_tuned, tpr_lr_tuned, label=f'Tuned (AUC={lr_tuned_results[\"roc_auc\"]:.4f})', linewidth=2, linestyle='--')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate', fontweight='bold')\n",
    "axes[0].set_ylabel('True Positive Rate', fontweight='bold')\n",
    "axes[0].set_title('Logistic Regression: ROC Curves', fontweight='bold', fontsize=12)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Random Forest\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_pred_proba[:, 1])\n",
    "fpr_rf_tuned, tpr_rf_tuned, _ = roc_curve(y_test, rf_tuned_proba[:, 1])\n",
    "\n",
    "axes[1].plot(fpr_rf, tpr_rf, label=f'Baseline (AUC={rf_results[\"roc_auc\"]:.4f})', linewidth=2)\n",
    "axes[1].plot(fpr_rf_tuned, tpr_rf_tuned, label=f'Tuned (AUC={rf_tuned_results[\"roc_auc\"]:.4f})', linewidth=2, linestyle='--')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random Classifier')\n",
    "axes[1].set_xlabel('False Positive Rate', fontweight='bold')\n",
    "axes[1].set_ylabel('True Positive Rate', fontweight='bold')\n",
    "axes[1].set_title('Random Forest: ROC Curves', fontweight='bold', fontsize=12)\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# XGBoost\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_pred_proba[:, 1])\n",
    "fpr_xgb_tuned, tpr_xgb_tuned, _ = roc_curve(y_test, xgb_tuned_proba[:, 1])\n",
    "\n",
    "axes[2].plot(fpr_xgb, tpr_xgb, label=f'Baseline (AUC={xgb_results[\"roc_auc\"]:.4f})', linewidth=2)\n",
    "axes[2].plot(fpr_xgb_tuned, tpr_xgb_tuned, label=f'Tuned (AUC={xgb_tuned_results[\"roc_auc\"]:.4f})', linewidth=2, linestyle='--')\n",
    "axes[2].plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random Classifier')\n",
    "axes[2].set_xlabel('False Positive Rate', fontweight='bold')\n",
    "axes[2].set_ylabel('True Positive Rate', fontweight='bold')\n",
    "axes[2].set_title('XGBoost: ROC Curves', fontweight='bold', fontsize=12)\n",
    "axes[2].legend(loc='lower right')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/baseline_vs_tuned_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"[SAVED] ROC curve comparison to figures/baseline_vs_tuned_roc_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7086284",
   "metadata": {},
   "source": [
    "## 5. Compare Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed018f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison table\n",
    "comparison_df = compare_models(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a24d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ROC curves on same plot\n",
    "compare_roc_curves(models, X_test, y_test, figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importances (RF vs XGBoost)\n",
    "plot_feature_importances_comparison(models, top_n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378c9a8",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model comparison\n",
    "comparison_df.to_csv('../results/unsw_supervised_comparison.csv', index=False)\n",
    "print(\"[SAVED] Model comparison to results/unsw_supervised_comparison.csv\")\n",
    "\n",
    "# Save feature importances\n",
    "rf_importances.to_csv('../results/unsw_rf_feature_importances.csv', index=False)\n",
    "print(\"[SAVED] RF feature importances to results/unsw_rf_feature_importances.csv\")\n",
    "\n",
    "xgb_importances_gain.to_csv('../results/unsw_xgb_feature_importances.csv', index=False)\n",
    "print(\"[SAVED] XGBoost feature importances to results/unsw_xgb_feature_importances.csv\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'true_label': y_test,\n",
    "    'lr_pred': lr_pred,\n",
    "    'lr_proba': lr_pred_proba[:, 1],\n",
    "    'rf_pred': rf_pred,\n",
    "    'rf_proba': rf_pred_proba[:, 1],\n",
    "    'xgb_pred': xgb_pred,\n",
    "    'xgb_proba': xgb_pred_proba[:, 1]\n",
    "})\n",
    "\n",
    "predictions_df.to_csv('../results/unsw_predictions.csv', index=False)\n",
    "print(\"[SAVED] Predictions to results/unsw_predictions.csv\")\n",
    "\n",
    "print(\"\\nAll results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb0fe3",
   "metadata": {},
   "source": [
    "## 8. Two-Stage Prediction Pipeline: Attack Detection + Classification\n",
    "\n",
    "**Motivation:** In real-world security operations, the workflow is:\n",
    "1. **Stage 1 (Detection)**: Is this traffic Normal or an Attack? (Binary classification)\n",
    "2. **Stage 2 (Classification)**: If Attack, what type? (Multi-class on attacks only)\n",
    "\n",
    "This 2-stage approach is more realistic because:\n",
    "- Stage 1 focuses on high recall (don't miss attacks)\n",
    "- Stage 2 only processes detected attacks (efficiency)\n",
    "- Stage 2 provides actionable intelligence for incident response\n",
    "- **Normal traffic is excluded from Stage 2** (no need to classify type)\n",
    "\n",
    "### Important: Class Imbalance Handling\n",
    "\n",
    "Attack types in UNSW-NB15 have **severe class imbalance**:\n",
    "- Some attack types (e.g., Generic, Exploits) are common\n",
    "- Other types (e.g., Backdoor, Shellcode) are rare (<1% of attacks)\n",
    "\n",
    "To handle this, we'll use **SMOTE (Synthetic Minority Over-sampling Technique)**:\n",
    "- Generates synthetic samples for minority classes\n",
    "- Balances training data without simply duplicating samples\n",
    "- Applied only to Stage 2 training (not Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a7eef",
   "metadata": {},
   "source": [
    "### 8.1 Stage 1: Binary Attack Detection\n",
    "\n",
    "Use the best performing model from Section 5 to detect attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STAGE 1: BINARY ATTACK DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use best baseline model (typically XGBoost performs best)\n",
    "# You can change this to use tuned_models['xgboost'] after running Section 7\n",
    "stage1_model = models['xgboost']\n",
    "stage1_predictions = stage1_model.predict(X_test)\n",
    "stage1_proba = stage1_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate Stage 1 performance\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\n[Stage 1 Results: Attack Detection]\")\n",
    "print(classification_report(y_test, stage1_predictions, target_names=['Normal', 'Attack']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_stage1 = confusion_matrix(y_test, stage1_predictions)\n",
    "print(\"\\n[Stage 1 Confusion Matrix]\")\n",
    "print(f\"{'':12s} Pred Normal  Pred Attack\")\n",
    "print(f\"True Normal  {cm_stage1[0,0]:11d}  {cm_stage1[0,1]:11d}\")\n",
    "print(f\"True Attack  {cm_stage1[1,0]:11d}  {cm_stage1[1,1]:11d}\")\n",
    "\n",
    "# Identify samples predicted as attacks for Stage 2\n",
    "attack_mask = stage1_predictions == 1\n",
    "n_detected_attacks = attack_mask.sum()\n",
    "n_total_samples = len(y_test)\n",
    "\n",
    "print(f\"\\n[Stage 1 Summary]\")\n",
    "print(f\"   Total test samples: {n_total_samples:,}\")\n",
    "print(f\"   Predicted as Normal: {(~attack_mask).sum():,} ({100*(~attack_mask).sum()/n_total_samples:.2f}%)\")\n",
    "print(f\"   Predicted as Attack: {n_detected_attacks:,} ({100*n_detected_attacks/n_total_samples:.2f}%)\")\n",
    "print(f\"\\n   -> Stage 2 will classify {n_detected_attacks:,} detected attacks\")\n",
    "print(f\"   -> {(~attack_mask).sum():,} normal samples excluded from Stage 2\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43f7d8",
   "metadata": {},
   "source": [
    "### 8.2 Stage 2: Attack Type Classification with SMOTE\n",
    "\n",
    "Train a multi-class classifier **only on attack samples** to predict the specific attack type. \n",
    "\n",
    "**Key Points:**\n",
    "- Normal traffic (label=0) is **excluded** from Stage 2\n",
    "- Attack types have **severe class imbalance**\n",
    "- We'll use **SMOTE** to balance minority attack types in training\n",
    "- SMOTE is applied **only to training data**, not test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 2: ATTACK TYPE CLASSIFICATION (ATTACKS ONLY)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if attack_cat column exists in the label dictionary\n",
    "if 'attack_cat' not in y_labels['train']:\n",
    "    print(\"\\n[ERROR] 'attack_cat' column not found in dataset.\")\n",
    "    print(\"Stage 2 classification requires attack type labels.\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    # Prepare Stage 2 training data (only attacks from training set)\n",
    "    train_attack_mask = y_labels['train']['label'] == 1\n",
    "    X_train_stage2 = X_train[train_attack_mask]\n",
    "    y_train_stage2 = y_labels['train']['attack_cat'][train_attack_mask]\n",
    "    \n",
    "    # Prepare Stage 2 test data (only samples predicted as attacks by Stage 1)\n",
    "    X_test_stage2 = X_test[attack_mask]\n",
    "    # Get the true attack categories for the samples that were predicted as attacks\n",
    "    y_test_stage2_true = y_labels['test']['attack_cat'][attack_mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n[Stage 2 Data Preparation]\")\n",
    "    print(f\"   Training: {len(X_train_stage2):,} attack samples (normal excluded)\")\n",
    "    print(f\"   Test: {len(X_test_stage2):,} detected attacks from Stage 1\")\n",
    "    print(f\"   Attack types: {y_train_stage2.nunique()} unique classes\")\n",
    "    \n",
    "    # Display attack type distribution in training (check for imbalance)\n",
    "    print(f\"\\n[Stage 2 Training - Attack Type Distribution]\")\n",
    "    print(f\"{'Attack Type':<20s} {'Count':>10s} {'Percentage':>12s}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    attack_type_counts = y_train_stage2.value_counts().sort_values(ascending=False)\n",
    "    total_attacks = len(y_train_stage2)\n",
    "    \n",
    "    for attack_type, count in attack_type_counts.items():\n",
    "        percentage = 100 * count / total_attacks\n",
    "        print(f\"{str(attack_type):<20s} {count:>10,d} {percentage:>11.2f}%\")\n",
    "    \n",
    "    # Identify minority classes (less than 5% of data)\n",
    "    minority_threshold = 0.05\n",
    "    minority_classes = attack_type_counts[attack_type_counts / total_attacks < minority_threshold]\n",
    "    \n",
    "    if len(minority_classes) > 0:\n",
    "        print(f\"\\n[Class Imbalance Detected]\")\n",
    "        print(f\"   Minority classes (<5%): {len(minority_classes)} attack types\")\n",
    "        print(f\"   -> Will apply SMOTE to balance training data\")\n",
    "    \n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab052b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING STAGE 2 CLASSIFIER WITH SMOTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Encode attack types as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_stage2_encoded = label_encoder.fit_transform(y_train_stage2)\n",
    "\n",
    "print(f\"\\n[Before SMOTE]\")\n",
    "print(f\"   Training samples: {len(X_train_stage2):,}\")\n",
    "print(f\"   Class distribution:\")\n",
    "unique, counts = np.unique(y_train_stage2_encoded, return_counts=True)\n",
    "for label_idx, count in zip(unique, counts):\n",
    "    attack_name = label_encoder.inverse_transform([label_idx])[0]\n",
    "    print(f\"      {attack_name:<20s}: {count:>6,d}\")\n",
    "\n",
    "# Apply SMOTE to balance minority classes\n",
    "# Use k_neighbors=3 to handle very small classes\n",
    "try:\n",
    "    smote = SMOTE(random_state=42, k_neighbors=min(3, counts.min() - 1))\n",
    "    X_train_stage2_resampled, y_train_stage2_resampled = smote.fit_resample(\n",
    "        X_train_stage2, \n",
    "        y_train_stage2_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[After SMOTE]\")\n",
    "    print(f\"   Training samples: {len(X_train_stage2_resampled):,} (^{len(X_train_stage2_resampled) - len(X_train_stage2):,})\")\n",
    "    print(f\"   Class distribution:\")\n",
    "    unique_resampled, counts_resampled = np.unique(y_train_stage2_resampled, return_counts=True)\n",
    "    for label_idx, count in zip(unique_resampled, counts_resampled):\n",
    "        attack_name = label_encoder.inverse_transform([label_idx])[0]\n",
    "        print(f\"      {attack_name:<20s}: {count:>6,d}\")\n",
    "    \n",
    "    X_train_final = X_train_stage2_resampled\n",
    "    y_train_final = y_train_stage2_resampled\n",
    "    print(f\"\\nSMOTE applied successfully\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"\\n[WARNING] SMOTE failed: {e}\")\n",
    "    print(f\"   Using original imbalanced data\")\n",
    "    X_train_final = X_train_stage2\n",
    "    y_train_final = y_train_stage2_encoded\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Training Random Forest for Attack Type Classification...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Train Random Forest for multi-class attack classification\n",
    "stage2_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=25,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',  # Additional balancing\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "with Timer(\"Stage 2 Training\"):\n",
    "    stage2_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Predict attack types for detected attacks\n",
    "stage2_predictions_encoded = stage2_model.predict(X_test_stage2)\n",
    "stage2_predictions = label_encoder.inverse_transform(stage2_predictions_encoded)\n",
    "stage2_proba = stage2_model.predict_proba(X_test_stage2)\n",
    "\n",
    "print(\"\\n[Stage 2 Training Complete!]\")\n",
    "print(f\"   Model: Random Forest (200 trees)\")\n",
    "print(f\"   Classes: {len(label_encoder.classes_)} attack types\")\n",
    "print(f\"   Feature importance available: Yes\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24670e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Stage 2 (only on true attacks that were detected by Stage 1)\n",
    "# First, find the indices of the samples that Stage 1 predicted as attacks\n",
    "predicted_attack_indices = X_test[attack_mask].index\n",
    "\n",
    "# Next, find which of those are *actually* attacks\n",
    "true_attacks_detected_mask = y_labels['test']['label'][predicted_attack_indices] == 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 2 RESULTS: ATTACK TYPE CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# *** FIX: Reset the index of the boolean mask to align with y_test_stage2_true ***\n",
    "true_attacks_detected_mask = true_attacks_detected_mask.reset_index(drop=True)\n",
    "\n",
    "if true_attacks_detected_mask.sum() > 0:\n",
    "    # Filter the true and predicted labels to only include the true attacks that were detected\n",
    "    y_true_attacks = y_test_stage2_true[true_attacks_detected_mask]\n",
    "    y_pred_attacks = pd.Series(stage2_predictions, index=y_test_stage2_true.index)[true_attacks_detected_mask]\n",
    "    \n",
    "    print(f\"\\n[Evaluating on {len(y_true_attacks)} true attacks correctly detected by Stage 1]\")\n",
    "    print(\"\\n[Stage 2 Classification Report]\")\n",
    "    # Use the original string labels for the report\n",
    "    print(classification_report(y_true_attacks, y_pred_attacks, zero_division=0))\n",
    "    \n",
    "    # Overall accuracy for attack type classification\n",
    "    stage2_accuracy = (y_true_attacks == y_pred_attacks).mean()\n",
    "    print(f\"\\n[Stage 2 Accuracy]: {stage2_accuracy:.4f} ({100*stage2_accuracy:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo true attacks were detected by Stage 1, so Stage 2 evaluation is not possible.\")\n",
    "    stage2_accuracy = 0.0 # Set to 0 if no evaluation can be done\n",
    "    y_true_attacks = pd.Series([]) # Ensure y_true_attacks exists\n",
    "\n",
    "# Confusion matrix for attack types\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "if len(y_true_attacks) > 0:\n",
    "    # Get all possible attack types from the encoder to ensure the matrix has the correct dimensions\n",
    "    all_attack_types = label_encoder.classes_\n",
    "    \n",
    "    # **IMPROVEMENT**: Normalize the confusion matrix by the true labels (rows) to get percentages\n",
    "    cm_stage2 = confusion_matrix(y_true_attacks, y_pred_attacks, labels=all_attack_types, normalize='true')\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    # **IMPROVEMENT**: Change annotation format to percentage and update color bar\n",
    "    sns.heatmap(cm_stage2, annot=True, fmt='.1%', cmap='Blues', \n",
    "                xticklabels=all_attack_types, yticklabels=all_attack_types,\n",
    "                cbar_kws={'label': 'Percentage of True Class'})\n",
    "    plt.title('Stage 2: Attack Type Classification - Confusion Matrix (Normalized by True Label)', \n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Attack Type', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Attack Type', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/stage2_attack_type_confusion_matrix_percent.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"[SAVED] Percentage-based confusion matrix to figures/stage2_attack_type_confusion_matrix_percent.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c9ce6",
   "metadata": {},
   "source": [
    "### 8.3 End-to-End Pipeline Evaluation\n",
    "\n",
    "Evaluate the complete pipeline: Detection (Stage 1) -> Classification (Stage 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152313a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END-TO-END 2-STAGE PIPELINE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create final predictions combining both stages\n",
    "final_predictions = pd.Series(['Normal'] * len(y_test), index=y_test.index)\n",
    "final_predictions[attack_mask] = stage2_predictions\n",
    "\n",
    "# Get true labels (attack types for attacks, 'Normal' for normal)\n",
    "true_labels = y_labels['test']['attack_cat'].copy()\n",
    "true_labels[y_labels['test']['label'] == 0] = 'Normal'\n",
    "\n",
    "\n",
    "# Overall pipeline accuracy (including both normal detection and attack classification)\n",
    "pipeline_accuracy = (final_predictions == true_labels).mean()\n",
    "\n",
    "print(f\"\\n[Pipeline Performance]\")\n",
    "print(f\"   Stage 1 (Detection) Accuracy: {(stage1_predictions == y_test).mean():.4f}\")\n",
    "print(f\"   Stage 2 (Classification) Accuracy: {stage2_accuracy:.4f} (on detected true attacks)\")\n",
    "print(f\"   End-to-End Pipeline Accuracy: {pipeline_accuracy:.4f}\")\n",
    "\n",
    "# Breakdown by category\n",
    "print(f\"\\n[Pipeline Breakdown]\")\n",
    "print(f\"   Total samples: {len(y_test):,}\")\n",
    "print(f\"   Correctly classified as Normal: {((final_predictions == 'Normal') & (true_labels == 'Normal')).sum():,}\")\n",
    "print(f\"   Correctly classified attack type: {((final_predictions != 'Normal') & (final_predictions == true_labels)).sum():,}\")\n",
    "print(f\"   Misclassified Normal as Attack (False Positive): {((final_predictions != 'Normal') & (true_labels == 'Normal')).sum():,}\")\n",
    "print(f\"   Missed attacks (classified as Normal / False Negative): {((final_predictions == 'Normal') & (true_labels != 'Normal')).sum():,}\")\n",
    "print(f\"   Wrong attack type (Detected but misclassified): {((final_predictions != 'Normal') & (true_labels != 'Normal') & (final_predictions != true_labels)).sum():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 2-stage pipeline flow\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot 1: Stage 1 flow (Binary)\n",
    "stage1_flow = {\n",
    "'Normal  Normal': ((stage1_predictions == 0) & (y_test == 0)).sum(),\n",
    "'Normal  Attack': ((stage1_predictions == 1) & (y_test == 0)).sum(),\n",
    "'Attack  Normal': ((stage1_predictions == 0) & (y_test == 1)).sum(),\n",
    "'Attack  Attack': ((stage1_predictions == 1) & (y_test == 1)).sum()\n",
    "}\n",
    "\n",
    "colors_stage1 = ['green', 'orange', 'red', 'blue']\n",
    "axes[0].bar(range(len(stage1_flow)), stage1_flow.values(), color=colors_stage1, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xticks(range(len(stage1_flow)))\n",
    "axes[0].set_xticklabels(stage1_flow.keys(), rotation=15, ha='right', fontsize=10)\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Stage 1: Binary Detection Flow', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, (label, count) in enumerate(stage1_flow.items()):\n",
    "    axes[0].text(i, count + max(stage1_flow.values())*0.02, str(count), \n",
    "        ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Stage 2 attack type distribution\n",
    "if len(y_true_attacks) > 0:\n",
    "    correct_types = (y_true_attacks == y_pred_attacks).sum()\n",
    "    wrong_types = (y_true_attacks != y_pred_attacks).sum()\n",
    "    \n",
    "    stage2_breakdown = {\n",
    "        'Correct Attack Type': correct_types,\n",
    "        'Wrong Attack Type': wrong_types\n",
    "    }\n",
    "    \n",
    "    colors_stage2 = ['green', 'orange']\n",
    "    axes[1].bar(range(len(stage2_breakdown)), stage2_breakdown.values(), \n",
    "                color=colors_stage2, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xticks(range(len(stage2_breakdown)))\n",
    "    axes[1].set_xticklabels(stage2_breakdown.keys(), fontsize=11)\n",
    "    axes[1].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title(f'Stage 2: Attack Type Classification\\n({len(y_true_attacks)} detected attacks)', \n",
    "                        fontsize=13, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels and percentages\n",
    "    for i, (label, count) in enumerate(stage2_breakdown.items()):\n",
    "        pct = 100 * count / len(y_true_attacks)\n",
    "        axes[1].text(i, count + max(stage2_breakdown.values())*0.02, \n",
    "                    f'{count}\\n({pct:.1f}%)', \n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No attacks detected in Stage 1', \n",
    "                ha='center', va='center', transform=axes[1].transAxes, fontsize=12)\n",
    "    axes[1].set_title('Stage 2: Attack Type Classification', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/two_stage_pipeline_flow.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[SAVED] Pipeline flow visualization to figures/two_stage_pipeline_flow.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e6e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 2-stage pipeline results\n",
    "pipeline_results = pd.DataFrame({\n",
    "    'true_label': true_labels,\n",
    "    'stage1_prediction': ['Normal' if p == 0 else 'Attack' for p in stage1_predictions],\n",
    "    'stage1_proba': stage1_proba,\n",
    "    'final_prediction': final_predictions\n",
    "})\n",
    "\n",
    "pipeline_results.to_csv('../results/unsw_two_stage_predictions.csv', index=False)\n",
    "print(\"\\n[SAVED] 2-stage predictions to results/unsw_two_stage_predictions.csv\")\n",
    "\n",
    "# Save Stage 2 model performance per attack type\n",
    "if len(y_true_attacks) > 0:\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    \n",
    "    # Get unique labels from the true values to ensure we report on all expected classes\n",
    "    unique_labels = sorted(y_true_attacks.unique())\n",
    "    \n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true_attacks, y_pred_attacks, labels=unique_labels, zero_division=0\n",
    "    )\n",
    "    \n",
    "    attack_type_performance = pd.DataFrame({\n",
    "        'attack_type': unique_labels,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'support': support\n",
    "    })\n",
    "    \n",
    "    attack_type_performance = attack_type_performance.sort_values('f1_score', ascending=False)\n",
    "    attack_type_performance.to_csv('../results/unsw_stage2_attack_type_performance.csv', index=False)\n",
    "    print(\"[SAVED] Stage 2 attack type performance to results/unsw_stage2_attack_type_performance.csv\")\n",
    "    \n",
    "    print(\"\\n[Stage 2 Performance by Attack Type]\")\n",
    "    print(attack_type_performance.to_string(index=False))\n",
    "\n",
    "print(\"\\nTwo-stage pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e3ba4",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**1. Model Performance (Binary Classification):**\n",
    "- Successfully trained and evaluated three supervised classification models on UNSW-NB15 dataset\n",
    "- Performed comprehensive hyperparameter tuning using Grid Search (LR, RF) and Randomized Search (XGBoost)\n",
    "- Tuned models showed improvements over baseline configurations\n",
    "- XGBoost achieved strong performance with high recall for attack detection\n",
    "- Random Forest and Logistic Regression provided competitive baseline performance\n",
    "- All models demonstrated strong discriminative ability between normal and attack traffic\n",
    "\n",
    "**2. Feature Analysis:**\n",
    "- Identified top predictive features using model-based importance rankings\n",
    "- XGBoost gain-based importance highlighted critical network flow characteristics\n",
    "- Random Forest feature importances validated network traffic patterns\n",
    "- Logistic Regression coefficients showed linear decision boundaries\n",
    "- Feature importance patterns consistent across models, validating feature engineering\n",
    "\n",
    "**3. Two-Stage Pipeline:**\n",
    "- **Stage 1 (Binary Detection)**: High accuracy in identifying attacks vs normal traffic\n",
    "- **Stage 2 (Attack Classification)**: Multi-class prediction on detected attacks only\n",
    "  - **Class Imbalance Handled**: SMOTE oversampling for minority attack types\n",
    "  - **Normal Traffic Excluded**: Only attacks classified in Stage 2\n",
    "- Pipeline approach mimics real-world security operations center (SOC) workflow\n",
    "- Provides actionable intelligence for incident response teams\n",
    "- Realistic deployment scenario with detection -> classification flow\n",
    "\n",
    "**4. Methodology Insights:**\n",
    "- Baseline models establish performance floor before tuning\n",
    "- Hyperparameter tuning provides incremental improvements\n",
    "- Two-stage approach more efficient than single multi-class classifier\n",
    "- SMOTE successfully balances minority attack types in Stage 2\n",
    "- Proper train/validation/test splits prevent data leakage\n",
    "\n",
    "### Performance Highlights:\n",
    "- Binary classification: High accuracy (>95%) and F1-scores\n",
    "- Attack type classification: Balanced performance across attack types with SMOTE\n",
    "- End-to-end pipeline: Realistic operational deployment ready\n",
    "\n",
    "### Next Steps:\n",
    "- Deploy models in production monitoring environment\n",
    "- Implement real-time prediction pipeline for network traffic\n",
    "- Continuous model retraining with new attack patterns\n",
    "- Integration with security information and event management (SIEM) systems\n",
    "- A/B testing of single-stage vs two-stage approaches in production\n",
    "- Threshold tuning for Stage 1 based on operational false positive tolerance\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Complete!** [OK]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}